{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translating [Markdown] to [Python]\n",
    "\n",
    "A primary translation is literate programming is the tangle step that converts the literate program into \n",
    "the programming language. The 1979 implementation converts `\".WEB\"` files to valid pascal - `\".PAS\"` - files.\n",
    "The `pidgy` approach begins with [Markdown] files and proper [Python] files as the outcome. The rest of this \n",
    "document configures how [IPython] acknowledges the transformation and the heuristics the translate [Markdown] to [Python].\n",
    "\n",
    "[Markdown]: #\n",
    "[Python]: #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    import IPython, typing, mistune as markdown, IPython, textwrap, ast, doctest, re, dataclasses, pidgy\n",
    "    try: \n",
    "        from . import base, util\n",
    "        from .util import FENCE, CONTINUATION, SEMI, COLON, MAGIC, DOCTEST, QUOTES, SPACE, WHITESPACE\n",
    "    except: \n",
    "        import base, util\n",
    "        from util import FENCE, CONTINUATION, SEMI, COLON, MAGIC, DOCTEST, QUOTES, SPACE, WHITESPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pidgy` tangle workflow has three steps:\n",
    "\n",
    "1. Block-level lexical analysis to tokenize [Markdown].\n",
    "2. Normalize the tokens to compacted `\"code\" and not \"code\"` tokens.\n",
    "3. Translate the normalized tokens to a string of valid [Python] code.\n",
    "\n",
    "[Markdown]: #\n",
    "[Python]: #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block level lexical analysis.\n",
    "\n",
    "`pidgy` uses a modified `mistune.BlockLexer` to create block level tokens\n",
    "for a [Markdown] source. A specific `pidgy` addition is the addition off \n",
    "a `doctest` block object, `doctest` are testable strings that are ignored by the tangle\n",
    "step. The tokens are to be normalized and translated to [Python] strings.\n",
    "\n",
    "<details><summary><code>BlockLexer</code></summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "    class BlockLexer(markdown.BlockLexer, util.ContextDepth):\n",
    "        class grammar_class(markdown.BlockGrammar):\n",
    "            doctest = doctest.DocTestParser._EXAMPLE_RE\n",
    "            block_code = re.compile(r'^((?!\\s+>>>\\s) {4}[^\\n]+\\n*)+')\n",
    "            default_rules = \"newline hrule block_code fences heading nptable lheading block_quote list_block def_links def_footnotes table paragraph text\".split()\n",
    "\n",
    "        def parse_doctest(self, m): self.tokens.append({'type': 'paragraph', 'text': m.group(0)})\n",
    "\n",
    "        def parse_fences(self, m):\n",
    "            if m.group(2): self.tokens.append({'type': 'paragraph', 'text': m.group(0)})\n",
    "            else: super().parse_fences(m)\n",
    "\n",
    "        def parse_hrule(self, m): self.tokens.append(dict(type='hrule', text=m.group(0)))\n",
    "            \n",
    "        def parse_def_links(self, m):\n",
    "            super().parse_def_links(m)\n",
    "            self.tokens.append(dict(type='def_link', text=m.group(0)))\n",
    "            \n",
    "        def parse(self, text: str, default_rules=None, normalize=True) -> typing.List[dict]:\n",
    "            if not self.depth: self.tokens = []\n",
    "            with self: tokens = super().parse(util.whiten(text), default_rules)\n",
    "            if normalize and not self.depth: tokens = normalizer(text, tokens)\n",
    "            return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing the tokens\n",
    "\n",
    "Tokenizing [Markdown] typically extracts conventions at both the block and inline level.\n",
    "Fortunately, `pidgy`'s translation is restricted to block level [Markdown] tokens, and mitigating some potential complexities from having opinions about inline code while tangling.\n",
    "\n",
    "<details><summary><code>normalizer</code></summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def normalizer(text, tokens):\n",
    "        compacted = []\n",
    "        while tokens:\n",
    "            token = tokens.pop(0)\n",
    "            if 'text' not in token: continue\n",
    "            else: \n",
    "                if not token['text'].strip(): continue\n",
    "                block, body = token['text'].splitlines(), \"\"\n",
    "            while block:\n",
    "                line = block.pop(0)\n",
    "                if line:\n",
    "                    before, line, text = text.partition(line)\n",
    "                    body += before + line\n",
    "            if token['type']=='code':\n",
    "                compacted.append({'type': 'code', 'lang': None, 'text': body})\n",
    "            else:\n",
    "                if compacted and compacted[-1]['type'] == 'paragraph':\n",
    "                    compacted[-1]['text'] += body\n",
    "                else: compacted.append({'type': 'paragraph', 'text': body})\n",
    "        if compacted and compacted[-1]['type'] == 'paragraph':\n",
    "            compacted[-1]['text'] += text\n",
    "        elif text.strip():\n",
    "            compacted.append({'type': 'paragraph', 'text': text})\n",
    "        # Deal with front matter\n",
    "        if compacted and compacted[0]['text'].startswith('---\\n') and '\\n---' in compacted[0]['text'][4:]:\n",
    "            token = compacted.pop(0)\n",
    "            front_matter, sep, paragraph = token['text'][4:].partition('---')\n",
    "            compacted = [{'type': 'front_matter', 'text': F\"\\n{front_matter}\"},\n",
    "                        {'type': 'paragraph', 'text': paragraph}] + compacted\n",
    "        return compacted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flattening the tokens to a [Python] string.\n",
    "\n",
    "The tokenizer controls the translation of markdown strings to python strings.  Our major constraint is that the Markdown input should retain line numbers.\n",
    "\n",
    "<details><summary><code>Flatten</code></summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    class Tokenizer(BlockLexer):\n",
    "        def stringify(self, tokens: typing.List[dict], source: str = \"\"\"\"\"\", last: int =0) -> str:\n",
    "            INDENT = indent = util.base_indent(tokens) or 4\n",
    "            for i, token in enumerate(tokens):\n",
    "                object = token['text']\n",
    "                if token and token['type'] == 'code':\n",
    "                    if object.lstrip().startswith(FENCE):\n",
    "\n",
    "                        object = ''.join(''.join(object.partition(FENCE)[::2]).rpartition(FENCE)[::2])\n",
    "                        indent = INDENT + util.num_first_indent(object)\n",
    "                        object = textwrap.indent(object, INDENT*SPACE)\n",
    "\n",
    "                    if object.lstrip().startswith(MAGIC):  ...\n",
    "                    else: indent = util.num_last_indent(object)\n",
    "                elif token and token['type'] == 'front_matter': \n",
    "                    object = textwrap.indent(\n",
    "                        F\"locals().update(__import__('ruamel.yaml').yaml.safe_load({util.quote(object)}))\\n\", indent*SPACE)\n",
    "\n",
    "                elif not object: ...\n",
    "                else:\n",
    "                    object = textwrap.indent(object, SPACE*max(indent-util.num_first_indent(object), 0))\n",
    "                    for next in tokens[i+1:]:\n",
    "                        if next['type'] == 'code':\n",
    "                            next = util.num_first_indent(next['text'])\n",
    "                            break\n",
    "                    else: next = indent       \n",
    "                    Δ = max(next-indent, 0)\n",
    "\n",
    "                    if not Δ and source.rstrip().rstrip(CONTINUATION).endswith(COLON): \n",
    "                        Δ += 4\n",
    "\n",
    "                    spaces = util.indents(object)\n",
    "                    \"what if the spaces are ling enough\"\n",
    "                    object = object[:spaces] + Δ*SPACE+ object[spaces:]\n",
    "                    if not source.rstrip().rstrip(CONTINUATION).endswith(QUOTES): \n",
    "                        object = util.quote(object)\n",
    "                source += object\n",
    "\n",
    "            # add a semicolon to the source if the last block is code.\n",
    "            for token in reversed(tokens):\n",
    "                if token['text'].strip():\n",
    "                    if token['type'] != 'code': \n",
    "                        source = source.rstrip() + SEMI\n",
    "                    break\n",
    "\n",
    "            return source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tangling functions\n",
    "\n",
    "<details><summary><code>Utilities</code></summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for x in \"default_rules footnote_rules list_rules\".split():\n",
    "        setattr(BlockLexer, x, list(getattr(BlockLexer, x)))\n",
    "        getattr(BlockLexer, x).insert(getattr(BlockLexer, x).index('block_code'), 'doctest')\n",
    "        if 'block_html' in getattr(BlockLexer, x):\n",
    "            getattr(BlockLexer, x).pop(getattr(BlockLexer, x).index('block_html'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</summary></details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    @base.implementation\n",
    "    def tangle(text:str):\n",
    "        tokenizer = Tokenizer()\n",
    "        return tokenizer.stringify(tokenizer.parse(''.join(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `pidgy` transform manager.\n",
    "\n",
    "The `pidgy` input transform manager puts together the tokenizing, normalizing, and flattening [Markdown]\n",
    "to [Python].  It is a configurable object than can be configured by `IPython`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
