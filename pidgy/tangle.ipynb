{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tangling [Markdown] to [Python]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<!--\n",
       "    \n",
       "    import IPython, typing, mistune as markdown, IPython, textwrap, ast, doctest, re\n",
       "    try: from . import base\n",
       "    except: import base\n",
       "\n",
       "-->"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    \n",
    "    import IPython, typing, mistune as markdown, IPython, textwrap, ast, doctest, re\n",
    "    try: from . import base\n",
    "    except: import base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pidgyTransformer` using the existing `IPython.core.inputtransformer2.TransformerManager` to configure the [Markdown] language features, and it is the public API for manipulating `pidgy` strings.  It implements the heuristics applied create predictable [Python] from [Markdown]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "    class pidgyTransformer(IPython.core.inputtransformer2.TransformerManager, base.Extension):\n",
       "        def pidgy_transform(self, cell: str) -> str: \n",
       "            return self.tokenizer.untokenize(self.tokenizer.parse(''.join(cell)))\n",
       "        \n",
       "        def transform_cell(self, cell: str) -> str:\n",
       "            return super().transform_cell(self.pidgy_transform(cell))\n",
       "        transform = transform_cell\n",
       "        \n",
       "        def __init__(self, *args, **kwargs):\n",
       "            super().__init__(*args, **kwargs)\n",
       "            self.tokenizer = Tokenizer()\n",
       "\n",
       "        def pidgy_magic(self, *text): \n",
       "            return IPython.display.Code(self.pidgy_transform(''.join(text)), language='python')"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    class pidgyTransformer(IPython.core.inputtransformer2.TransformerManager, base.Extension):\n",
    "        def pidgy_transform(self, cell: str) -> str: \n",
    "            return self.tokenizer.untokenize(self.tokenizer.parse(''.join(cell)))\n",
    "        \n",
    "        def transform_cell(self, cell: str) -> str:\n",
    "            return super().transform_cell(self.pidgy_transform(cell))\n",
    "        transform = transform_cell\n",
    "        \n",
    "        def __init__(self, *args, **kwargs):\n",
    "            super().__init__(*args, **kwargs)\n",
    "            self.tokenizer = Tokenizer()\n",
    "\n",
    "        def pidgy_magic(self, *text): \n",
    "            return IPython.display.Code(self.pidgy_transform(''.join(text)), language='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The translation process\n",
    "\n",
    "A convenient considerations when tangling pidgy documents is that we are only concerned with the relative placements of block code objects relative the `not \"code\"` blocks.  `pidgy` customizes `mistune` language features for the purposes of a [Literate Computing] experience. The conversion to [Python] uses:\n",
    "1. lexical analysis to tokenize the markdown.\n",
    "2. the token are normalized to block `\"code\" and not \"code\"` objects.\n",
    "3. the tokens are translated to a string using heuristics that maintain line numbers between the representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block level lexical analysis.\n",
    "\n",
    "The block lexer converts a string in tokens that represent blocks of markdown in a text.  `pidgy` establishes a modified mistune block lexer that patches some needed features. It includes `doctest` syntax as a language feature. `doctest` are tested interactively, but they are consider `not \"code\"` objects in the [Markdown] to [Python] translation.  `doctest` is added because it is a common documentation approach in [Python], it is an example of [Literate Programming]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><code>BlockLexer</code></summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "    class BlockLexer(markdown.BlockLexer, util.ContextDepth):\n",
       "        class grammar_class(markdown.BlockGrammar):\n",
       "            doctest = doctest.DocTestParser._EXAMPLE_RE\n",
       "            block_code = re.compile(r'^((?!\\s+>>>\\s) {4}[^\\n]+\\n*)+')\n",
       "            default_rules = \"newline hrule block_code fences heading nptable lheading block_quote list_block def_links def_footnotes table paragraph text\".split()\n",
       "\n",
       "        def parse_doctest(self, m): self.tokens.append({'type': 'paragraph', 'text': m.group(0)})\n",
       "\n",
       "        def parse_fences(self, m):\n",
       "            if m.group(2): self.tokens.append({'type': 'paragraph', 'text': m.group(0)})\n",
       "            else: super().parse_fences(m)\n",
       "\n",
       "        def parse_hrule(self, m): self.tokens.append(dict(type='hrule', text=m.group(0)))\n",
       "            \n",
       "        def parse_def_links(self, m):\n",
       "            super().parse_def_links(m)\n",
       "            self.tokens.append(dict(type='def_link', text=m.group(0)))\n",
       "            \n",
       "        def parse(self, text: str, default_rules=None, normalize=True) -> typing.List[dict]:\n",
       "            if not self.depth: self.tokens = []\n",
       "            with self: tokens = super().parse(whiten(text), default_rules)\n",
       "            if normalize and not self.depth: tokens = normalizer(text, tokens)\n",
       "            return tokens\n",
       "        \n",
       "        depth = 0\n",
       "        def __enter__(self): self.depth += 1\n",
       "        def __exit__(self, *e): self.depth -= 1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    class BlockLexer(markdown.BlockLexer, util.ContextDepth):\n",
    "        class grammar_class(markdown.BlockGrammar):\n",
    "            doctest = doctest.DocTestParser._EXAMPLE_RE\n",
    "            block_code = re.compile(r'^((?!\\s+>>>\\s) {4}[^\\n]+\\n*)+')\n",
    "            default_rules = \"newline hrule block_code fences heading nptable lheading block_quote list_block def_links def_footnotes table paragraph text\".split()\n",
    "\n",
    "        def parse_doctest(self, m): self.tokens.append({'type': 'paragraph', 'text': m.group(0)})\n",
    "\n",
    "        def parse_fences(self, m):\n",
    "            if m.group(2): self.tokens.append({'type': 'paragraph', 'text': m.group(0)})\n",
    "            else: super().parse_fences(m)\n",
    "\n",
    "        def parse_hrule(self, m): self.tokens.append(dict(type='hrule', text=m.group(0)))\n",
    "            \n",
    "        def parse_def_links(self, m):\n",
    "            super().parse_def_links(m)\n",
    "            self.tokens.append(dict(type='def_link', text=m.group(0)))\n",
    "            \n",
    "        def parse(self, text: str, default_rules=None, normalize=True) -> typing.List[dict]:\n",
    "            if not self.depth: self.tokens = []\n",
    "            with self: tokens = super().parse(whiten(text), default_rules)\n",
    "            if normalize and not self.depth: tokens = normalizer(text, tokens)\n",
    "            return tokens\n",
    "        \n",
    "        depth = 0\n",
    "        def __enter__(self): self.depth += 1\n",
    "        def __exit__(self, *e): self.depth -= 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<!--\n",
       "    \n",
       "    for x in \"default_rules footnote_rules list_rules\".split():\n",
       "        setattr(BlockLexer, x, list(getattr(BlockLexer, x)))\n",
       "        getattr(BlockLexer, x).insert(getattr(BlockLexer, x).index('block_code'), 'doctest')\n",
       "        if 'block_html' in getattr(BlockLexer, x):\n",
       "            getattr(BlockLexer, x).pop(getattr(BlockLexer, x).index('block_html'))\n",
       "\n",
       "\n",
       "-->"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    \n",
    "    for x in \"default_rules footnote_rules list_rules\".split():\n",
    "        setattr(BlockLexer, x, list(getattr(BlockLexer, x)))\n",
    "        getattr(BlockLexer, x).insert(getattr(BlockLexer, x).index('block_code'), 'doctest')\n",
    "        if 'block_html' in getattr(BlockLexer, x):\n",
    "            getattr(BlockLexer, x).pop(getattr(BlockLexer, x).index('block_html'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the tokens\n",
    "\n",
    "This extra step flattens the canonical mistune token representation to the collection of `\"code\" and not \"code\"` tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><code>normalizer</code></summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "    def normalizer(text, tokens):\n",
       "        \"\"\"Combine non-code tokens into contiguous blocks.\"\"\"\n",
       "        compacted = []\n",
       "        while tokens:\n",
       "            token = tokens.pop(0)\n",
       "            if 'text' not in token: continue\n",
       "            else: \n",
       "                if not token['text'].strip(): continue\n",
       "                block, body = token['text'].splitlines(), \"\"\n",
       "            while block:\n",
       "                line = block.pop(0)\n",
       "                if line:\n",
       "                    before, line, text = text.partition(line)\n",
       "                    body += before + line\n",
       "            if token['type']=='code':\n",
       "                compacted.append({'type': 'code', 'lang': None, 'text': body})\n",
       "            else:\n",
       "                if compacted and compacted[-1]['type'] == 'paragraph':\n",
       "                    compacted[-1]['text'] += body\n",
       "                else: compacted.append({'type': 'paragraph', 'text': body})\n",
       "        if compacted and compacted[-1]['type'] == 'paragraph':\n",
       "            compacted[-1]['text'] += text\n",
       "        elif text.strip():\n",
       "            compacted.append({'type': 'paragraph', 'text': text})\n",
       "        # Deal with front matter\n",
       "        if compacted[0]['text'].startswith('---\\n') and '\\n---' in compacted[0]['text'][4:]:\n",
       "            token = compacted.pop(0)\n",
       "            front_matter, sep, paragraph = token['text'][4:].partition('---')\n",
       "            compacted = [{'type': 'front_matter', 'text': F\"\\n{front_matter}\"},\n",
       "                        {'type': 'paragraph', 'text': paragraph}] + compacted\n",
       "        return compacted"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    def normalizer(text, tokens):\n",
    "        \"\"\"Combine non-code tokens into contiguous blocks.\"\"\"\n",
    "        compacted = []\n",
    "        while tokens:\n",
    "            token = tokens.pop(0)\n",
    "            if 'text' not in token: continue\n",
    "            else: \n",
    "                if not token['text'].strip(): continue\n",
    "                block, body = token['text'].splitlines(), \"\"\n",
    "            while block:\n",
    "                line = block.pop(0)\n",
    "                if line:\n",
    "                    before, line, text = text.partition(line)\n",
    "                    body += before + line\n",
    "            if token['type']=='code':\n",
    "                compacted.append({'type': 'code', 'lang': None, 'text': body})\n",
    "            else:\n",
    "                if compacted and compacted[-1]['type'] == 'paragraph':\n",
    "                    compacted[-1]['text'] += body\n",
    "                else: compacted.append({'type': 'paragraph', 'text': body})\n",
    "        if compacted and compacted[-1]['type'] == 'paragraph':\n",
    "            compacted[-1]['text'] += text\n",
    "        elif text.strip():\n",
    "            compacted.append({'type': 'paragraph', 'text': text})\n",
    "        # Deal with front matter\n",
    "        if compacted[0]['text'].startswith('---\\n') and '\\n---' in compacted[0]['text'][4:]:\n",
    "            token = compacted.pop(0)\n",
    "            front_matter, sep, paragraph = token['text'][4:].partition('---')\n",
    "            compacted = [{'type': 'front_matter', 'text': F\"\\n{front_matter}\"},\n",
    "                        {'type': 'paragraph', 'text': paragraph}] + compacted\n",
    "        return compacted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flattening the tokens to a [Python] string.\n",
    "\n",
    "The tokenizer controls the translation of markdown strings to python strings.  Our major constraint is that the Markdown input should retain line numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><code>Flatten</code></summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "    class Tokenizer(BlockLexer):\n",
       "        def untokenize(self, tokens: typing.List[dict], source: str = \"\"\"\"\"\", last: int =0) -> str:\n",
       "            INDENT = indent = util.base_indent(tokens) or 4\n",
       "            for i, token in enumerate(tokens):\n",
       "                object = token['text']\n",
       "                if token and token['type'] == 'code':\n",
       "                    if object.lstrip().startswith(FENCE):\n",
       "\n",
       "                        object = ''.join(''.join(object.partition(FENCE)[::2]).rpartition(FENCE)[::2])\n",
       "                        indent = INDENT + util.num_first_indent(object)\n",
       "                        object = textwrap.indent(object, INDENT*SPACE)\n",
       "\n",
       "                    if object.lstrip().startswith(MAGIC):  ...\n",
       "                    else: indent = util.num_last_indent(object)\n",
       "                elif token and token['type'] == 'front_matter': \n",
       "                    object = textwrap.indent(\n",
       "                        F\"locals().update(__import__('yaml').safe_load({util.quote(object)}))\\n\", indent*SPACE)\n",
       "\n",
       "                elif not object: ...\n",
       "                else:\n",
       "                    object = textwrap.indent(object, SPACE*max(indent-util.num_first_indent(object), 0))\n",
       "                    for next in tokens[i+1:]:\n",
       "                        if next['type'] == 'code':\n",
       "                            next = util.num_first_indent(next['text'])\n",
       "                            break\n",
       "                    else: next = indent       \n",
       "                    Δ = max(next-indent, 0)\n",
       "\n",
       "                    if not Δ and source.rstrip().rstrip(CONTINUATION).endswith(COLON): \n",
       "                        Δ += 4\n",
       "\n",
       "                    spaces = util.num_whitespace(object)\n",
       "                    \"what if the spaces are ling enough\"\n",
       "                    object = object[:spaces] + Δ*SPACE+ object[spaces:]\n",
       "                    if not source.rstrip().rstrip(CONTINUATION).endswith(QUOTES): \n",
       "                        object = util.quote(object)\n",
       "                source += object\n",
       "\n",
       "            # add a semicolon to the source if the last block is code.\n",
       "            for token in reversed(tokens):\n",
       "                if token['text'].strip():\n",
       "                    if token['type'] != 'code': \n",
       "                        source = source.rstrip() + SEMI\n",
       "                    break\n",
       "\n",
       "            return source"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    class Tokenizer(BlockLexer):\n",
    "        def untokenize(self, tokens: typing.List[dict], source: str = \"\"\"\"\"\", last: int =0) -> str:\n",
    "            INDENT = indent = base_indent(tokens) or 4\n",
    "            for i, token in enumerate(tokens):\n",
    "                object = token['text']\n",
    "                if token and token['type'] == 'code':\n",
    "                    if object.lstrip().startswith(FENCE):\n",
    "\n",
    "                        object = ''.join(''.join(object.partition(FENCE)[::2]).rpartition(FENCE)[::2])\n",
    "                        indent = INDENT + util.num_first_indent(object)\n",
    "                        object = textwrap.indent(object, INDENT*SPACE)\n",
    "\n",
    "                    if object.lstrip().startswith(MAGIC):  ...\n",
    "                    else: indent = util.num_last_indent(object)\n",
    "                elif token and token['type'] == 'front_matter': \n",
    "                    object = textwrap.indent(\n",
    "                        F\"locals().update(__import__('yaml').safe_load({util.quote(object)}))\\n\", indent*SPACE)\n",
    "\n",
    "                elif not object: ...\n",
    "                else:\n",
    "                    object = textwrap.indent(object, SPACE*max(indent-util.num_first_indent(object), 0))\n",
    "                    for next in tokens[i+1:]:\n",
    "                        if next['type'] == 'code':\n",
    "                            next = util.num_first_indent(next['text'])\n",
    "                            break\n",
    "                    else: next = indent       \n",
    "                    Δ = max(next-indent, 0)\n",
    "\n",
    "                    if not Δ and source.rstrip().rstrip(CONTINUATION).endswith(COLON): \n",
    "                        Δ += 4\n",
    "\n",
    "                    spaces = util.num_whitespace(object)\n",
    "                    \"what if the spaces are ling enough\"\n",
    "                    object = object[:spaces] + Δ*SPACE+ object[spaces:]\n",
    "                    if not source.rstrip().rstrip(CONTINUATION).endswith(QUOTES): \n",
    "                        object = util.quote(object)\n",
    "                source += object\n",
    "\n",
    "            # add a semicolon to the source if the last block is code.\n",
    "            for token in reversed(tokens):\n",
    "                if token['text'].strip():\n",
    "                    if token['type'] != 'code': \n",
    "                        source = source.rstrip() + SEMI\n",
    "                    break\n",
    "\n",
    "            return source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the tokens\n",
    "\n",
    "This step may be superfluous, but it assisted in considering the logic necessary to compose the resulting python.  This extra step flattens the canonical mistune token representation is reduced to one of `\"paragraph code front_matter\"` tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Utility functions for the tangle module</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "    def normalizer(text: str, tokens: typing.List[dict]):\n",
       "        \"\"\"Combine non-code tokens into contiguous blocks.\"\"\"\n",
       "        compacted = []\n",
       "        while tokens:\n",
       "            token = tokens.pop(0)\n",
       "            if 'text' not in token: continue\n",
       "            else: \n",
       "                if not token['text'].strip(): continue\n",
       "                block, body = token['text'].splitlines(), \"\"\n",
       "            while block:\n",
       "                line = block.pop(0)\n",
       "                if line:\n",
       "                    before, line, text = text.partition(line)\n",
       "                    body += before + line\n",
       "            if token['type']=='code':\n",
       "                compacted.append({'type': 'code', 'lang': None, 'text': body})\n",
       "            else:\n",
       "                if compacted and compacted[-1]['type'] == 'paragraph':\n",
       "                    compacted[-1]['text'] += body\n",
       "                else: compacted.append({'type': 'paragraph', 'text': body})\n",
       "        if compacted and compacted[-1]['type'] == 'paragraph':\n",
       "            compacted[-1]['text'] += text\n",
       "        elif text.strip():\n",
       "            compacted.append({'type': 'paragraph', 'text': text})\n",
       "        \n",
       "        if compacted[0]['text'].startswith('---\\n') and '\\n---' in compacted[0]['text'][4:]:\n",
       "            token = compacted.pop(0)\n",
       "            front_matter, sep, paragraph = token['text'][4:].partition('---')\n",
       "            compacted = [{'type': 'front_matter', 'text': F\"\\n{front_matter}\"},\n",
       "                        {'type': 'paragraph', 'text': paragraph}] + compacted\n",
       "        return compacted"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    def normalizer(text: str, tokens: typing.List[dict]):\n",
    "        \"\"\"Combine non-code tokens into contiguous blocks.\"\"\"\n",
    "        compacted = []\n",
    "        while tokens:\n",
    "            token = tokens.pop(0)\n",
    "            if 'text' not in token: continue\n",
    "            else: \n",
    "                if not token['text'].strip(): continue\n",
    "                block, body = token['text'].splitlines(), \"\"\n",
    "            while block:\n",
    "                line = block.pop(0)\n",
    "                if line:\n",
    "                    before, line, text = text.partition(line)\n",
    "                    body += before + line\n",
    "            if token['type']=='code':\n",
    "                compacted.append({'type': 'code', 'lang': None, 'text': body})\n",
    "            else:\n",
    "                if compacted and compacted[-1]['type'] == 'paragraph':\n",
    "                    compacted[-1]['text'] += body\n",
    "                else: compacted.append({'type': 'paragraph', 'text': body})\n",
    "        if compacted and compacted[-1]['type'] == 'paragraph':\n",
    "            compacted[-1]['text'] += text\n",
    "        elif text.strip():\n",
    "            compacted.append({'type': 'paragraph', 'text': text})\n",
    "        \n",
    "        if compacted[0]['text'].startswith('---\\n') and '\\n---' in compacted[0]['text'][4:]:\n",
    "            token = compacted.pop(0)\n",
    "            front_matter, sep, paragraph = token['text'][4:].partition('---')\n",
    "            compacted = [{'type': 'front_matter', 'text': F\"\\n{front_matter}\"},\n",
    "                        {'type': 'paragraph', 'text': paragraph}] + compacted\n",
    "        return compacted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "    def load_ipython_extension(shell):\n",
       "        shell.tangle = pidgyTransformer().register()\n",
       "\n",
       "    def unload_ipython_extension(shell):\n",
       "        if hasattr(shell, 'tangle'): shell.tangle.unregister(shell)\n",
       "\n",
       "    (FENCE, CONTINUATION, SEMI, COLON, MAGIC, DOCTEST), QUOTES, SPACE ='``` \\\\ ; : %% >>>'.split(), ('\"\"\"', \"'''\"), ' '\n",
       "    WHITESPACE = re.compile('^\\s*', re.MULTILINE)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    def load_ipython_extension(shell):\n",
    "        shell.tangle = pidgyTransformer().register()\n",
    "    \n",
    "    def unload_ipython_extension(shell):\n",
    "        if hasattr(shell, 'tangle'): shell.tangle.unregister(shell)\n",
    "    \n",
    "    (FENCE, CONTINUATION, SEMI, COLON, MAGIC, DOCTEST), QUOTES, SPACE ='``` \\\\ ; : %% >>>'.split(), ('\"\"\"', \"'''\"), ' '\n",
    "    WHITESPACE = re.compile('^\\s*', re.MULTILINE)\n",
    "\n",
    "    def num_first_indent(text):\n",
    "        for str in text.splitlines():\n",
    "            if str.strip(): return len(str) - len(str.lstrip())\n",
    "        return 0\n",
    "    \n",
    "    def num_last_indent(text):\n",
    "        for str in reversed(text.splitlines()):\n",
    "            if str.strip(): return len(str) - len(str.lstrip())\n",
    "        return 0\n",
    "\n",
    "    def unload_ipython_extension(shell):\n",
    "        if hasattr(shell, 'tangle'): shell.tangle.unregister(shell)\n",
    "\n",
    "    (FENCE, CONTINUATION, SEMI, COLON, MAGIC, DOCTEST), QUOTES, SPACE ='``` \\\\ ; : %% >>>'.split(), ('\"\"\"', \"'''\"), ' '\n",
    "    WHITESPACE = re.compile('^\\s*', re.MULTILINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<!--\n",
       "    \n",
       "    for x in \"default_rules footnote_rules list_rules\".split():\n",
       "        setattr(BlockLexer, x, list(getattr(BlockLexer, x)))\n",
       "        getattr(BlockLexer, x).insert(getattr(BlockLexer, x).index('block_code'), 'doctest')\n",
       "        if 'block_html' in getattr(BlockLexer, x):\n",
       "            getattr(BlockLexer, x).pop(getattr(BlockLexer, x).index('block_html'))\n",
       "\n",
       "\n",
       "-->"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    \n",
    "    for x in \"default_rules footnote_rules list_rules\".split():\n",
    "        setattr(BlockLexer, x, list(getattr(BlockLexer, x)))\n",
    "        getattr(BlockLexer, x).insert(getattr(BlockLexer, x).index('block_code'), 'doctest')\n",
    "        if 'block_html' in getattr(BlockLexer, x):\n",
    "            getattr(BlockLexer, x).pop(getattr(BlockLexer, x).index('block_html'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</summary></details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pidgy 3",
   "language": "python",
   "name": "pidgy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
