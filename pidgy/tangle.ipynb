{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translating [Markdown] to [Python]\n",
    "\n",
    "A primary translation is literate programming is the tangle step that converts the literate program into \n",
    "the programming language. The original implementation converts `\".WEB\"` files to valid pascal - `\".PAS\"` - files.\n",
    "The `pidgy` approach begins with [Markdown] files and proper [Python] files as the outcome. The rest of this \n",
    "document configures how [IPython] acknowledges the transformation and the heuristics the translate [Markdown] to [Python].\n",
    "\n",
    "[Markdown]: #\n",
    "[Python]: #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import typing, mistune, IPython, pidgy.util, ast, textwrap, markdown_it\n",
    "    __all__ = 'tangle', 'Tangle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pidgy` tangle workflow has three steps:\n",
    "\n",
    "1. Block-level lexical analysis to tokenize [Markdown].\n",
    "2. Normalize the tokens to compacted `\"code\" and not \"code\"` tokens.\n",
    "3. Translate the normalized tokens to a string of valid [Python] code.\n",
    "\n",
    "[Markdown]: #\n",
    "[Python]: #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block level lexical analysis.\n",
    "\n",
    "`pidgy` uses a modified `mistune.BlockLexer` to create block level tokens\n",
    "for a [Markdown] source. A specific `pidgy` addition is the addition off \n",
    "a `doctest` block object, `doctest` are testable strings that are ignored by the tangle\n",
    "step. The tokens are to be normalized and translated to [Python] strings.\n",
    "\n",
    "<details><summary><code>BlockLexer</code></summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "    class BaseRenderer(pidgy.util.BaseRenderer):        \n",
    "        def quote(self, str, trailing=''):\n",
    "            \"\"\"Wrap a truple block quotations.\"\"\"\n",
    "            quote, length = self.QUOTES[self.QUOTES[0] in str], len(str)\n",
    "            left, right = length - len(str.lstrip()), len(str.rstrip())\n",
    "            if not str[left:right].strip(): return str\n",
    "            while str[right-1] == '\\\\':\n",
    "                right -= 1\n",
    "            return str[:left] + quote + str[left:right] + quote + trailing + str[right:]\n",
    "\n",
    "        def measure_base_indent(self, tokens, env): \n",
    "            next = self.get_next_code_token(tokens, -1)\n",
    "            if next and next.type == 'code_block':\n",
    "                env['base_indent'] = pidgy.util.lead_indent(env['src'][slice(*next.map)])\n",
    "            else:\n",
    "                env['base_indent'] = 4\n",
    "                \n",
    "        def get_next_code_token(self, tokens, idx):\n",
    "            for token in tokens[idx+1:]:\n",
    "                if token.type in {'code_block'}:\n",
    "                    return token\n",
    "        \n",
    "        def hanging_indent(self, str, env):\n",
    "            start = len(str)-len(str.lstrip())\n",
    "            return str[:start] + ' '* env['extra_indent'] + str[start:]\n",
    "        \n",
    "        def indent(self, str, env):\n",
    "            return textwrap.indent(str, ' ' *env['base_indent'])\n",
    "\n",
    "\n",
    "        def token_to_str(self, tokens, idx, env):\n",
    "            if idx < len(tokens):\n",
    "                if tokens[idx] and tokens[idx].map:\n",
    "                    return ''.join(env['src'][slice(*tokens[idx].map)])\n",
    "            return \"\"\n",
    "        \n",
    "        def update_env(self, code, tokens, idx, env):\n",
    "            next = self.get_next_code_token(tokens, idx)\n",
    "            extra_indent = 0\n",
    "            if next:\n",
    "                extra_indent = max(0, pidgy.util.lead_indent(env['src'][slice(*next.map)]) -env['base_indent'])\n",
    "            if not extra_indent and code.rstrip().endswith(\":\"):\n",
    "                extra_indent += 4\n",
    "            rstrip = code.rstrip()\n",
    "            env.update(\n",
    "                extra_indent=extra_indent,\n",
    "                base_indent=pidgy.util.trailing_indent(code),\n",
    "                continued=rstrip.endswith('\\\\'), \n",
    "                quoted=rstrip.rstrip('\\\\').endswith(self.QUOTES)\n",
    "            )\n",
    "        def render(self, tokens, options, env):\n",
    "            env.update(base_indent=0, quoted=False, extra_indent=0, continued=False)\n",
    "            tokens = pidgy.util.reconfigure_tokens(pidgy.util.filter_tangle_tokens(tokens), env)\n",
    "            self.measure_base_indent(tokens, env)\n",
    "            if not tokens:\n",
    "                return self.quote(''.join(env['src']), trailing=';')\n",
    "            return textwrap.dedent(pidgy.util.continuation(\n",
    "                markdown_it.renderer.RendererHTML.render(self, tokens, options, env), env\n",
    "            ) + \"\\n\" + self.noncode(tokens, len(tokens), env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    class PythonRender(BaseRenderer):\n",
    "        QUOTES = '\"\"\"', \"'''\"\n",
    "    \n",
    "        def noncode(self, tokens, idx, env): \n",
    "            token, range, prior = None, slice(None), slice(*tokens[-1].map)\n",
    "            if idx < len(tokens):\n",
    "                token = tokens[idx]\n",
    "                range, prior = slice(*tokens[idx].map), slice(*tokens[idx-1].map) if idx else slice(0,0)                \n",
    "            \n",
    "            non_code = pidgy.util.dedent_block(''.join(env['src'][prior.stop:range.start]))\n",
    "            non_code = self.indent(self.hanging_indent(non_code, env), env)\n",
    "            if not env.get('quoted', False):\n",
    "                non_code = self.quote(non_code, trailing=';' if token is None else '')\n",
    "            return non_code\n",
    "                \n",
    "        def update_env(self, code, tokens, idx, env):\n",
    "            next = self.get_next_code_token(tokens, idx)\n",
    "            env.update(base_indent=pidgy.util.trailing_indent(code))\n",
    "\n",
    "            extra_indent = 0\n",
    "            if next:\n",
    "                extra_indent = max(0, pidgy.util.lead_indent(env['src'][slice(*next.map)]) -env['base_indent'])\n",
    "            if not extra_indent and code.rstrip().endswith(\":\"):\n",
    "                extra_indent += 4\n",
    "            rstrip = code.rstrip()\n",
    "            env.update(\n",
    "                extra_indent=extra_indent,\n",
    "                continued=rstrip.endswith('\\\\'), \n",
    "                quoted=rstrip.rstrip('\\\\').endswith(self.QUOTES)\n",
    "            )\n",
    "        \n",
    "        def code_block(self, tokens, idx, options, env):\n",
    "            code = self.noncode(tokens, idx, env) + pidgy.util.quote_docstrings(self.token_to_str(tokens, idx, env))\n",
    "            return self.update_env(code, tokens, idx, env) or code\n",
    "\n",
    "        \n",
    "        def fence(self, tokens, idx, options, env):\n",
    "            \"We'll only recieve fences without a lang.\"\n",
    "            code =  self.noncode(tokens, idx, env) + textwrap.indent(\n",
    "                pidgy.util.quote_docstrings(pidgy.util.unfence(self.token_to_str(tokens, idx, env))), ' '*4\n",
    "            )\n",
    "            return self.update_env(code, tokens, idx, env) or code\n",
    "\n",
    "        def front_matter(self, tokens, idx, options, env):\n",
    "            token, code = tokens[idx], self.token_to_str(tokens, idx, env)\n",
    "            if token.markup == '+++':\n",
    "                code = F'''locals().update(__import__('toml').loads(\"\"\"{code}\"\"\".partition('+++')[2].rpartition('+++')[0]))\\n'''\n",
    "            elif token.markup == '---':\n",
    "                code = F'''locals().update(__import__('yaml').safe_load(\"\"\"{code}\"\"\".partition('---')[2].rpartition('---')[0]))\\n'''            \n",
    "            return self.indent(code, env)\n",
    "\n",
    "            \n",
    "        def reference(self, tokens, idx, options, env, *, re='link_item'):\n",
    "            token, code = tokens[idx], self.token_to_str(tokens, idx, env)\n",
    "            if env['quoted']:\n",
    "                return code\n",
    "            \n",
    "            expr  = \"{\"+F\"\"\"x.group(1): x.group(2).rstrip() for x in __import__('pidgy').util.{re}.finditer({\n",
    "                self.quote(textwrap.dedent(code), trailing=\")}\").rstrip()\n",
    "            }\"\"\"\n",
    "            if not env['continued']:\n",
    "                expr = \"\"\"locals()[\"__annotations__\"] = {**%s, **locals().get('__annotations__', {})}\"\"\"%expr\n",
    "            code = self.noncode(tokens, idx, env) + self.indent(expr + \"\\n\", env)\n",
    "            return code\n",
    "        \n",
    "        def footnote_reference_open(self, tokens, idx, options, env):\n",
    "            return self.reference(tokens, idx, options, env, re='footnote_item')\n",
    "        \n",
    "        def bullet_list_open(self, tokens, idx, options, env):\n",
    "            token, code = tokens[idx], self.token_to_str(tokens, idx, env)\n",
    "            if env['quoted']:\n",
    "                return code\n",
    "            if env['continued']:\n",
    "                return self.indent(\n",
    "                    (F\"\"\"[x.group().rstrip().partition(' ')[2] for x in __import__('pidgy').util.list_item.finditer({\n",
    "                        self.quote(textwrap.dedent(code), trailing=')]')\n",
    "                    }\\n\"\"\"), env)\n",
    "            code = self.quote(textwrap.dedent(code), trailing=';')\n",
    "            code = self.indent(self.hanging_indent(code, env), env)\n",
    "            return code\n",
    "\n",
    "        ordered_list_open = bullet_list_open "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    @pidgy.implementation\n",
    "    def tangle(str:str)->str:\n",
    "        translate = Tangle()\n",
    "        return translate.stringify(''.join(str or []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    class pidgyManager(IPython.core.inputtransformer2.TransformerManager):\n",
    "        def transform_cell(self, cell): return super(type(self), self).transform_cell(tangle(str=cell))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flattening the tokens to a [Python] string.\n",
    "\n",
    "The tokenizer controls the translation of markdown strings to python strings.  Our major constraint is that the Markdown input should retain line numbers.\n",
    "\n",
    "<details><summary><code>Flatten</code></summary>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append the lexer for nested rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "        class Tangle(markdown_it.MarkdownIt):\n",
    "            def __init__(self, *args, **kwargs):\n",
    "                kwargs['renderer_cls'] = kwargs.get('renderer_cls', PythonRender)\n",
    "                super().__init__(*args, **kwargs)\n",
    "                [self.block.ruler.before(\n",
    "                    \"code\",\n",
    "                    \"front_matter\",\n",
    "                    __import__('functools').partial(pidgy.util.frontMatter, x),\n",
    "                    {\"alt\": [\"paragraph\", \"reference\", \"blockquote\", \"list\"]},\n",
    "                ) for x in \"-+\"]\n",
    "                self.block.ruler.before(\n",
    "                    \"reference\", \"footnote_def\", markdown_it.extensions.footnote.index.footnote_def, {\"alt\": [\"paragraph\", \"reference\"]}\n",
    "                )\n",
    "                self.disable('html_block')\n",
    "                \n",
    "            def parse(self, src, env=None, normalize=False):\n",
    "                src = pidgy.util.enforce_blanklines(src)\n",
    "                if env is None:\n",
    "                    env = markdown_it.utils.AttrDict()\n",
    "                env.update(src=src.splitlines(True))\n",
    "                tokens = super().parse(src, env)\n",
    "                if normalize: tokens = pidgy.util.reconfigure_tokens(pidgy.util.filter_tangle_tokens(tokens), env)\n",
    "                return tokens\n",
    "            def render(self, src, env=None):                \n",
    "                if env is None:\n",
    "                    env  = markdown_it.utils.AttrDict()\n",
    "                return super().render(src, env)\n",
    "            def stringify(self, src, env=None):               \n",
    "                env = env or markdown_it.utils.AttrDict()\n",
    "                return self.render(src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More `pidgy` langauge features\n",
    "\n",
    "`pidgy` experiments extra language features for python, using the same system\n",
    "that IPython uses to add features like line and cell magics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recently, IPython introduced a convention that allows top level await statements outside of functions. Building of this convenience, `pidgy` allows for top-level __return__ and __yield__ statements.  These statements are replaced with the an IPython display statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    class ExtraSyntax(ast.NodeTransformer):\n",
    "        def visit_FunctionDef(self, node): return node\n",
    "        visit_AsyncFunctionDef = visit_FunctionDef        \n",
    "\n",
    "        def visit_Return(self, node):\n",
    "            replace = ast.parse('''__import__('IPython').display.display()''').body[0]\n",
    "            replace.value.args = node.value.elts if isinstance(node.value, ast.Tuple) else [node.value]\n",
    "            return ast.copy_location(replace, node)\n",
    "\n",
    "        def visit_Expr(self, node):\n",
    "            if isinstance(node.value, (ast.Yield, ast.YieldFrom)):  return ast.copy_location(self.visit_Return(node.value), node)\n",
    "            return node\n",
    "\n",
    "        visit_Expression = visit_Expr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know naming is hard, there is no point focusing on it. `pidgy` allows authors\n",
    "to use emojis as variables in python. They add extra color and expression to the narrative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def demojize(lines, delimiters=('_', '_')):\n",
    "        str = ''.join(lines or [])\n",
    "        import tokenize, emoji, stringcase; tokens = []\n",
    "        try:\n",
    "            for token in list(tokenize.tokenize(\n",
    "                __import__('io').BytesIO(str.encode()).readline)):\n",
    "                if token.type == tokenize.ERRORTOKEN:\n",
    "                    string = emoji.demojize(token.string, delimiters=delimiters\n",
    "                                           ).replace('-', '_').replace(\"â€™\", \"_\")\n",
    "                    if tokens and tokens[-1].type == tokenize.NAME: tokens[-1] = tokenize.TokenInfo(tokens[-1].type, tokens[-1].string + string, tokens[-1].start, tokens[-1].end, tokens[-1].line)\n",
    "                    else: tokens.append(\n",
    "                        tokenize.TokenInfo(\n",
    "                            tokenize.NAME, string, token.start, token.end, token.line))\n",
    "                else: tokens.append(token)\n",
    "            return tokenize.untokenize(tokens).decode()\n",
    "        except BaseException: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def init_json():\n",
    "        import builtins\n",
    "        builtins.yes = builtins.true = True\n",
    "        builtins.no = builtins.false = False\n",
    "        builtins.null = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
