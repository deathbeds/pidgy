{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tangling [Markdown] to [Python]\n",
    "\n",
    "The `tangle` step is the keystone of `pidgy` by defining the\n",
    "heuristics that translate [Markdown] to [Python] execute\n",
    "blocks of narrative as interactive code, and entire programs.\n",
    "A key constraint in the translation is a line-for-line mapping\n",
    "between representations, with this we'll benefit from reusable \n",
    "tracebacks for [Markdown] source.\n",
    "\n",
    "There are many ways to translate [Markdown] to other formats specifically with tools\n",
    "like `\"pandoc\"`.  The formats are document formatting language, and not programs.\n",
    "The [Markdown] to [Python] translation adds a computable dimension to the document.\n",
    "`pidgy` is one implementation and it should be possible to apply to different heuristics to other\n",
    "programming languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<!--\n",
       "    \n",
       "    import IPython, typing as τ, mistune as markdown, IPython, importnb as _import_, textwrap, ast, doctest, typing, re, dataclasses\n",
       "    if __name__ == '__main__':\n",
       "        import pidgy\n",
       "        shell = IPython.get_ipython()\n",
       "\n",
       "-->"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    \n",
    "    import IPython, typing as τ, mistune as markdown, IPython, importnb as _import_, textwrap, ast, doctest, typing, re, dataclasses\n",
    "    if __name__ == '__main__':\n",
    "        import pidgy\n",
    "        shell = IPython.get_ipython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pidgyTransformer` manages the high level API the `IPython.InteractiveShell` interacts with for `pidgy`.\n",
    "The `IPython.core.inputtransformer2.TransformerManager` is a configurable class for modifying\n",
    "input source to before it passes to the compiler.  It is the object that introduces `IPython`s line\n",
    "and cell magics.\n",
    "\n",
    "    >>> assert isinstance(shell.input_transformer_manager, IPython.core.inputtransformer2.TransformerManager)\n",
    "    \n",
    "This configurable class has three different flavors of transformations.\n",
    "\n",
    "* `shell.input_transformer_manager.cleanup_transforms`\n",
    "* `shell.input_transformer_manager.line_transforms`\n",
    "* `shell.input_transformer_manager.token_transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "    class pidgyTransformer(IPython.core.inputtransformer2.TransformerManager):\n",
       "        def pidgy_transform(self, cell: str) -> str: \n",
       "            return self.tokenizer.untokenize(self.tokenizer.parse(''.join(cell)))\n",
       "        \n",
       "        def transform_cell(self, cell):\n",
       "            return super().transform_cell(self.pidgy_transform(cell))\n",
       "        \n",
       "        def __init__(self, *args, **kwargs):\n",
       "            super().__init__(*args, **kwargs)\n",
       "            self.tokenizer = Tokenizer()\n",
       "\n",
       "        def pidgy_magic(self, *text): \n",
       "            return IPython.display.Code(self.pidgy_transform(''.join(text)), language='python')"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    class pidgyTransformer(IPython.core.inputtransformer2.TransformerManager):\n",
    "        def pidgy_transform(self, cell: str) -> str: \n",
    "            return self.tokenizer.untokenize(self.tokenizer.parse(''.join(cell)))\n",
    "        \n",
    "        def transform_cell(self, cell):\n",
    "            return super().transform_cell(self.pidgy_transform(cell))\n",
    "        \n",
    "        def __init__(self, *args, **kwargs):\n",
    "            super().__init__(*args, **kwargs)\n",
    "            self.tokenizer = Tokenizer()\n",
    "\n",
    "        def pidgy_magic(self, *text): \n",
    "            return IPython.display.Code(self.pidgy_transform(''.join(text)), language='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block level lexical analysis.\n",
    "\n",
    "Translating [Markdown] to [Python] rely only on block level objects in the [Markdown]\n",
    "grammar.  The `BlockLexer` is a modified analyzer that adds logic to include `doctest` \n",
    "blocks in the grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "    class BlockLexer(markdown.BlockLexer):\n",
       "        class grammar_class(markdown.BlockGrammar):\n",
       "            doctest = doctest.DocTestParser._EXAMPLE_RE\n",
       "            block_code = re.compile(r'^((?!\\s+>>>\\s) {4}[^\\n]+\\n*)+')\n",
       "            default_rules = \"newline hrule block_code fences heading nptable lheading block_quote list_block def_links def_footnotes table paragraph text\".split()\n",
       "\n",
       "        def parse_doctest(self, m): self.tokens.append({'type': 'paragraph', 'text': m.group(0)})\n",
       "\n",
       "        def parse_fences(self, m):\n",
       "            if m.group(2): self.tokens.append({'type': 'paragraph', 'text': m.group(0)})\n",
       "            else: super().parse_fences(m)\n",
       "\n",
       "        def parse_hrule(self, m): self.tokens.append(dict(type='hrule', text=m.group(0)))\n",
       "            \n",
       "        def parse_def_links(self, m):\n",
       "            super().parse_def_links(m)\n",
       "            self.tokens.append(dict(type='def_link', text=m.group(0)))\n",
       "            \n",
       "        def parse(self, text: str, default_rules=None, normalize=True) -> typing.List[dict]:\n",
       "            if not self.depth: self.tokens = []\n",
       "            with self: tokens = super().parse(whiten(text), default_rules)\n",
       "            if normalize and not self.depth: tokens = self.normalize(text, tokens)\n",
       "            return tokens\n",
       "        \n",
       "        depth = 0\n",
       "        def __enter__(self): self.depth += 1\n",
       "        def __exit__(self, *e): self.depth -= 1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    class BlockLexer(markdown.BlockLexer):\n",
    "        class grammar_class(markdown.BlockGrammar):\n",
    "            doctest = doctest.DocTestParser._EXAMPLE_RE\n",
    "            block_code = re.compile(r'^((?!\\s+>>>\\s) {4}[^\\n]+\\n*)+')\n",
    "            default_rules = \"newline hrule block_code fences heading nptable lheading block_quote list_block def_links def_footnotes table paragraph text\".split()\n",
    "\n",
    "        def parse_doctest(self, m): self.tokens.append({'type': 'paragraph', 'text': m.group(0)})\n",
    "\n",
    "        def parse_fences(self, m):\n",
    "            if m.group(2): self.tokens.append({'type': 'paragraph', 'text': m.group(0)})\n",
    "            else: super().parse_fences(m)\n",
    "\n",
    "        def parse_hrule(self, m): self.tokens.append(dict(type='hrule', text=m.group(0)))\n",
    "            \n",
    "        def parse_def_links(self, m):\n",
    "            super().parse_def_links(m)\n",
    "            self.tokens.append(dict(type='def_link', text=m.group(0)))\n",
    "            \n",
    "        def parse(self, text: str, default_rules=None, normalize=True) -> typing.List[dict]:\n",
    "            if not self.depth: self.tokens = []\n",
    "            with self: tokens = super().parse(whiten(text), default_rules)\n",
    "            if normalize and not self.depth: tokens = self.normalize(text, tokens)\n",
    "            return tokens\n",
    "        \n",
    "        depth = 0\n",
    "        def __enter__(self): self.depth += 1\n",
    "        def __exit__(self, *e): self.depth -= 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `doctest` token is identified before the block code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<!--\n",
       "    \n",
       "    for x in \"default_rules footnote_rules list_rules\".split():\n",
       "        setattr(BlockLexer, x, list(getattr(BlockLexer, x)))\n",
       "        getattr(BlockLexer, x).insert(getattr(BlockLexer, x).index('block_code'), 'doctest')\n",
       "        if 'block_html' in getattr(BlockLexer, x):\n",
       "            getattr(BlockLexer, x).pop(getattr(BlockLexer, x).index('block_html'))\n",
       "\n",
       "\n",
       "-->"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    \n",
    "    for x in \"default_rules footnote_rules list_rules\".split():\n",
    "        setattr(BlockLexer, x, list(getattr(BlockLexer, x)))\n",
    "        getattr(BlockLexer, x).insert(getattr(BlockLexer, x).index('block_code'), 'doctest')\n",
    "        if 'block_html' in getattr(BlockLexer, x):\n",
    "            getattr(BlockLexer, x).pop(getattr(BlockLexer, x).index('block_html'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our translation creates tokens specific to each [Markdown] rule, \n",
    "for code it is only necessary to identify code and paragraph tokens.\n",
    "The normalizer compacts tokens into the necessary tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "    class Normalizer(BlockLexer):\n",
       "        def normalize(self, text, tokens):\n",
       "            \"\"\"Combine non-code tokens into contiguous blocks.\"\"\"\n",
       "            compacted = []\n",
       "            while tokens:\n",
       "                token = tokens.pop(0)\n",
       "                if 'text' not in token: continue\n",
       "                else: \n",
       "                    if not token['text'].strip(): continue\n",
       "                    block, body = token['text'].splitlines(), \"\"\n",
       "                while block:\n",
       "                    line = block.pop(0)\n",
       "                    if line:\n",
       "                        before, line, text = text.partition(line)\n",
       "                        body += before + line\n",
       "                if token['type']=='code':\n",
       "                    compacted.append({'type': 'code', 'lang': None, 'text': body})\n",
       "                else:\n",
       "                    if compacted and compacted[-1]['type'] == 'paragraph':\n",
       "                        compacted[-1]['text'] += body\n",
       "                    else: compacted.append({'type': 'paragraph', 'text': body})\n",
       "            if compacted and compacted[-1]['type'] == 'paragraph':\n",
       "                compacted[-1]['text'] += text\n",
       "            elif text.strip():\n",
       "                compacted.append({'type': 'paragraph', 'text': text})\n",
       "            # Deal with front matter\n",
       "            if compacted[0]['text'].startswith('---\\n') and '\\n---' in compacted[0]['text'][4:]:\n",
       "                token = compacted.pop(0)\n",
       "                front_matter, sep, paragraph = token['text'][4:].partition('---')\n",
       "                compacted = [{'type': 'front_matter', 'text': F\"\\n{front_matter}\"},\n",
       "                            {'type': 'paragraph', 'text': paragraph}] + compacted\n",
       "            return compacted"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    class Normalizer(BlockLexer):\n",
    "        def normalize(self, text, tokens):\n",
    "            \"\"\"Combine non-code tokens into contiguous blocks.\"\"\"\n",
    "            compacted = []\n",
    "            while tokens:\n",
    "                token = tokens.pop(0)\n",
    "                if 'text' not in token: continue\n",
    "                else: \n",
    "                    if not token['text'].strip(): continue\n",
    "                    block, body = token['text'].splitlines(), \"\"\n",
    "                while block:\n",
    "                    line = block.pop(0)\n",
    "                    if line:\n",
    "                        before, line, text = text.partition(line)\n",
    "                        body += before + line\n",
    "                if token['type']=='code':\n",
    "                    compacted.append({'type': 'code', 'lang': None, 'text': body})\n",
    "                else:\n",
    "                    if compacted and compacted[-1]['type'] == 'paragraph':\n",
    "                        compacted[-1]['text'] += body\n",
    "                    else: compacted.append({'type': 'paragraph', 'text': body})\n",
    "            if compacted and compacted[-1]['type'] == 'paragraph':\n",
    "                compacted[-1]['text'] += text\n",
    "            elif text.strip():\n",
    "                compacted.append({'type': 'paragraph', 'text': text})\n",
    "            # Deal with front matter\n",
    "            if compacted[0]['text'].startswith('---\\n') and '\\n---' in compacted[0]['text'][4:]:\n",
    "                token = compacted.pop(0)\n",
    "                front_matter, sep, paragraph = token['text'][4:].partition('---')\n",
    "                compacted = [{'type': 'front_matter', 'text': F\"\\n{front_matter}\"},\n",
    "                            {'type': 'paragraph', 'text': paragraph}] + compacted\n",
    "            return compacted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer controls the translation of markdown strings to python strings.  Our major constraint is that the Markdown input should retain line numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "    class Tokenizer(Normalizer):\n",
       "        def untokenize(self, tokens: τ.List[dict], source: str = \"\"\"\"\"\", last: int =0) -> str:\n",
       "            INDENT = indent = base_indent(tokens) or 4\n",
       "            for i, token in enumerate(tokens):\n",
       "                object = token['text']\n",
       "                if token and token['type'] == 'code':\n",
       "                    if object.lstrip().startswith(FENCE):\n",
       "\n",
       "                        object = ''.join(''.join(object.partition(FENCE)[::2]).rpartition(FENCE)[::2])\n",
       "                        indent = INDENT + num_first_indent(object)\n",
       "                        object = textwrap.indent(object, INDENT*SPACE)\n",
       "\n",
       "                    if object.lstrip().startswith(MAGIC):  ...\n",
       "                    else: indent = num_last_indent(object)\n",
       "                elif token and token['type'] == 'front_matter': \n",
       "                    object = textwrap.indent(\n",
       "                        F\"locals().update(__import__('yaml').safe_load({quote(object)}))\\n\", indent*SPACE)\n",
       "\n",
       "                elif not object: ...\n",
       "                else:\n",
       "                    object = textwrap.indent(object, SPACE*max(indent-num_first_indent(object), 0))\n",
       "                    for next in tokens[i+1:]:\n",
       "                        if next['type'] == 'code':\n",
       "                            next = num_first_indent(next['text'])\n",
       "                            break\n",
       "                    else: next = indent       \n",
       "                    Δ = max(next-indent, 0)\n",
       "\n",
       "                    if not Δ and source.rstrip().rstrip(CONTINUATION).endswith(COLON): \n",
       "                        Δ += 4\n",
       "\n",
       "                    spaces = num_whitespace(object)\n",
       "                    \"what if the spaces are ling enough\"\n",
       "                    object = object[:spaces] + Δ*SPACE+ object[spaces:]\n",
       "                    if not source.rstrip().rstrip(CONTINUATION).endswith(QUOTES): \n",
       "                        object = quote(object)\n",
       "                source += object\n",
       "\n",
       "            # add a semicolon to the source if the last block is code.\n",
       "            for token in reversed(tokens):\n",
       "                if token['text'].strip():\n",
       "                    if token['type'] != 'code': \n",
       "                        source = source.rstrip() + SEMI\n",
       "                    break\n",
       "\n",
       "            return source\n",
       "            \n",
       "    pidgy = pidgyTransformer()"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    class Tokenizer(Normalizer):\n",
    "        def untokenize(self, tokens: τ.List[dict], source: str = \"\"\"\"\"\", last: int =0) -> str:\n",
    "            INDENT = indent = base_indent(tokens) or 4\n",
    "            for i, token in enumerate(tokens):\n",
    "                object = token['text']\n",
    "                if token and token['type'] == 'code':\n",
    "                    if object.lstrip().startswith(FENCE):\n",
    "\n",
    "                        object = ''.join(''.join(object.partition(FENCE)[::2]).rpartition(FENCE)[::2])\n",
    "                        indent = INDENT + num_first_indent(object)\n",
    "                        object = textwrap.indent(object, INDENT*SPACE)\n",
    "\n",
    "                    if object.lstrip().startswith(MAGIC):  ...\n",
    "                    else: indent = num_last_indent(object)\n",
    "                elif token and token['type'] == 'front_matter': \n",
    "                    object = textwrap.indent(\n",
    "                        F\"locals().update(__import__('yaml').safe_load({quote(object)}))\\n\", indent*SPACE)\n",
    "\n",
    "                elif not object: ...\n",
    "                else:\n",
    "                    object = textwrap.indent(object, SPACE*max(indent-num_first_indent(object), 0))\n",
    "                    for next in tokens[i+1:]:\n",
    "                        if next['type'] == 'code':\n",
    "                            next = num_first_indent(next['text'])\n",
    "                            break\n",
    "                    else: next = indent       \n",
    "                    Δ = max(next-indent, 0)\n",
    "\n",
    "                    if not Δ and source.rstrip().rstrip(CONTINUATION).endswith(COLON): \n",
    "                        Δ += 4\n",
    "\n",
    "                    spaces = num_whitespace(object)\n",
    "                    \"what if the spaces are ling enough\"\n",
    "                    object = object[:spaces] + Δ*SPACE+ object[spaces:]\n",
    "                    if not source.rstrip().rstrip(CONTINUATION).endswith(QUOTES): \n",
    "                        object = quote(object)\n",
    "                source += object\n",
    "\n",
    "            # add a semicolon to the source if the last block is code.\n",
    "            for token in reversed(tokens):\n",
    "                if token['text'].strip():\n",
    "                    if token['type'] != 'code': \n",
    "                        source = source.rstrip() + SEMI\n",
    "                    break\n",
    "\n",
    "            return source\n",
    "            \n",
    "    pidgy = pidgyTransformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Utility functions for the tangle module</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "    def load_ipython_extension(shell):\n",
       "        shell.input_transformer_manager = shell.tangle = pidgyTransformer()        \n",
       "    \n",
       "    def unload_ipython_extension(shell):\n",
       "        shell.input_transformer_manager = __import__('IPython').core.inputtransformer2.TransformerManager()\n",
       "    \n",
       "    (FENCE, CONTINUATION, SEMI, COLON, MAGIC, DOCTEST), QUOTES, SPACE ='``` \\\\ ; : %% >>>'.split(), ('\"\"\"', \"'''\"), ' '\n",
       "    WHITESPACE = re.compile('^\\s*', re.MULTILINE)\n",
       "\n",
       "    def num_first_indent(text):\n",
       "        for str in text.splitlines():\n",
       "            if str.strip(): return len(str) - len(str.lstrip())\n",
       "        return 0\n",
       "    \n",
       "    def num_last_indent(text):\n",
       "        for str in reversed(text.splitlines()):\n",
       "            if str.strip(): return len(str) - len(str.lstrip())\n",
       "        return 0\n",
       "\n",
       "    def base_indent(tokens):\n",
       "        \"Look ahead for the base indent.\"\n",
       "        for i, token in enumerate(tokens):\n",
       "            if token['type'] == 'code':\n",
       "                code = token['text']\n",
       "                if code.lstrip().startswith(FENCE): continue\n",
       "                indent = num_first_indent(code)\n",
       "                break\n",
       "        else: indent = 4\n",
       "        return indent\n",
       "\n",
       "    def quote(text):\n",
       "        \"\"\"wrap text in `QUOTES`\"\"\"\n",
       "        if text.strip():\n",
       "            left, right = len(text)-len(text.lstrip()), len(text.rstrip())\n",
       "            quote = QUOTES[(text[right-1] in QUOTES[0]) or (QUOTES[0] in text)]\n",
       "            return text[:left] + quote + text[left:right] + quote + text[right:]\n",
       "        return text    \n",
       "\n",
       "    def num_whitespace(text): return len(text) - len(text.lstrip())\n",
       "    \n",
       "    def whiten(text: str) -> str:\n",
       "        \"\"\"`whiten` strips empty lines because the `markdown.BlockLexer` doesn't like that.\"\"\"\n",
       "        return '\\n'.join(x.rstrip() for x in text.splitlines())"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    def load_ipython_extension(shell):\n",
    "        shell.input_transformer_manager = shell.tangle = pidgyTransformer()        \n",
    "    \n",
    "    def unload_ipython_extension(shell):\n",
    "        shell.input_transformer_manager = __import__('IPython').core.inputtransformer2.TransformerManager()\n",
    "    \n",
    "    (FENCE, CONTINUATION, SEMI, COLON, MAGIC, DOCTEST), QUOTES, SPACE ='``` \\\\ ; : %% >>>'.split(), ('\"\"\"', \"'''\"), ' '\n",
    "    WHITESPACE = re.compile('^\\s*', re.MULTILINE)\n",
    "\n",
    "    def num_first_indent(text):\n",
    "        for str in text.splitlines():\n",
    "            if str.strip(): return len(str) - len(str.lstrip())\n",
    "        return 0\n",
    "    \n",
    "    def num_last_indent(text):\n",
    "        for str in reversed(text.splitlines()):\n",
    "            if str.strip(): return len(str) - len(str.lstrip())\n",
    "        return 0\n",
    "\n",
    "    def base_indent(tokens):\n",
    "        \"Look ahead for the base indent.\"\n",
    "        for i, token in enumerate(tokens):\n",
    "            if token['type'] == 'code':\n",
    "                code = token['text']\n",
    "                if code.lstrip().startswith(FENCE): continue\n",
    "                indent = num_first_indent(code)\n",
    "                break\n",
    "        else: indent = 4\n",
    "        return indent\n",
    "\n",
    "    def quote(text):\n",
    "        \"\"\"wrap text in `QUOTES`\"\"\"\n",
    "        if text.strip():\n",
    "            left, right = len(text)-len(text.lstrip()), len(text.rstrip())\n",
    "            quote = QUOTES[(text[right-1] in QUOTES[0]) or (QUOTES[0] in text)]\n",
    "            return text[:left] + quote + text[left:right] + quote + text[right:]\n",
    "        return text    \n",
    "\n",
    "    def num_whitespace(text): return len(text) - len(text.lstrip())\n",
    "    \n",
    "    def whiten(text: str) -> str:\n",
    "        \"\"\"`whiten` strips empty lines because the `markdown.BlockLexer` doesn't like that.\"\"\"\n",
    "        return '\\n'.join(x.rstrip() for x in text.splitlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</summary></details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pidgy 3",
   "language": "python",
   "name": "pidgy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
