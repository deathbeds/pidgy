{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interactive computing as a medium for modeling ideas as computational literature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# `pidgy` programming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<!--\n",
       "    \n",
       "    import pidgy, pathlib, nbconvert\n",
       "\n",
       "    load = lambda x, level=1: demote(pathlib.Path(x.__file__).read_text(), level)\n",
       "    demote = lambda x, i: ''.join(\n",
       "        '#'*i + x if x.startswith('#') else x for x in x.splitlines(True)\n",
       "    )\n",
       "\n",
       "    def load(x, level=1):\n",
       "        file = getattr(x, '__file__', x)\n",
       "        name = getattr(x, '__name__', x)\n",
       "        object = demote(\n",
       "            pathlib.Path(file).read_text()\n",
       "            if file.endswith('.md')\n",
       "            else nbconvert.get_exporter('markdown')(exclude_input=True).from_filename(file)[0], level) \n",
       "        if object.startswith('---'): fm, sep, object = object.lstrip('---').partition('---')\n",
       "            \n",
       "        object = str.replace(object, '# ', F'# [<code>[source]</code>]({pathlib.Path(file).relative_to(pathlib.Path().absolute())})', 1)\n",
       "        return object\n",
       "    \n",
       "\n",
       "\n",
       "    with pidgy.pidgyLoader():\n",
       "        import pidgy.pytest_config.readme, pidgy.tests.test_pidgin_syntax, pidgy.tests.test_basic\n",
       "        import docs, docs.best_practices\n",
       "\n",
       "-->"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    \n",
    "    import pidgy, pathlib, nbconvert\n",
    "\n",
    "    load = lambda x, level=1: demote(pathlib.Path(x.__file__).read_text(), level)\n",
    "    demote = lambda x, i: ''.join(\n",
    "        '#'*i + x if x.startswith('#') else x for x in x.splitlines(True)\n",
    "    )\n",
    "\n",
    "    def load(x, level=1):\n",
    "        file = getattr(x, '__file__', x)\n",
    "        name = getattr(x, '__name__', x)\n",
    "        object = demote(\n",
    "            pathlib.Path(file).read_text()\n",
    "            if file.endswith('.md')\n",
    "            else nbconvert.get_exporter('markdown')(exclude_input=True).from_filename(file)[0], level) \n",
    "        if object.startswith('---'): fm, sep, object = object.lstrip('---').partition('---')\n",
    "            \n",
    "        object = str.replace(object, '# ', F'# [<code>[source]</code>]({pathlib.Path(file).relative_to(pathlib.Path().absolute())})', 1)\n",
    "        return object\n",
    "    \n",
    "\n",
    "\n",
    "    with pidgy.pidgyLoader():\n",
    "        import pidgy.pytest_config.readme, pidgy.tests.test_pidgin_syntax, pidgy.tests.test_basic\n",
    "        import docs, docs.best_practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Most of those who have heard the term [Literate Programming] think of an approach to document."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Most of those who have heard the term [Literate Programming] think of an approach to document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "toc-hr-collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## [<code>[source]</code>](docs/readme.md)Abstract\n",
       "\n",
       "`pidgy` presents a fun and expressive interactive literate programming approach\n",
       "for computational literature, that is also a valid programs.\n",
       "A literate program is implicitly multilingual, a document formatting language\n",
       "and programming language are defined as the substrate for the\n",
       "literate programming language.\n",
       "\n",
       "The original 1979 implementation defined the [WEB] metalanguage\n",
       "of [Latex] and [Pascal]. `pidgy` is modern and interactive\n",
       "take on [Literate Programming] that uses [Markdown] and [Python]\n",
       "as the respective document and programming languages,\n",
       "of course we'll add some other bits and bobs.\n",
       "\n",
       "This conceptual work treats the program as literature and literature\n",
       "as programs. The result of the `pidgy` implementation is an interactive programming\n",
       "experience where authors design and program simultaneously in [Markdown].\n",
       "An effective literate programming will use machine logic to supplement\n",
       "human logic to explain a program program.\n",
       "If the document is a valid module (ie. it can restart and run all),\n",
       "the literate programs can be imported as [Python] modules\n",
       "then used as terminal applications, web applications,\n",
       "formal testing object, or APIs. All the while, the program\n",
       "itself is a readable work of literature as html, pdf.\n",
       "\n",
       "`pidgy` is written as a literate program using [Markdown]\n",
       "and [Python].\n",
       "Throughout this document we'll discuss\n",
       "the applications and methods behind the `pidgy`\n",
       "and what it takes to implement a [Literate Programming]\n",
       "interface in `IPython`.\n",
       "\n",
       "## Topics\n",
       "\n",
       "- Literate Programming\n",
       "- Computational Notebooks\n",
       "- Markdown\n",
       "- Python\n",
       "- Jupyter\n",
       "- IPython\n",
       "\n",
       "## Author\n",
       "\n",
       "[Tony Fast]\n",
       "\n",
       "<!--\n",
       "\n",
       "    import __init__ as paper\n",
       "    import nbconvert, pathlib, click\n",
       "    file = pathlib.Path(locals().get('__file__', 'readme.md')).parent / 'index.ipynb'\n",
       "\n",
       "    @click.group()\n",
       "    def application(): ...\n",
       "\n",
       "    @application.command()\n",
       "    def build():\n",
       "        to = file.with_suffix('.html')\n",
       "        to.write_text(\n",
       "            nbconvert.get_exporter('html')(\n",
       "                exclude_input=True).from_filename(\n",
       "                    str(file))[0])\n",
       "        click.echo(F'Built {to}')\n",
       "    import subprocess\n",
       "\n",
       "\n",
       "    @application.command()\n",
       "    @click.argument('files', nargs=-1)\n",
       "    def push(files):\n",
       "        click.echo(__import__('subprocess').check_output(\n",
       "                F\"gist -u 2947b4bb582e193f5b2a7dbf8b009b62\".split() + list(files)))\n",
       "\n",
       "    if __name__ == '__main__':\n",
       "        application() if '__file__' in locals() else application.callback()\n",
       "\n",
       "\n",
       "-->\n",
       "\n",
       "[tony fast]: #\n",
       "[markdown]: #\n",
       "[python]: #\n",
       "[jupyter]: #\n",
       "[ipython]: #\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "{{load(docs.readme)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "toc-hr-collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## [<code>[source]</code>](docs/best-practices.md)Best practices for literate programming\n",
       "\n",
       "The first obligation of the literate programmer, defined by [Donald Knuth](ie.\n",
       "the prophet of _[Literate Programming]_), is a core moral commitment to write\n",
       "literate programs, because:\n",
       "\n",
       "> ...; surely nobody wants to admit writing an illiterate program.\n",
       ">\n",
       "> > - [Donald Knuth] _[Literate Programming]_\n",
       "\n",
       "The following best practices for literate programming have emerged while\n",
       "desiging `pidgy`.\n",
       "\n",
       "### List of best practices\n",
       "\n",
       "- Restart and run all or it didn't happen.\n",
       "\n",
       "  A document should be literate in all readable, reproducible, and reusable\n",
       "  contexts.\n",
       "\n",
       "- When in doubt, abide [Web Content Accessibility Guidelines][wcag] so that\n",
       "  information can be accessed by differently abled audiences.\n",
       "\n",
       "- [Markdown] documents are sufficient for single units of thought.\n",
       "\n",
       "  Markdown documents that translate to python can encode literate programs in a\n",
       "  form that is better if version control systems that the `json` format that\n",
       "  encodes notebooks.\n",
       "\n",
       "- All code should compute.\n",
       "\n",
       "  Testing code in a narrative provides supplemental meaning to the `\"code\"`\n",
       "  signifiers. They provide a test of veracity at least for the computational\n",
       "  literacy.\n",
       "\n",
       "- [`readme.md`] is a good default name for a program.\n",
       "\n",
       "  Eventually authors will compose [`\"readme.md\"`] documents that act as both the\n",
       "  `\"__init__\"` method and `\"__main__\"` methods of the program.\n",
       "\n",
       "- Each document should stand alone,\n",
       "  [despite all possibilities to fall.](http://ing.univaq.it/continenza/Corso%20di%20Disegno%20dell'Architettura%202/TESTI%20D'AUTORE/Paul-klee-Pedagogical-Sketchbook.pdf#page=6)\n",
       "- Use code, data, and visualization to fill the voids of natural language.\n",
       "- Find pleasure in writing.\n",
       "\n",
       "[wcag]: https://www.w3.org/WAI/standards-guidelines/wcag/\n",
       "[donald knuth]: #\n",
       "[literate programming]: #\n",
       "[markdown]: #\n",
       "[`readme.md`]: #\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "{{load(docs.best_practices)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "## [<code>[source]</code>](docs/intro.md)Introduction to literate programs and computable essays.\n",
       "\n",
       "> I believe that the time is ripe for significantly better documentation of\n",
       "> programs, and that we can best achieve this by considering programs to be\n",
       "> works of literature.\n",
       ">\n",
       "> > [Donald Knuth]\n",
       "\n",
       "<!--The introduction should be written as a stand alone essay.-->\n",
       "<!--\n",
       "\n",
       "    try: from . import figures\n",
       "    except: ...\n",
       "\n",
       "-->\n",
       "\n",
       "\"[Literate programming]\" is a pioneering paper published by [Donald Knuth] in 1979. It\n",
       "describes a multiobjective, multilingual style of programming that treats programs\n",
       "primarily as documentation. Literate programs have measures along two dimensions:\n",
       "\n",
       "1. the literary qualities determined the document formatting language.\n",
       "2. the computational qualities determined by the programming language.\n",
       "\n",
       "The multilingual nature of literate program creates the opportunity\n",
       "for programmers and non-programmers to contribute to the same literature.\n",
       "\n",
       "Literate programs accept `\"code\"` as an integral part of the narrative.\n",
       "`\"code\"` signs can be used in places where language lacks just as figures and equations are used in scientific literature.\n",
       "An advantage of `\"code\"` is that it can provide augmented representations\n",
       "of documents and their symbols that are tactile and interactive.\n",
       "\n",
       "![Tangle Weave Diagram]({{tangle_weave_diagram}})\n",
       "\n",
       "The literate program concurrently describes a program and literature.\n",
       "Within the document, natural language and the programming language interact\n",
       "through two different process:\n",
       "\n",
       "1. the tangle process that converts to the programming language.\n",
       "2. the weave process that converts to the document formatting language.\n",
       "\n",
       "The original WEB literate programming implementation chose to tangle to Pascal and weave to Tex. `pidgy`'s modern take on literate programming tangles to [Python] and weaves to [Markdown], and they can be written in either [Markdown] files or `jupyter` `notebooks`.\n",
       "\n",
       "[Pascal] was originally chosen for its widespread use throughout education,\n",
       "and the same can be said for the choice of `jupyter` `notebook`s used\n",
       "for education in many programming languages, but most commonly [Python].\n",
       "The preferred document language for the `notebook` is [Markdown]\n",
       "considering it is part of the notebook schema.\n",
       "CP4E\n",
       "The motivations made the natural choice for a [Markdown] and [Python]\n",
       "programming lanuage.\n",
       "Some advantages of this hybrid are that Python is idiomatic and\n",
       "sometimes the narrative may be explicitly executable.\n",
       "\n",
       "[Literate Programming] is alive in places like [Org mode for Emacs], [RMarkdown], [Pweave], [Doctest], or [Literate Coffeescript].\n",
       "A conventional look at literate programming will place a focus on the final document. `pidgy` meanwhile places a focus on the interactive literate computing steps required achieve a quality document.\n",
       "\n",
       "Originally, `pidgy` was designed specifically for the `notebook` file format, but it failed a constraint\n",
       "of not being an existing file.\n",
       "Now `pidgy` is native for [Markdown] files, and valid testing units.\n",
       "It turns out the [Markdown] documents can provide\n",
       "a most compact representation of literate program,\n",
       "relative to a notebook. And it diffs better.\n",
       "\n",
       "Design constraints:\n",
       "\n",
       "- Use an existing file formats.\n",
       "- Minimal bespoke syntax.\n",
       "- Importable and testable\n",
       "\n",
       "A last take on this work is to affirm the reproducibly of enthusiasm when writing literate programs.\n",
       "\n",
       "The outcome of writing `pidgy` programs are readable, reusable, and reproducible\n",
       "documents.  \n",
       "`pidgy` natively supports importing markdown and notebooks as source code.\n",
       "\n",
       "Modern computing has different pieces of software infrastructure than were\n",
       "available\n",
       "\n",
       "[literate programming]: #\n",
       "[donald knuth]: #\n",
       "[literate coffeescript]: #\n",
       "[org mode for emacs]: #\n",
       "[jupyter notebooks]: #\n",
       "[rmarkdown]: #\n",
       "[doctest]: #\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "{{load(docs.intro)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "toc-hr-collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "## [<code>[source]</code>](pidgy/extension.md)The `pidgy` extension for [Markdown][literate programming]\n",
       "\n",
       "The pidgy implementation is successful because of the existing\n",
       "shell configuration system provide by the [`IPython`].\n",
       "[`IPython`] is an industry standard for interactive python programming,\n",
       "and provided the substrate for the first [`IPython`] and later [`jupyter`]\n",
       "notebook implementations. This unit specifically configurations the\n",
       "high-level names we'll refer to when extending `pidgy` including the tangle and weave steps in literate computing.\n",
       "\n",
       "<!--excerpt-->\n",
       "<!--\n",
       "\n",
       "    import jupyter, notebook, IPython, mistune as markdown, IPython as python, ast, jinja2 as template, importnb, doctest, pathlib\n",
       "    with importnb.Notebook(lazy=True):\n",
       "        try: from . import loader, tangle, extras\n",
       "        except: import loader, tangle, extras\n",
       "    with loader.pidgyLoader(lazy=True):\n",
       "        try: from . import weave, testing, metadata\n",
       "        except: import weave, testing, metadata\n",
       "\n",
       "-->\n",
       "\n",
       "There are two approaches to extending the `jupyter` experience:\n",
       "\n",
       "1. Write custom jupyter extensions in python and javascript. (eg.[lab extensions], `IPython` widgets)\n",
       "2. Use the existing configurable objects to modify behaviors in python. (eg. any jupyter kernel)\n",
       "\n",
       "`pidgy` takes the second approach as it builds a [Markdown]-forward REPL interface. Frequently, the `load_ipython_extension` method reappears frequently in this work. This function is used by `IPython` to recognize modifications made by modules to the interactive shell. The `\"load_ext reload_ext unload_ext\"` line magics used commonly by other tools creating features for interactive computing. Demonstrated in the following, the `load_ipython_extension` recieves the current `IPython.InteractiveShell` as an argument to be configured.\n",
       "\n",
       "    def load_ipython_extension(shell: IPython.InteractiveShell) -> None:\n",
       "\n",
       "The `extension` module aggregates the extensions that were designed for `pidgy`.\n",
       "Currently, `pidgy` defines 6 extensions to produce the enhanced literate programming experience. Each module configures isoluted components of the `IPython.InteractiveShell`.\n",
       "\n",
       "        [object.load_ipython_extension(shell) for object in (\n",
       "            loader, tangle, extras, metadata, testing, weave\n",
       "        )]\n",
       "    ...\n",
       "\n",
       "- `loader` ensures the ability to important python, markdown, and notebook documents as reusable modules.\n",
       "- `tangle` defines the heuristics for translating [Markdown] to [Python].\n",
       "- `extras` introduces experimental syntaxes specific to `pidgy`.\n",
       "- `metadata` retains information as the shell and kernel interact with each other.\n",
       "- `testing` adds unittest and doctest capabilities to each cell execution.\n",
       "- `weave` defines a [Markdown] forward display system that templates and displays the input.\n",
       "\n",
       "<!--\n",
       "\n",
       "    def unload_ipython_extension(shell):\n",
       "\n",
       "`unload_ipython_extension` unloads all the extensions loads in `load_ipython_extension`.\n",
       "\n",
       "        for x in (weave, testing, extras, metadata, tangle):\n",
       "            x.unload_ipython_extension(shell)\n",
       "\n",
       "-->\n",
       "\n",
       "[markdown]: #\n",
       "[literate programming]: #\n",
       "[`ipython`]: #\n",
       "[`jupyter`]: #\n",
       "[kernels]: https://github.com/jupyter/jupyter/wiki/Jupyter-kernels\n",
       "[`ipython` extensions]: https://ipython.readthedocs.io/en/stable/config/extensions/\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "{{load(pidgy.extension)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### [<code>[source]</code>](pidgy/events.md)The `IPython` step during a [Read-Eval-Print-Loop] iteration.\n",
       "\n",
       "> 44. Sometimes I think the only universal in the computing field is the fetch-execute cycle.\n",
       ">     >\n",
       "\n",
       "During a fetch-execute cycle in interactive computing, a [Read-Eval-Print-Loop] (ie. REPL) application transmits input to a compiler that returns a representative display for the source. `IPython` is [Read-Eval-Print-Loop] application for interactive python programming. It is a product of the scientific computing that\n",
       "required the ability interact with code to gain insight about information.\n",
       "\n",
       "`IPython` is superset of [Python], it provides custom syntaxes (eg. magics, system calls). `IPython` designed a configurable interface that can customize the input source before executing a command.\n",
       "\n",
       "<!--\n",
       "\n",
       "    import datetime, dataclasses, sys, IPython as python, IPython, nbconvert as export, collections, IPython as python, mistune as markdown, hashlib, functools, hashlib, jinja2.meta, ast\n",
       "    exporter, shell = export.exporters.TemplateExporter(), python.get_ipython()\n",
       "    modules = lambda:[x for x in sys.modules if '.' not in x and not str.startswith(x,'_')]\n",
       "\n",
       "-->\n",
       "\n",
       "    @dataclasses.dataclass\n",
       "    class Events:\n",
       "\n",
       "The `Events` class is a configurable `dataclasses` object that simplifies\n",
       "configuring code execution and metadata collection during interactive computing\n",
       "sessions.\n",
       "There are a few note-worthy events that `IPython` identifies.\n",
       "\n",
       "        _events = \"pre_execute pre_run_cell post_execute post_run_cell\".split()\n",
       "        shell: IPython.InteractiveShell = dataclasses.field(default_factory=IPython.get_ipython)\n",
       "\n",
       "        def register(self, shell=None, *, method=''):\n",
       "\n",
       "`Events.register`s the object as an `IPython` extension, it mimics the interface for the `load_ipython_extension` and `unload_ipython_extension` methods.\n",
       "\n",
       "shell = shell or self.shell\n",
       "\n",
       "            for event in self._events:\n",
       "                callable = getattr(self, event, None)\n",
       "                callable and getattr(self.shell.events, F'{method}register')(event, callable)\n",
       "            if isinstance(self, ast.NodeTransformer):\n",
       "                if method:\n",
       "\n",
       "`ast.NodeTransformers` can be used to intercept parsed [Python] code and apply changes before compilations. If the `Events` object\n",
       "is an `ast.NodeTransfromer` then it is registered on the current shell.\n",
       "\n",
       "                    self.shell.ast_transformers.pop(self.shell.ast_transformers.index(self))\n",
       "                else:\n",
       "                    self.shell.ast_transformers.append(self)\n",
       "\n",
       "            return self\n",
       "\n",
       "<!--\n",
       "\n",
       "        unregister = functools.partialmethod(register, method='un')\n",
       "\n",
       "-->\n",
       "\n",
       "[read-eval-print-loop]: #\n",
       "[perlisisms]: https://www.cs.yale.edu/homes/perlis-alan/quotes.html\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "{{load(pidgy.events, 2)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## [<code>[source]</code>](pidgy/tests/test_basic.md.ipynb)A description of the pidgy metalanguage\n",
       "\n",
       "When combined together, the pidgy extensions form the [Markdown]-forward [Literate Programming]\n",
       "environment.\n",
       "\n",
       "\n",
       "\n",
       "### Everything is markdown\n",
       "\n",
       "\n",
       "\n",
       "#### Naming markdown blocks.\n",
       "\n",
       "pidgy was designed so that [Python] objects can consume [Markdown].\n",
       "[Markdown] content can interact with code in a few ways.\n",
       "* named variables\n",
       "* doctests\n",
       "\n",
       "#### Wrapping units of markdown.\n",
       "\n",
       "\n",
       "\n",
       "### Transclusing data into the display.\n",
       "\n",
       "\n",
       "\n",
       "### Interactively testing code.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "{{load(pidgy.tests.test_basic)}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### [<code>[source]</code>](pidgy/loader.ipynb)Importing and reusing `pidgy` literature\n",
       "\n",
       "A constraint consistent across most programming languages is that\n",
       "programs are executed line-by-line without any\n",
       "statements or expressions. raising exceptions \n",
       "If literate programs have the computational quality that they __restart\n",
       "and run all__ the they should \n",
       "When `pidgy` programs have this quality they can <code>import</code> in [Python], they become importable essays or reports.\n",
       "\n",
       "<!--\n",
       "\n",
       "\n",
       "    __all__ = 'pidgyLoader',\n",
       "    import pidgy, sys, IPython, mistune as markdown, importnb, IPython as python\n",
       "    with importnb.Notebook(lazy=True):\n",
       "        try: from . import tangle, extras\n",
       "        except: import tangle, extras\n",
       "    if __name__ == '__main__':\n",
       "        shell = get_ipython()\n",
       "\n",
       "\n",
       "-->\n",
       "\n",
       "The `pidgyLoader` customizes [Python]'s ability to discover \n",
       "[Markdown] and `pidgy` [Notebook]s have the composite `\".md.ipynb\"` extension.\n",
       "`importnb` provides a high level API for modifying how content\n",
       "[Python] imports different file types.\n",
       "\n",
       "`sys.meta_path and sys.path_hooks`\n",
       "\n",
       "\n",
       "    class pidgyLoader(importnb.Notebook): \n",
       "        extensions = \".md .md.ipynb\".split()\n",
       "\n",
       "\n",
       "`get_data` determines how a file is decoding from disk.  We use it to make an escape hatch for markdown files otherwise we are importing a notebook.\n",
       "\n",
       "\n",
       "    def get_data(self, path):\n",
       "        if self.path.endswith('.md'):\n",
       "            return self.code(self.decode())\n",
       "        return super(pidgyLoader, self).get_data(path)\n",
       "\n",
       "\n",
       "The `code` method tangles the [Markdown] to [Python] before compiling to an [Abstract Syntax Tree].\n",
       "\n",
       "\n",
       "    def code(self, str): \n",
       "        with importnb.Notebook(lazy=True):\n",
       "            try: from . import tangle\n",
       "            except: import tangle\n",
       "        return ''.join(tangle.pidgy.transform_cell(str))\n",
       "\n",
       "\n",
       "The `visit` method allows custom [Abstract Syntax Tree] transformations to be applied.\n",
       "\n",
       "\n",
       "        def visit(self, node):\n",
       "            with importnb.Notebook():\n",
       "                try: from . import tangle\n",
       "                except: import tangle\n",
       "            return tangle.ReturnYield().visit(node)\n",
       "        \n",
       "\n",
       "\n",
       "Attach these methods to the `pidgy` loader.\n",
       "\n",
       "\n",
       "    pidgyLoader.code, pidgyLoader.visit = code, visit\n",
       "    pidgyLoader.get_source = pidgyLoader.get_data = get_data\n",
       "\n",
       "\n",
       "The `pidgy` `loader` configures how [Python] discovers modules when they are\n",
       "imported.\n",
       "Usually the loader is used as a content manager and in this case we hold the enter \n",
       "the context, but do not leave it until `unload_ipython_extension` is executed.\n",
       "\n",
       "\n",
       "    def load_ipython_extension(shell):\n",
       "        setattr(shell, 'loaders', getattr(shell, 'loaders', {}))\n",
       "        shell.loaders[pidgyLoader] = pidgyLoader(position=-1, lazy=True)\n",
       "        shell.loaders[pidgyLoader].__enter__()\n",
       "\n",
       "\n",
       "<!--\n",
       "\n",
       "-->\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "{{load(pidgy.loader, 2)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### [<code>[source]</code>](pidgy/pytest_config/readme.md)Literature as the test\n",
       "\n",
       "    import pidgy, pytest, nbval, doctest, importnb.utils.pytest_importnb\n",
       "\n",
       "Literate documents can be motivated by the need to test a concept. In a fact, a common\n",
       "use case of notebooks is that they interactively test units of thought. Often the thought\n",
       "of reusability is an after thought.\n",
       "\n",
       "`pidgy` documents are meant to be treated as test objects. In fact, the `pidgy` test suite\n",
       "executed by `pytest` through [Github Actions][actions] uses `pidgy` notebooks (ie. documents with the `\".md\" or \".md.ipynb\"` extension). `pidgy` supplies its own `pytest` extensions, and it uses [`nbval`][nbval] and the `pytest`\"--doctest-modules\"`flag. With these conditions we discover pytest conventions, unitests, doctests, and options cell input output validated. Ultimately,`pidgy` documents may represent units of literate that double as formal test objects.\n",
       "\n",
       "The document accessed by the `\"pytest11\"` console_script and includes the extension with a pytest runner.\n",
       "\n",
       "    class pidgyModule(importnb.utils.pytest_importnb.NotebookModule):\n",
       "\n",
       "The `pidgyModule` derives from an existing `pytest` extension that extracts formal tests from `notebook`s\n",
       "as if they were regular python files. We'll use the `pidgy.pidgyLoader` to load Markdown-forward documents\n",
       "as python objects.\n",
       "\n",
       "        loader = pidgy.pidgyLoader\n",
       "\n",
       "    class pidgyTests(importnb.utils.pytest_importnb.NotebookTests):\n",
       "\n",
       "`pidgyTests` makes sure to include the alternative source formats to tangle to python executions.\n",
       "\n",
       "        modules = pidgyModule,\n",
       "\n",
       "[nbval]: https://github.com/computationalmodelling/nbval/ \"The pidgy kernel works directly with `nbval`.\"\n",
       "[actions]: https://github.com/deathbeds/pidgy/runs/478462971\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "{{load(pidgy.pytest_config.readme, 2)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### [<code>[source]</code>](pidgy/readme.md)`\"readme.md\"` is a good name for a file.\n",
       "\n",
       "> [**Eat Me, Drink Me, Read Me.**][readme history]\n",
       "\n",
       "In `pidgy`, the `\"readme.md\"` is treated as the description and implementation\n",
       "of the `__main__` program. The code below outlines the `pidgy` command line\n",
       "application to reuse literate `pidgy` documents in `markdown` and `notebook`\n",
       "files. It outlines how static `pidgy` documents may be reused outside of the\n",
       "interactive context.\n",
       "\n",
       "<!--excerpt-->\n",
       "\n",
       "    ...\n",
       "\n",
       "<!--\n",
       "\n",
       "    import click, IPython, pidgy, nbconvert, pathlib, re\n",
       "\n",
       "-->\n",
       "\n",
       "    @click.group()\n",
       "    def application()->None:\n",
       "\n",
       "The `pidgy` `application` will group together a few commands that can view,\n",
       "execute, and test pidgy documents.\n",
       "\n",
       "<!---->\n",
       "\n",
       "#### `\"pidgy run\"` literature as code\n",
       "\n",
       "    @application.command(context_settings=dict(allow_extra_args=True))\n",
       "    @click.option('--verbose/--quiet', default=True)\n",
       "    @click.argument('ref', type=click.STRING)\n",
       "    @click.pass_context\n",
       "    def run(ctx, ref, verbose):\n",
       "\n",
       "`pidgy` `run` makes it possible to execute `pidgy` documents as programs, and\n",
       "view their pubished results.\n",
       "\n",
       "        import pidgy, importnb, runpy, sys, importlib, jinja2\n",
       "        comment = re.compile(r'(?s:<!--.*?-->)')\n",
       "        absolute = str(pathlib.Path().absolute())\n",
       "        sys.path = ['.'] + sys.path\n",
       "        with pidgy.pidgyLoader(main=True), importnb.Notebook(main=True):\n",
       "            click.echo(F\"Running {ref}.\")\n",
       "            sys.argv, argv = [ref] + ctx.args, sys.argv\n",
       "            try:\n",
       "                if pathlib.Path(ref).exists():\n",
       "                    for ext in \".py .ipynb .md\".split(): ref = ref[:-len(ext)] if ref[-len(ext):] == ext else ref\n",
       "                if ref in sys.modules:\n",
       "                    with pidgy.pidgyLoader(): # cant reload main\n",
       "                        object = importlib.reload(importlib.import_module(ref))\n",
       "                else: object = importlib.import_module(ref)\n",
       "                if verbose:\n",
       "                    md = (nbconvert.get_exporter('markdown')(\n",
       "                        exclude_output=object.__file__.endswith('.md.ipynb')).from_filename(object.__file__)[0]\n",
       "                            if object.__file__.endswith('.ipynb')\n",
       "                            else pathlib.Path(object.__file__).read_text())\n",
       "                    md = re.sub(comment, '', md)\n",
       "                    click.echo(\n",
       "                        jinja2.Template(md).render(vars(object)))\n",
       "            finally: sys.argv = argv\n",
       "\n",
       "<!---->\n",
       "\n",
       "#### Test `pidgy` documents in pytest.\n",
       "\n",
       "    @application.command(context_settings=dict(allow_extra_args=True))\n",
       "    @click.argument('files', nargs=-1, type=click.STRING)\n",
       "    @click.pass_context\n",
       "    def test(ctx, files):\n",
       "\n",
       "Formally test markdown documents, notebooks, and python files.\n",
       "\n",
       "         import pytest\n",
       "         pytest.main(ctx.args+['--doctest-modules', '--disable-pytest-warnings']+list(files))\n",
       "\n",
       "<!---->\n",
       "\n",
       "#### Install `pidgy` as a known kernel.\n",
       "\n",
       "    @application.group()\n",
       "    def kernel():\n",
       "\n",
       "`pidgy` is mainly designed to improve the interactive experience of creating\n",
       "literature in computational notebooks.\n",
       "\n",
       "<!---->\n",
       "\n",
       "    @kernel.command()\n",
       "    def install(user=False, replace=None, prefix=None):\n",
       "\n",
       "`install` the pidgy kernel.\n",
       "\n",
       "        manager = __import__('jupyter_client').kernelspec.KernelSpecManager()\n",
       "        path = str((pathlib.Path(__file__).parent / 'kernelspec').absolute())\n",
       "        try:\n",
       "            dest = manager.install_kernel_spec(path, 'pidgy')\n",
       "        except:\n",
       "            click.echo(F\"System install was unsuccessful. Attempting to install the pidgy kernel to the user.\")\n",
       "            dest = manager.install_kernel_spec(path, 'pidgy', True)\n",
       "        click.echo(F\"The pidgy kernel was install in {dest}\")\n",
       "\n",
       "<!--\n",
       "\n",
       "    @kernel.command()\n",
       "    def uninstall(user=True, replace=None, prefix=None):\n",
       "\n",
       "`uninstall` the kernel.\n",
       "\n",
       "        import jupyter_client\n",
       "        jupyter_client.kernelspec.KernelSpecManager().remove_kernel_spec('pidgy')\n",
       "        click.echo(F\"The pidgy kernel was removed.\")\n",
       "\n",
       "\n",
       "    @kernel.command()\n",
       "    @click.option('-f')\n",
       "    def start(user=True, replace=None, prefix=None, f=None):\n",
       "\n",
       "Launch a `pidgy` kernel applications.\n",
       "\n",
       "        import ipykernel.kernelapp\n",
       "        with pidgy.pidgyLoader():\n",
       "            from . import kernel\n",
       "        ipykernel.kernelapp.IPKernelApp.launch_instance(\n",
       "            kernel_class=kernel.pidgyKernel)\n",
       "    ...\n",
       "\n",
       "-->\n",
       "\n",
       "[art of the readme]: https://github.com/noffle/art-of-readme\n",
       "[readme history]: https://medium.com/@NSomar/readme-md-history-and-components-a365aff07f10\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "{{load(pidgy.readme, 2)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "toc-hr-collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### [<code>[source]</code>](pidgy/kernel.md)Configuring the `pidgy` shell and kernel architecture.\n",
       "\n",
       "![](https://jupyter.readthedocs.io/en/latest/_images/other_kernels.png)\n",
       "\n",
       "Interactive programming in `pidgy` documents is accessed using the polyglot\n",
       "[Jupyter] kernel architecture. In fact, the provenance the [Jupyter]\n",
       "name is a combination the native kernel architectures for\n",
       "[ju~~lia~~][julia], [pyt~~hon~~][python], and [r]. [Jupyter]'s\n",
       "generalization of the kernel/shell interface allows\n",
       "over 100 languages to be used in `notebook and jupyterlab`.\n",
       "It is possible to define prescribe wrapper kernels around existing\n",
       "methods; this is the appraoach that `pidgy` takes\n",
       "\n",
       "> A kernel provides programming language support in Jupyter. IPython is the default kernel. Additional kernels include R, Julia, and many more.\n",
       ">\n",
       "> > - [`jupyter` kernel definition](https://jupyter.readthedocs.io/en/latest/glossary.html#term-kernel)\n",
       "\n",
       "`pidgy` is not not a native kernel. It is a wrapper kernel around the\n",
       "existing `ipykernel and IPython.InteractiveShell` configurables.\n",
       "`IPython` adds extra syntax to python that simulate literate programming\n",
       "macros.\n",
       "\n",
       "<!--\n",
       "\n",
       "    import jupyter_client, IPython, ipykernel.ipkernel, ipykernel.kernelapp, pidgy, traitlets, pidgy, traitlets, ipykernel.kernelspec, ipykernel.zmqshell, pathlib, traitlets\n",
       "\n",
       "-->\n",
       "\n",
       "The shell is the application either jupyterlab or jupyter notebook, the kernel\n",
       "determines the programming language. Below we design a just jupyter kernel that\n",
       "can be installed using\n",
       "\n",
       "- What is the advantage of installing the kernel and how to do it.\n",
       "\n",
       "```bash\n",
       "pidgy kernel install\n",
       "```\n",
       "\n",
       "#### Configure the `pidgy` shell.\n",
       "\n",
       "    class pidgyInteractiveShell(ipykernel.zmqshell.ZMQInteractiveShell):\n",
       "\n",
       "Configure a native `pidgy` `IPython.InteractiveShell`\n",
       "\n",
       "        loaders = traitlets.Dict(allow_none=True)\n",
       "        weave = traitlets.Any(allow_none=True)\n",
       "        tangle = ipykernel.zmqshell.ZMQInteractiveShell.input_transformer_manager\n",
       "        extras = traitlets.Any(allow_none=True)\n",
       "        testing = traitlets.Any(allow_none=True)\n",
       "        enable_html_pager = traitlets.Bool(True)\n",
       "\n",
       "`pidgyInteractiveShell.enable_html_pager` is necessary to see rich displays in\n",
       "the inspector.\n",
       "\n",
       "        def __init__(self,*args, **kwargs):\n",
       "            super().__init__(*args, **kwargs)\n",
       "            with pidgy.pidgyLoader():\n",
       "                from .extension import load_ipython_extension\n",
       "            load_ipython_extension(self)\n",
       "\n",
       "#### Configure the `pidgy` kernel.\n",
       "\n",
       "    class pidgyKernel(ipykernel.ipkernel.IPythonKernel):\n",
       "        shell_class = traitlets.Type(pidgyInteractiveShell)\n",
       "        _last_parent = traitlets.Dict()\n",
       "\n",
       "        def init_metadata(self, parent):\n",
       "            self._last_parent = parent\n",
       "            return super().init_metadata(parent)\n",
       "\n",
       "\n",
       "        def do_inspect(self, code, cursor_pos, detail_level=0):\n",
       "\n",
       "<details><summary>Customizing the Jupyter inspector behavior for literate computing</summary><p>\n",
       "When we have access to the kernel class it is possible to customize\n",
       "a number of interactive shell features.   The do inspect function\n",
       "adds some features to `jupyter`'s  inspection behavior when working in \n",
       "`pidgy`.\n",
       "</p><pre></code>\n",
       "\n",
       "            object = {'found': False}\n",
       "            if code[:cursor_pos][-3:] == '!!!':\n",
       "                object = {'found': True, 'data': {'text/markdown': self.shell.weave.format_markdown(code[:cursor_pos-3]+code[cursor_pos:])}}\n",
       "            else:\n",
       "                try:\n",
       "                    object = super().do_inspect(code, cursor_pos, detail_level=0)\n",
       "                except: ...\n",
       "\n",
       "            if not object['found']:\n",
       "\n",
       "Simulate finding an object and return a preview of the markdown.\n",
       "\n",
       "                object['found'] = True\n",
       "                line, offset = IPython.utils.tokenutil.line_at_cursor(code, cursor_pos)\n",
       "                lead = code[:cursor_pos]\n",
       "                col = cursor_pos - offset\n",
       "\n",
       "\n",
       "                code = F\"\"\"<code>·L{\n",
       "                    len(lead.splitlines()) + int(not(col))\n",
       "                },C{col + 1}</code><br/>\\n\\n\"\"\" + code[:cursor_pos]+'·'+('' if col else '<br/>\\n')+code[cursor_pos:]\n",
       "\n",
       "                object['data'] = {'text/markdown': code}\n",
       "\n",
       "We include the line number and cursor position to enrich the connection between\n",
       "the inspector and the source code displayed on another part of the screen.\n",
       "\n",
       "            return object\n",
       "        ...\n",
       "\n",
       "</details>\n",
       "\n",
       "#### `pidgy`-like interfaces in other languages.\n",
       "\n",
       "[julia]: #\n",
       "[r]: #\n",
       "[python]: #\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "{{load(pidgy.kernel, 2)}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "toc-hr-collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### [<code>[source]</code>](pidgy/tangle.ipynb)Tangling [Markdown] to [Python]\n",
       "\n",
       "The `tangle` step is the keystone of `pidgy` by defining the\n",
       "heuristics that translate [Markdown] to [Python] execute\n",
       "blocks of narrative as interactive code, and entire programs.\n",
       "A key constraint in the translation is a line-for-line mapping\n",
       "between representations, with this we'll benefit from reusable \n",
       "tracebacks for [Markdown] source.\n",
       "\n",
       "There are many ways to translate [Markdown] to other formats specifically with tools\n",
       "like `\"pandoc\"`.  The formats are document formatting language, and not programs.\n",
       "The [Markdown] to [Python] translation adds a computable dimension to the document.\n",
       "`pidgy` is one implementation and it should be possible to apply to different heuristics to other\n",
       "programming languages.\n",
       "\n",
       "\n",
       "<!--\n",
       "    \n",
       "    import IPython, typing as τ, mistune as markdown, IPython, importnb as _import_, textwrap, ast, doctest, typing, re, dataclasses\n",
       "    if __name__ == '__main__':\n",
       "        import pidgy\n",
       "        shell = IPython.get_ipython()\n",
       "\n",
       "-->\n",
       "\n",
       "\n",
       "The `pidgyTransformer` manages the high level API the `IPython.InteractiveShell` interacts with for `pidgy`.\n",
       "The `IPython.core.inputtransformer2.TransformerManager` is a configurable class for modifying\n",
       "input source to before it passes to the compiler.  It is the object that introduces `IPython`s line\n",
       "and cell magics.\n",
       "\n",
       "    >>> assert isinstance(shell.input_transformer_manager, IPython.core.inputtransformer2.TransformerManager)\n",
       "    \n",
       "This configurable class has three different flavors of transformations.\n",
       "\n",
       "* `shell.input_transformer_manager.cleanup_transforms`\n",
       "* `shell.input_transformer_manager.line_transforms`\n",
       "* `shell.input_transformer_manager.token_transformers`\n",
       "\n",
       "\n",
       "    class pidgyTransformer(IPython.core.inputtransformer2.TransformerManager):\n",
       "        def pidgy_transform(self, cell: str) -> str: \n",
       "            return self.tokenizer.untokenize(self.tokenizer.parse(''.join(cell)))\n",
       "        \n",
       "        def transform_cell(self, cell):\n",
       "            return super().transform_cell(self.pidgy_transform(cell))\n",
       "        \n",
       "        def __init__(self, *args, **kwargs):\n",
       "            super().__init__(*args, **kwargs)\n",
       "            self.tokenizer = Tokenizer()\n",
       "\n",
       "        def pidgy_magic(self, *text): \n",
       "            return IPython.display.Code(self.pidgy_transform(''.join(text)), language='python')\n",
       "\n",
       "\n",
       "#### Block level lexical analysis.\n",
       "\n",
       "Translating [Markdown] to [Python] rely only on block level objects in the [Markdown]\n",
       "grammar.  The `BlockLexer` is a modified analyzer that adds logic to include `doctest` \n",
       "blocks in the grammar.\n",
       "\n",
       "\n",
       "    class BlockLexer(markdown.BlockLexer):\n",
       "        class grammar_class(markdown.BlockGrammar):\n",
       "            doctest = doctest.DocTestParser._EXAMPLE_RE\n",
       "            block_code = re.compile(r'^((?!\\s+>>>\\s) {4}[^\\n]+\\n*)+')\n",
       "            default_rules = \"newline hrule block_code fences heading nptable lheading block_quote list_block def_links def_footnotes table paragraph text\".split()\n",
       "\n",
       "        def parse_doctest(self, m): self.tokens.append({'type': 'paragraph', 'text': m.group(0)})\n",
       "\n",
       "        def parse_fences(self, m):\n",
       "            if m.group(2): self.tokens.append({'type': 'paragraph', 'text': m.group(0)})\n",
       "            else: super().parse_fences(m)\n",
       "\n",
       "        def parse_hrule(self, m): self.tokens.append(dict(type='hrule', text=m.group(0)))\n",
       "            \n",
       "        def parse_def_links(self, m):\n",
       "            super().parse_def_links(m)\n",
       "            self.tokens.append(dict(type='def_link', text=m.group(0)))\n",
       "            \n",
       "        def parse(self, text: str, default_rules=None, normalize=True) -> typing.List[dict]:\n",
       "            if not self.depth: self.tokens = []\n",
       "            with self: tokens = super().parse(whiten(text), default_rules)\n",
       "            if normalize and not self.depth: tokens = self.normalize(text, tokens)\n",
       "            return tokens\n",
       "        \n",
       "        depth = 0\n",
       "        def __enter__(self): self.depth += 1\n",
       "        def __exit__(self, *e): self.depth -= 1\n",
       "\n",
       "\n",
       "The `doctest` token is identified before the block code.\n",
       "\n",
       "\n",
       "<!--\n",
       "    \n",
       "    for x in \"default_rules footnote_rules list_rules\".split():\n",
       "        setattr(BlockLexer, x, list(getattr(BlockLexer, x)))\n",
       "        getattr(BlockLexer, x).insert(getattr(BlockLexer, x).index('block_code'), 'doctest')\n",
       "        if 'block_html' in getattr(BlockLexer, x):\n",
       "            getattr(BlockLexer, x).pop(getattr(BlockLexer, x).index('block_html'))\n",
       "\n",
       "\n",
       "-->\n",
       "\n",
       "\n",
       "Our translation creates tokens specific to each [Markdown] rule, \n",
       "for code it is only necessary to identify code and paragraph tokens.\n",
       "The normalizer compacts tokens into the necessary tokens.\n",
       "\n",
       "\n",
       "    class Normalizer(BlockLexer):\n",
       "        def normalize(self, text, tokens):\n",
       "            \"\"\"Combine non-code tokens into contiguous blocks.\"\"\"\n",
       "            compacted = []\n",
       "            while tokens:\n",
       "                token = tokens.pop(0)\n",
       "                if 'text' not in token: continue\n",
       "                else: \n",
       "                    if not token['text'].strip(): continue\n",
       "                    block, body = token['text'].splitlines(), \"\"\n",
       "                while block:\n",
       "                    line = block.pop(0)\n",
       "                    if line:\n",
       "                        before, line, text = text.partition(line)\n",
       "                        body += before + line\n",
       "                if token['type']=='code':\n",
       "                    compacted.append({'type': 'code', 'lang': None, 'text': body})\n",
       "                else:\n",
       "                    if compacted and compacted[-1]['type'] == 'paragraph':\n",
       "                        compacted[-1]['text'] += body\n",
       "                    else: compacted.append({'type': 'paragraph', 'text': body})\n",
       "            if compacted and compacted[-1]['type'] == 'paragraph':\n",
       "                compacted[-1]['text'] += text\n",
       "            elif text.strip():\n",
       "                compacted.append({'type': 'paragraph', 'text': text})\n",
       "            # Deal with front matter\n",
       "            if compacted[0]['text'].startswith('---\\n') and '\\n---' in compacted[0]['text'][4:]:\n",
       "                token = compacted.pop(0)\n",
       "                front_matter, sep, paragraph = token['text'][4:].partition('---')\n",
       "                compacted = [{'type': 'front_matter', 'text': F\"\\n{front_matter}\"},\n",
       "                            {'type': 'paragraph', 'text': paragraph}] + compacted\n",
       "            return compacted\n",
       "\n",
       "\n",
       "#### Tokenizer logic\n",
       "\n",
       "The tokenizer controls the translation of markdown strings to python strings.  Our major constraint is that the Markdown input should retain line numbers.\n",
       "\n",
       "\n",
       "    class Tokenizer(Normalizer):\n",
       "        def untokenize(self, tokens: τ.List[dict], source: str = \"\"\"\"\"\", last: int =0) -> str:\n",
       "            INDENT = indent = base_indent(tokens) or 4\n",
       "            for i, token in enumerate(tokens):\n",
       "                object = token['text']\n",
       "                if token and token['type'] == 'code':\n",
       "                    if object.lstrip().startswith(FENCE):\n",
       "\n",
       "                        object = ''.join(''.join(object.partition(FENCE)[::2]).rpartition(FENCE)[::2])\n",
       "                        indent = INDENT + num_first_indent(object)\n",
       "                        object = textwrap.indent(object, INDENT*SPACE)\n",
       "\n",
       "                    if object.lstrip().startswith(MAGIC):  ...\n",
       "                    else: indent = num_last_indent(object)\n",
       "                elif token and token['type'] == 'front_matter': \n",
       "                    object = textwrap.indent(\n",
       "                        F\"locals().update(__import__('yaml').safe_load({quote(object)}))\\n\", indent*SPACE)\n",
       "\n",
       "                elif not object: ...\n",
       "                else:\n",
       "                    object = textwrap.indent(object, SPACE*max(indent-num_first_indent(object), 0))\n",
       "                    for next in tokens[i+1:]:\n",
       "                        if next['type'] == 'code':\n",
       "                            next = num_first_indent(next['text'])\n",
       "                            break\n",
       "                    else: next = indent       \n",
       "                    Δ = max(next-indent, 0)\n",
       "\n",
       "                    if not Δ and source.rstrip().rstrip(CONTINUATION).endswith(COLON): \n",
       "                        Δ += 4\n",
       "\n",
       "                    spaces = num_whitespace(object)\n",
       "                    \"what if the spaces are ling enough\"\n",
       "                    object = object[:spaces] + Δ*SPACE+ object[spaces:]\n",
       "                    if not source.rstrip().rstrip(CONTINUATION).endswith(QUOTES): \n",
       "                        object = quote(object)\n",
       "                source += object\n",
       "\n",
       "            # add a semicolon to the source if the last block is code.\n",
       "            for token in reversed(tokens):\n",
       "                if token['text'].strip():\n",
       "                    if token['type'] != 'code': \n",
       "                        source = source.rstrip() + SEMI\n",
       "                    break\n",
       "\n",
       "            return source\n",
       "            \n",
       "    pidgy = pidgyTransformer()\n",
       "\n",
       "\n",
       "<details><summary>Utility functions for the tangle module</summary>\n",
       "\n",
       "\n",
       "    def load_ipython_extension(shell):\n",
       "        shell.input_transformer_manager = shell.tangle = pidgyTransformer()        \n",
       "    \n",
       "    def unload_ipython_extension(shell):\n",
       "        shell.input_transformer_manager = __import__('IPython').core.inputtransformer2.TransformerManager()\n",
       "    \n",
       "    (FENCE, CONTINUATION, SEMI, COLON, MAGIC, DOCTEST), QUOTES, SPACE ='``` \\\\ ; : %% >>>'.split(), ('\"\"\"', \"'''\"), ' '\n",
       "    WHITESPACE = re.compile('^\\s*', re.MULTILINE)\n",
       "\n",
       "    def num_first_indent(text):\n",
       "        for str in text.splitlines():\n",
       "            if str.strip(): return len(str) - len(str.lstrip())\n",
       "        return 0\n",
       "    \n",
       "    def num_last_indent(text):\n",
       "        for str in reversed(text.splitlines()):\n",
       "            if str.strip(): return len(str) - len(str.lstrip())\n",
       "        return 0\n",
       "\n",
       "    def base_indent(tokens):\n",
       "        \"Look ahead for the base indent.\"\n",
       "        for i, token in enumerate(tokens):\n",
       "            if token['type'] == 'code':\n",
       "                code = token['text']\n",
       "                if code.lstrip().startswith(FENCE): continue\n",
       "                indent = num_first_indent(code)\n",
       "                break\n",
       "        else: indent = 4\n",
       "        return indent\n",
       "\n",
       "    def quote(text):\n",
       "        \"\"\"wrap text in `QUOTES`\"\"\"\n",
       "        if text.strip():\n",
       "            left, right = len(text)-len(text.lstrip()), len(text.rstrip())\n",
       "            quote = QUOTES[(text[right-1] in QUOTES[0]) or (QUOTES[0] in text)]\n",
       "            return text[:left] + quote + text[left:right] + quote + text[right:]\n",
       "        return text    \n",
       "\n",
       "    def num_whitespace(text): return len(text) - len(text.lstrip())\n",
       "    \n",
       "    def whiten(text: str) -> str:\n",
       "        \"\"\"`whiten` strips empty lines because the `markdown.BlockLexer` doesn't like that.\"\"\"\n",
       "        return '\\n'.join(x.rstrip() for x in text.splitlines())\n",
       "\n",
       "\n",
       "</summary></details>\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "{{load(pidgy.tangle, 2)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### [<code>[source]</code>](pidgy/extras.ipynb)Extra langauge features of `pidgy`\n",
       "\n",
       "`pidgy` experiments extra language features for python, using the same system\n",
       "that IPython uses to add features like line and cell magics.\n",
       "\n",
       "<!--\n",
       "\n",
       "\n",
       "    import IPython, typing as τ, mistune as markdown, IPython, importnb as _import_, textwrap, ast, doctest, typing, re\n",
       "    import dataclasses, ast, pidgy\n",
       "    with pidgy.pidgyLoader(lazy=True):\n",
       "        try: from . import events\n",
       "        except: import events\n",
       "\n",
       "\n",
       "-->\n",
       "\n",
       "##### naming variables with gestures.\n",
       "\n",
       "We know naming is hard, there is no point focusing on it. `pidgy` allows authors\n",
       "to use emojis as variables in python. They add extra color and expression to the narrative.\n",
       "\n",
       "\n",
       "    def demojize(lines, delimiters=('_', '_')):\n",
       "        str = ''.join(lines)\n",
       "        import tokenize, emoji, stringcase; tokens = []\n",
       "        try:\n",
       "            for token in list(tokenize.tokenize(\n",
       "                __import__('io').BytesIO(str.encode()).readline)):\n",
       "                if token.type == tokenize.ERRORTOKEN:\n",
       "                    string = emoji.demojize(token.string, delimiters=delimiters\n",
       "                                           ).replace('-', '_').replace(\"’\", \"_\")\n",
       "                    if tokens and tokens[-1].type == tokenize.NAME: tokens[-1] = tokenize.TokenInfo(tokens[-1].type, tokens[-1].string + string, tokens[-1].start, tokens[-1].end, tokens[-1].line)\n",
       "                    else: tokens.append(\n",
       "                        tokenize.TokenInfo(\n",
       "                            tokenize.NAME, string, token.start, token.end, token.line))\n",
       "                else: tokens.append(token)\n",
       "            return tokenize.untokenize(tokens).decode().splitlines(True)\n",
       "        except BaseException: raise SyntaxError(str)\n",
       "\n",
       "\n",
       "##### Top level return and yield statements.\n",
       "\n",
       "<!--\n",
       "\n",
       "\n",
       "    def unload_ipython_extension(shell):\n",
       "        shell.extras.unregister()\n",
       "\n",
       "\n",
       "-->\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "{{load(pidgy.extras, 3)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "toc-hr-collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### [<code>[source]</code>](pidgy/weave.md)Weaving cells in pidgin programs\n",
       "\n",
       "<!--\n",
       "\n",
       "    import datetime, dataclasses, sys, IPython as python, IPython, nbconvert as export, collections, IPython as python, mistune as markdown, hashlib, functools, hashlib, jinja2.meta, pidgy\n",
       "    exporter, shell = export.exporters.TemplateExporter(), python.get_ipython()\n",
       "    modules = lambda:[x for x in sys.modules if '.' not in x and not str.startswith(x,'_')]\n",
       "    with pidgy.pidgyLoader(lazy=True):\n",
       "        try:\n",
       "            from . import events\n",
       "        except:\n",
       "            import events\n",
       "\n",
       "\n",
       "-->\n",
       "\n",
       "pidgin programming is an incremental approach to documents.\n",
       "\n",
       "    def load_ipython_extension(shell):\n",
       "        shell.display_formatter.formatters['text/markdown'].for_type(str, lambda x: x)\n",
       "        shell.weave = Weave(shell=shell)\n",
       "        shell.weave.register()\n",
       "\n",
       "    @dataclasses.dataclass\n",
       "    class Weave(events.Events):\n",
       "        shell: IPython.InteractiveShell = dataclasses.field(default_factory=IPython.get_ipython)\n",
       "        environment: jinja2.Environment = dataclasses.field(default=exporter.environment)\n",
       "        _null_environment = jinja2.Environment()\n",
       "\n",
       "        def format_markdown(self, text):\n",
       "            try:\n",
       "                template = exporter.environment.from_string(text, globals=getattr(self.shell, 'user_ns', {}))\n",
       "                text = template.render()\n",
       "            except BaseException as Exception:\n",
       "                self.shell.showtraceback((type(Exception), Exception, Exception.__traceback__))\n",
       "            return text\n",
       "\n",
       "        def format_metadata(self):\n",
       "            parent = getattr(self.shell.kernel, '_last_parent', {})\n",
       "            return {}\n",
       "\n",
       "        def _update_filters(self):\n",
       "            self.environment.filters.update({\n",
       "                k: v for k, v in getattr(self.shell, 'user_ns', {}).items() if callable(v) and k not in self.environment.filters})\n",
       "\n",
       "\n",
       "        def post_run_cell(self, result):\n",
       "            text = strip_front_matter(result.info.raw_cell)\n",
       "            lines = text.splitlines() or ['']\n",
       "            IPython.display.display(IPython.display.Markdown(\n",
       "                self.format_markdown(text) if lines[0].strip() else F\"\"\"<!--\\n{text}\\n\\n-->\"\"\", metadata=self.format_metadata())\n",
       "            )\n",
       "            return result\n",
       "\n",
       "    def unload_ipython_extension(shell):\n",
       "        try:\n",
       "            shell.weave.unregister()\n",
       "        except:...\n",
       "\n",
       "    def strip_front_matter(text):\n",
       "        if text.startswith('---\\n'):\n",
       "            front_matter, sep, rest = text[4:].partition(\"\\n---\")\n",
       "            if sep: return ''.join(rest.splitlines(True)[1:])\n",
       "        return text\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "{{load(pidgy.weave, 2)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "toc-hr-collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### [<code>[source]</code>](pidgy/testing.md)Interactive testing of literate programs\n",
       "\n",
       "Testing is something we added because of the application of notebooks as test units.\n",
       "\n",
       "A primary use case of notebooks is to test ideas. Typically this in informally using\n",
       "manual validation to qualify the efficacy of narrative and code. To ensure testable literate documents\n",
       "we formally test code incrementally during interactive computing.\n",
       "\n",
       "<!--\n",
       "\n",
       "    import unittest, doctest, textwrap, dataclasses, IPython, re, pidgy, sys, typing, types, contextlib, ast, inspect\n",
       "    with pidgy.pidgyLoader(lazy=True):\n",
       "        try: from . import events\n",
       "        except: import events\n",
       "\n",
       "-->\n",
       "\n",
       "    def make_test_suite(*objects: typing.Union[\n",
       "        unittest.TestCase, types.FunctionType, str\n",
       "    ], vars, name) -> unittest.TestSuite:\n",
       "\n",
       "The interactive testing suite execute `doctest and unittest` conventions\n",
       "for a flexible interface to verifying the computational qualities of literate programs.\n",
       "\n",
       "        suite, doctest_suite = unittest.TestSuite(), doctest.DocTestSuite()\n",
       "        suite.addTest(doctest_suite)\n",
       "        for object in objects:\n",
       "            if isinstance(object, type) and issubclass(object, unittest.TestCase):\n",
       "                suite.addTest(unittest.defaultTestLoader.loadTestsFromTestCase(object))\n",
       "            elif isinstance(object, str):\n",
       "                doctest_suite.addTest(doctest.DocTestCase(\n",
       "                doctest.DocTestParser().get_doctest(object, vars, name, name, 1), doctest.ELLIPSIS))\n",
       "                doctest_suite.addTest(doctest.DocTestCase(\n",
       "                InlineDoctestParser().get_doctest(object, vars, name, name, 1), checker=NullOutputCheck))\n",
       "            elif inspect.isfunction(object):\n",
       "                suite.addTest(unittest.FunctionTestCase(object))\n",
       "        return suite\n",
       "\n",
       "    @dataclasses.dataclass\n",
       "    class Testing(events.Events):\n",
       "\n",
       "The `Testing` class executes the test suite each time a cell is executed.\n",
       "\n",
       "        function_pattern: str = 'test_'\n",
       "        def post_run_cell(self, result):\n",
       "            globs, filename = self.shell.user_ns, F\"In[{self.shell.last_execution_result.execution_count}]\"\n",
       "\n",
       "            if not (result.error_before_exec or result.error_in_exec):\n",
       "                with ipython_compiler(self.shell):\n",
       "                    definitions = [self.shell.user_ns[x] for x in getattr(self.shell.metadata, 'definitions', [])\n",
       "                        if x.startswith(self.function_pattern) or\n",
       "                        (isinstance(self.shell.user_ns[x], type)\n",
       "                         and issubclass(self.shell.user_ns[x], unittest.TestCase))\n",
       "                    ]\n",
       "                    result = self.run(make_test_suite(result.info.raw_cell, *definitions, vars=self.shell.user_ns, name=filename), result)\n",
       "\n",
       "\n",
       "        def run(self, suite: unittest.TestCase, cell) -> unittest.TestResult:\n",
       "            result = unittest.TestResult(); suite.run(result)\n",
       "            if result.failures:\n",
       "                msg = '\\n'.join(msg for text, msg in result.failures)\n",
       "                msg = re.sub(re.compile(\"<ipython-input-[0-9]+-\\S+>\"), F'In[{cell.execution_count}]', clean_doctest_traceback(msg))\n",
       "                sys.stderr.writelines((str(result) + '\\n' + msg).splitlines(True))\n",
       "                return result\n",
       "\n",
       "    @contextlib.contextmanager\n",
       "    def ipython_compiler(shell):\n",
       "\n",
       "We'll have to replace how `doctest` compiles code with the `IPython` machinery.\n",
       "\n",
       "        def compiler(input, filename, symbol, *args, **kwargs):\n",
       "            nonlocal shell\n",
       "            return shell.compile(\n",
       "                ast.Interactive(\n",
       "                    body=shell.transform_ast(\n",
       "                    shell.compile.ast_parse(shell.transform_cell(textwrap.indent(input, ' '*4)))\n",
       "                ).body),\n",
       "                F\"In[{shell.last_execution_result.execution_count}]\",\n",
       "                \"single\",\n",
       "            )\n",
       "\n",
       "        yield setattr(doctest, \"compile\", compiler)\n",
       "        doctest.compile = compile\n",
       "\n",
       "    def clean_doctest_traceback(str, *lines):\n",
       "        str = re.sub(re.compile(\"\"\"\\n\\s+File [\\s\\S]+, line [0-9]+, in runTest\\s+raise[\\s\\S]+\\([\\s\\S]+\\)\\n?\"\"\"), '\\n', str)\n",
       "        return re.sub(re.compile(\"Traceback \\(most recent call last\\):\\n\"), '', str)\n",
       "\n",
       "<details><summary>Utilities for the testing module.</summary>\n",
       "    \n",
       "    class NullOutputCheck(doctest.OutputChecker):\n",
       "        def check_output(self, *e): return True\n",
       "\n",
       "    class InlineDoctestParser(doctest.DocTestParser):\n",
       "        _EXAMPLE_RE = re.compile(r'`(?P<indent>\\s{0})'\n",
       "    r'(?P<source>[^`].*?)'\n",
       "    r'`')\n",
       "        def _parse_example(self, m, name, lineno): return m.group('source'), None, \"...\", None\n",
       "\n",
       "\n",
       "    def load_ipython_extension(shell):\n",
       "        shell.testing = Testing(shell=shell).register()\n",
       "\n",
       "    def unload_ipython_extension(shell):\n",
       "        shell.testing.unregister()\n",
       "\n",
       "</details>\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "{{load(pidgy.testing, 2)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### [<code>[source]</code>](pidgy/metadata.md)Capturing metadata during the interactive compute process\n",
       "\n",
       "To an organization, human compute time bears an important cost\n",
       "and programming represents a small part of that cycle.\n",
       "\n",
       "    def load_ipython_extension(shell):\n",
       "\n",
       "The `metadata` module assists in collecting metadata about the interactive compute process.\n",
       "It appends the metadata atrribute to the shell.\n",
       "\n",
       "        shell.metadata = Metadata(shell=shell).register()\n",
       "\n",
       "<!--\n",
       "\n",
       "    import dataclasses, ast, pidgy\n",
       "    with pidgy.pidgyLoader(lazy=True):\n",
       "        try: from . import events\n",
       "        except: import events\n",
       "\n",
       "-->\n",
       "\n",
       "    @dataclasses.dataclass\n",
       "    class Metadata(events.Events, ast.NodeTransformer):\n",
       "        definitions: list = dataclasses.field(default_factory=list)\n",
       "        def pre_execute(self):\n",
       "            self.definitions = []\n",
       "\n",
       "        def visit_FunctionDef(self, node):\n",
       "            self.definitions.append(node.name)\n",
       "            return node\n",
       "\n",
       "        visit_ClassDef = visit_FunctionDef\n",
       "\n",
       "<!--\n",
       "\n",
       "    def unload_ipython_extension(shell):\n",
       "        shell.metadata.unregister()\n",
       "\n",
       "-->\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "{{load(pidgy.metadata, 2)}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{load('readme.md')}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # NBVAL_SKIP\n",
    "\n",
    "    \n",
    "    if __name__ == '__main__' and not '__file__' in globals():\n",
    "        !jupyter nbconvert --to rst --stdout --TemplateExporter.exclude_input=True index.md.ipynb > docs/index.rst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pidgy 3",
   "language": "python",
   "name": "pidgy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
