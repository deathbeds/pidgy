{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interactive computing as a medium for modeling ideas as computational literature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# `pidgy` programming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<!--\n",
       "    \n",
       "    import pidgy, pathlib, nbconvert\n",
       "\n",
       "    load = lambda x, level=1: demote(pathlib.Path(x.__file__).read_text(), level)\n",
       "    demote = lambda x, i: ''.join(\n",
       "        '#'*i + x if x.startswith('#') else x for x in x.splitlines(True)\n",
       "    )\n",
       "\n",
       "    def load(x, level=1):\n",
       "        file = getattr(x, '__file__', x)\n",
       "        name = getattr(x, '__name__', x)\n",
       "        object = demote(\n",
       "            pathlib.Path(file).read_text()\n",
       "            if file.endswith('.md')\n",
       "            else nbconvert.get_exporter('markdown')(exclude_input=True).from_filename(file)[0], level) \n",
       "        if object.startswith('---'): fm, sep, object = object.lstrip('---').partition('---')\n",
       "            \n",
       "        object = str.replace(object, '# ', F'# [<code>[source]</code>]({pathlib.Path(file).relative_to(pathlib.Path().absolute())})', 1)\n",
       "        return object\n",
       "    \n",
       "\n",
       "\n",
       "    with pidgy.pidgyLoader():\n",
       "        import pidgy.pytest_config.readme, pidgy.tests.test_pidgin_syntax, pidgy.tests.test_basic\n",
       "        import docs.readme\n",
       "\n",
       "-->"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    \n",
    "    import pidgy, pathlib, nbconvert\n",
    "\n",
    "    load = lambda x, level=1: demote(pathlib.Path(x.__file__).read_text(), level)\n",
    "    demote = lambda x, i: ''.join(\n",
    "        '#'*i + x if x.startswith('#') else x for x in x.splitlines(True)\n",
    "    )\n",
    "\n",
    "    def load(x, level=1):\n",
    "        file = getattr(x, '__file__', x)\n",
    "        name = getattr(x, '__name__', x)\n",
    "        object = demote(\n",
    "            pathlib.Path(file).read_text()\n",
    "            if file.endswith('.md')\n",
    "            else nbconvert.get_exporter('markdown')(exclude_input=True).from_filename(file)[0], level) \n",
    "        if object.startswith('---'): fm, sep, object = object.lstrip('---').partition('---')\n",
    "            \n",
    "        object = str.replace(object, '# ', F'# [<code>[source]</code>]({pathlib.Path(file).relative_to(pathlib.Path().absolute())})', 1)\n",
    "        return object\n",
    "    \n",
    "\n",
    "\n",
    "    with pidgy.pidgyLoader():\n",
    "        import pidgy.pytest_config.readme, pidgy.tests.test_pidgin_syntax, pidgy.tests.test_basic\n",
    "        import docs.readme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "toc-hr-collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## [<code>[source]</code>](docs/readme.md)Abstract\n",
       "\n",
       "`pidgy` presents a fun and expressive interactive literate programming approach\n",
       "for computational literature, that is also a valid programs.\n",
       "A literate program is implicitly multilingual, a document formatting language\n",
       "and programming language are defined as the substrate for the\n",
       "literate programming language.\n",
       "\n",
       "The original 1979 implementation defined the [WEB] metalanguage\n",
       "of [Latex] and [Pascal]. `pidgy` is modern and interactive\n",
       "take on [Literate Programming] that uses [Markdown] and [Python]\n",
       "as the respective document and programming languages,\n",
       "of course we'll add some other bits and bobs.\n",
       "\n",
       "This conceptual work treats the program as literature and literature\n",
       "as programs. The result of the `pidgy` implementation is an interactive programming\n",
       "experience where authors design and program simultaneously in [Markdown].\n",
       "An effective literate programming will use machine logic to supplement\n",
       "human logic to explain a program program.\n",
       "If the document is a valid module (ie. it can restart and run all),\n",
       "the literate programs can be imported as [Python] modules\n",
       "then used as terminal applications, web applications,\n",
       "formal testing object, or APIs. All the while, the program\n",
       "itself is a readable work of literature as html, pdf.\n",
       "\n",
       "`pidgy` is written as a literate program using [Markdown]\n",
       "and [Python].\n",
       "Throughout this document we'll discuss\n",
       "the applications and methods behind the `pidgy`\n",
       "and what it takes to implement a [Literate Programming]\n",
       "interface in `IPython`.\n",
       "\n",
       "## Topics\n",
       "\n",
       "- Literate Programming\n",
       "- Computational Notebooks\n",
       "- Markdown\n",
       "- Python\n",
       "- Jupyter\n",
       "- IPython\n",
       "\n",
       "## Author\n",
       "\n",
       "[Tony Fast]\n",
       "\n",
       "<!--\n",
       "\n",
       "    import __init__ as paper\n",
       "    import nbconvert, pathlib, click\n",
       "    file = pathlib.Path(locals().get('__file__', 'readme.md')).parent / 'index.ipynb'\n",
       "\n",
       "    @click.group()\n",
       "    def application(): ...\n",
       "\n",
       "    @application.command()\n",
       "    def build():\n",
       "        to = file.with_suffix('.html')\n",
       "        to.write_text(\n",
       "            nbconvert.get_exporter('html')(\n",
       "                exclude_input=True).from_filename(\n",
       "                    str(file))[0])\n",
       "        click.echo(F'Built {to}')\n",
       "    import subprocess\n",
       "\n",
       "\n",
       "    @application.command()\n",
       "    @click.argument('files', nargs=-1)\n",
       "    def push(files):\n",
       "        click.echo(__import__('subprocess').check_output(\n",
       "                F\"gist -u 2947b4bb582e193f5b2a7dbf8b009b62\".split() + list(files)))\n",
       "\n",
       "    if __name__ == '__main__':\n",
       "        application() if '__file__' in locals() else application.callback()\n",
       "\n",
       "\n",
       "-->\n",
       "\n",
       "[tony fast]: #\n",
       "[markdown]: #\n",
       "[python]: #\n",
       "[jupyter]: #\n",
       "[ipython]: #\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "{{load(docs.readme)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## [<code>[source]</code>](docs/intro.md)Introduction\n",
       "\n",
       "[Fernando Perez], creator of [`IPython`], wrote an essay titled [\"Literate computing\" and computational reproducibility]. Here he introduces the term [Literate Computing] that describes a style of workflow where representations of live computation are critical to the understanding of a computational narrative. At the scale of interactive computing, we focus on documenting the computational thinking process using narrative, code, and hypermedia.  Whereas [Literate Programming] considers the enduring literary qualities of the program.  In this work, we discuss the `pidgy` interactive shell that is designed to improve the [Literate Computing] experience so that the outcome are multi-objective readable, reproducible, and reusable [Literate Program]s.\n",
       "\n",
       "![](literate_computing_venn.jpeg)\n",
       "\n",
       "[Literate Programming] and [Literate Computing] shine light on perspectives on computational thinking as [Documentation].  These concerns represent passive and active states of computational document, literature is the resting state and computing is interactive state. In either condition, `pidgy` prefers the use of Markdown as the interface for composing literate programs. While interactive computing, the input represents both a display object and python code.  Interactively writing [Markdown] allows for a fluid intertextuality of `\"code\"`, hyperlinks, and hypermedia in human logic, an implicit outcome is a [Markdown] document that has literary and computational qualities.\n",
       "\n",
       "To remain consistent with [Literate Programming], `pidgy` defines the [Tangle] and [Weave] steps that convert the input [Markdown] to their document and program translations. Often [Literate Computing] is used as an informal test of thinking. `pidgy` codifies interactive testing practices that improve the expectation that a [Literate Program] is reusable or reproducible. The [Weave] step in `pidgy` is a enriched with a [Templating Language] that allows computable objects to be embedded and formatted into [Markdown] source.\n",
       "\n",
       "![](tangle_weave_diagram.svg)\n",
       "\n",
       "The [Tangle] step of literate programming converts translates the documentation language into the programming language.  The original 1979 [`\"WEB\"`][web] implementation chose [$\\TeX{}$][tex] and [PASCAL], and `pidgy` chooses [Markdown] and [Python].  `pidgy` applies line-for-line heuristics that format `not \"code\"` blocks into block strings relative block code objects in the literate input.\n",
       "\n",
       "Before [Weaving] code, `pidgy` executes formal `doctest` and `unittest` discovered by the customized test suite. Testing code interactively helps to ensure that entire documents are reusable as programs. These tests do not hault the program, rather they consider the standard error message to be a feature of readable literature that can communicate exceptions.\n",
       "\n",
       "To [Weave] a document refers to the transforms made to the input as it becomes a readable object. `pidgy` provides the ability include representation of live programming objects directly in the [Markdown] using `jinja2` syntax. [Markdown] can represent literate programs written in many languages. [Markdown] can include HTML and CSS.\n",
       "\n",
       "![](pidgy_literate_computing.jpeg)\n",
       "\n",
       "Throughout this work we'll design a purpose built interactive literate computing interface. This work is interested in designing an interactive experience that results in multi-objective computational documents that are readable, reusable, and reproducible over longer timelines than single use notebooks and programs.\n",
       "\n",
       "The intent of `pidgy` matured as different features began to take form.  Originally, `pidgy` was gungho about [Notebooks] being the primary interface for Literate Programming.  [Notebooks] provide a metastable serialization of the Literate Programming containing both the literate input and the woven hypermedia. And they still serve valid applications for conditions where the input and output are highly dependent on each other.  There are other conditions where we desire to write programmatic literature that is reliably reproducible over a longer timeline.  [Markdown] written in `pidgy` seems to provide a compact input for pythonic literate programs with [Markdown] first. If a program is reproducible, then it is input of its outputs.\n",
       "\n",
       "[\"literate computing\" and computational reproducibility]: http://blog.fperez.org/2013/04/literate-computing-and-computational.html\n",
       "[tools for the life cycle of a computational idea]: https://sinews.siam.org/Details-Page/jupyter-tools-for-the-life-cycle-of-a-computational-idea\n",
       "[tex]: #\n",
       "[web]: #\n",
       "[pascal]: #\n",
       "[markdown]: #\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "{{load(docs.intro)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "toc-hr-collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "## [<code>[source]</code>](pidgy/extension.md)Configuring the [Markdown]-forward interactive shell in `IPython`\n",
       "\n",
       "Open source software and practices shape the way `pidgy` is designed. It relies mainly on foundational tools from the scientific python computing community. The primary base for `pidgy` is the `IPython.InteractiveShell` that expose configurable features that customize the interactive computing experience.  `IPython` is one of the heritage languages developed in into the award winning `jupyter` project.\n",
       "\n",
       "<!--excerpt-->\n",
       "<!--\n",
       "\n",
       "    import jupyter, notebook, IPython, mistune as markdown, IPython as python, ast, jinja2 as template, importnb, doctest, pathlib\n",
       "    with importnb.Notebook(lazy=True):\n",
       "        try: from . import loader, tangle, extras\n",
       "        except: import loader, tangle, extras\n",
       "    with loader.pidgyLoader(lazy=True):\n",
       "        try: from . import weave, testing, metadata\n",
       "        except: import weave, testing, metadata\n",
       "\n",
       "-->\n",
       "\n",
       "Each module in `pidgy` is an `IPython` configuration module that transforms independent aspects of [Literate Computing]. Our extension appends the following abilities:\n",
       "\n",
       "- `loader` ensures the ability to important python, markdown, and notebook documents as reusable modules.\n",
       "- `tangle` defines the heuristics for translating [Markdown] to [Python].\n",
       "- `extras` introduces experimental syntaxes specific to `pidgy`.\n",
       "- `metadata` retains information as the shell and kernel interact with each other.\n",
       "- `testing` adds unittest and doctest capabilities to each cell execution.\n",
       "- `weave` defines a [Markdown] forward display system that templates and displays the input.\n",
       "\n",
       "\n",
       "<details><summary>What are the <code>load_ipython_extension and unload_ipython_extension</code> </summary>\n",
       "`load_ipython_extension and unload_ipython_extension` are used by `IPython` to trigger modifications to the interactive shell by a module. These methods are inovked by the `\"load_ext reload_ext unload_ext\"` line magics. Demonstrated in the following, the `load_ipython_extension` recieves the current `IPython.InteractiveShell` as an argument to be configured.\n",
       "</details>\n",
       "\n",
       "    def load_ipython_extension(shell: IPython.InteractiveShell) -> None:\n",
       "\n",
       "The `extension` module aggregates the extensions that were designed for `pidgy`.\n",
       "Currently, `pidgy` defines 6 extensions to produce the enhanced literate programming experience. Each module configures isoluted components of the `IPython.InteractiveShell`.\n",
       "\n",
       "        [object.load_ipython_extension(shell) for object in (\n",
       "            loader, tangle, extras, metadata, testing, weave\n",
       "        )]\n",
       "    ...\n",
       "\n",
       "\n",
       "<!--\n",
       "\n",
       "    def unload_ipython_extension(shell):\n",
       "\n",
       "`unload_ipython_extension` unloads all the extensions loads in `load_ipython_extension`.\n",
       "\n",
       "        [x.unload_ipython_extension(shell) for x in (loader, weave, testing, extras, metadata, tangle)]\n",
       "            \n",
       "\n",
       "-->\n",
       "\n",
       "[markdown]: #\n",
       "[literate programming]: #\n",
       "[`ipython`]: #\n",
       "[`jupyter`]: #\n",
       "[kernels]: https://github.com/jupyter/jupyter/wiki/Jupyter-kernels\n",
       "[`ipython` extensions]: https://ipython.readthedocs.io/en/stable/config/extensions/\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "{{load(pidgy.extension)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### [<code>[source]</code>](pidgy/events.md)The `IPython` step during a [Read-Eval-Print-Loop] iteration.\n",
       "\n",
       "> 44. Sometimes I think the only universal in the computing field is the fetch-execute cycle.\n",
       ">     >\n",
       "\n",
       "During a fetch-execute cycle in interactive computing, a [Read-Eval-Print-Loop] (ie. REPL) application transmits input to a compiler that returns a representative display for the source. `IPython` is [Read-Eval-Print-Loop] application for interactive python programming. It is a product of the scientific computing that\n",
       "required the ability interact with code to gain insight about information.\n",
       "\n",
       "`IPython` is superset of [Python], it provides custom syntaxes (eg. magics, system calls). `IPython` designed a configurable interface that can customize the input source before executing a command.\n",
       "\n",
       "<!--\n",
       "\n",
       "    import datetime, dataclasses, sys, IPython as python, IPython, nbconvert as export, collections, IPython as python, mistune as markdown, hashlib, functools, hashlib, jinja2.meta, ast\n",
       "    exporter, shell = export.exporters.TemplateExporter(), python.get_ipython()\n",
       "    modules = lambda:[x for x in sys.modules if '.' not in x and not str.startswith(x,'_')]\n",
       "\n",
       "-->\n",
       "\n",
       "    @dataclasses.dataclass\n",
       "    class Events:\n",
       "\n",
       "The `Events` class is a configurable `dataclasses` object that simplifies\n",
       "configuring code execution and metadata collection during interactive computing\n",
       "sessions.\n",
       "There are a few note-worthy events that `IPython` identifies.\n",
       "\n",
       "        _events = \"pre_execute pre_run_cell post_execute post_run_cell\".split()\n",
       "        shell: IPython.InteractiveShell = dataclasses.field(default_factory=IPython.get_ipython)\n",
       "\n",
       "        def register(self, shell=None, *, method=''):\n",
       "\n",
       "`Events.register`s the object as an `IPython` extension, it mimics the interface for the `load_ipython_extension` and `unload_ipython_extension` methods.\n",
       "\n",
       "shell = shell or self.shell\n",
       "\n",
       "            for event in self._events:\n",
       "                callable = getattr(self, event, None)\n",
       "                callable and getattr(self.shell.events, F'{method}register')(event, callable)\n",
       "            if isinstance(self, ast.NodeTransformer):\n",
       "                if method:\n",
       "\n",
       "`ast.NodeTransformers` can be used to intercept parsed [Python] code and apply changes before compilations. If the `Events` object\n",
       "is an `ast.NodeTransfromer` then it is registered on the current shell.\n",
       "\n",
       "                    self.shell.ast_transformers.pop(self.shell.ast_transformers.index(self))\n",
       "                else:\n",
       "                    self.shell.ast_transformers.append(self)\n",
       "\n",
       "            return self\n",
       "\n",
       "<!--\n",
       "\n",
       "        unregister = functools.partialmethod(register, method='un')\n",
       "\n",
       "-->\n",
       "\n",
       "[read-eval-print-loop]: #\n",
       "[perlisisms]: https://www.cs.yale.edu/homes/perlis-alan/quotes.html\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "{{load(pidgy.events, 2)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## [<code>[source]</code>](pidgy/tests/test_basic.md.ipynb)A description of the pidgy metalanguage\n",
       "\n",
       "When combined together, the pidgy extensions form the [Markdown]-forward [Literate Programming]\n",
       "environment.\n",
       "\n",
       "\n",
       "\n",
       "### Everything is markdown\n",
       "\n",
       "\n",
       "\n",
       "#### Naming markdown blocks.\n",
       "\n",
       "pidgy was designed so that [Python] objects can consume [Markdown].\n",
       "[Markdown] content can interact with code in a few ways.\n",
       "* named variables\n",
       "* doctests\n",
       "\n",
       "#### Wrapping units of markdown.\n",
       "\n",
       "\n",
       "\n",
       "### Transclusing data into the display.\n",
       "\n",
       "\n",
       "\n",
       "### Interactively testing code.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "{{load(pidgy.tests.test_basic)}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### [<code>[source]</code>](pidgy/loader.ipynb)Importing and reusing `pidgy` literature\n",
       "\n",
       "A constraint consistent across most programming languages is that\n",
       "programs are executed line-by-line without any\n",
       "statements or expressions. raising exceptions \n",
       "If literate programs have the computational quality that they __restart\n",
       "and run all__ the they should \n",
       "When `pidgy` programs have this quality they can <code>import</code> in [Python], they become importable essays or reports.\n",
       "\n",
       "<!--\n",
       "\n",
       "\n",
       "    __all__ = 'pidgyLoader',\n",
       "    import pidgy, sys, IPython, mistune as markdown, importnb, IPython as python\n",
       "    with importnb.Notebook(lazy=True):\n",
       "        try: from . import tangle, extras\n",
       "        except: import tangle, extras\n",
       "    if __name__ == '__main__':\n",
       "        shell = get_ipython()\n",
       "\n",
       "\n",
       "-->\n",
       "\n",
       "The `pidgyLoader` customizes [Python]'s ability to discover \n",
       "[Markdown] and `pidgy` [Notebook]s have the composite `\".md.ipynb\"` extension.\n",
       "`importnb` provides a high level API for modifying how content\n",
       "[Python] imports different file types.\n",
       "\n",
       "`sys.meta_path and sys.path_hooks`\n",
       "\n",
       "\n",
       "    class pidgyLoader(importnb.Notebook): \n",
       "        extensions = \".md .md.ipynb\".split()\n",
       "\n",
       "\n",
       "`get_data` determines how a file is decoding from disk.  We use it to make an escape hatch for markdown files otherwise we are importing a notebook.\n",
       "\n",
       "\n",
       "    def get_data(self, path):\n",
       "        if self.path.endswith('.md'):\n",
       "            return self.code(self.decode())\n",
       "        return super(pidgyLoader, self).get_data(path)\n",
       "\n",
       "\n",
       "The `code` method tangles the [Markdown] to [Python] before compiling to an [Abstract Syntax Tree].\n",
       "\n",
       "\n",
       "    def code(self, str): \n",
       "        with importnb.Notebook(lazy=True):\n",
       "            try: from . import tangle\n",
       "            except: import tangle\n",
       "        return ''.join(tangle.pidgy.transform_cell(str))\n",
       "\n",
       "\n",
       "The `visit` method allows custom [Abstract Syntax Tree] transformations to be applied.\n",
       "\n",
       "\n",
       "        def visit(self, node):\n",
       "            with importnb.Notebook():\n",
       "                try: from . import tangle\n",
       "                except: import tangle\n",
       "            return tangle.ReturnYield().visit(node)\n",
       "        \n",
       "\n",
       "\n",
       "Attach these methods to the `pidgy` loader.\n",
       "\n",
       "\n",
       "    pidgyLoader.code, pidgyLoader.visit = code, visit\n",
       "    pidgyLoader.get_source = pidgyLoader.get_data = get_data\n",
       "\n",
       "\n",
       "The `pidgy` `loader` configures how [Python] discovers modules when they are\n",
       "imported.\n",
       "Usually the loader is used as a content manager and in this case we hold the enter \n",
       "the context, but do not leave it until `unload_ipython_extension` is executed.\n",
       "\n",
       "\n",
       "    def load_ipython_extension(shell):\n",
       "        setattr(shell, 'loaders', getattr(shell, 'loaders', {}))\n",
       "        shell.loaders[pidgyLoader] = pidgyLoader(position=-1, lazy=True)\n",
       "        shell.loaders[pidgyLoader].__enter__()\n",
       "\n",
       "\n",
       "<!--\n",
       "\n",
       "-->\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "{{load(pidgy.loader, 2)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### [<code>[source]</code>](pidgy/pytest_config/readme.md)Literature as the test\n",
       "\n",
       "    import pidgy, pytest, nbval, doctest, importnb.utils.pytest_importnb\n",
       "\n",
       "Literate documents can be motivated by the need to test a concept. In a fact, a common\n",
       "use case of notebooks is that they interactively test units of thought. Often the thought\n",
       "of reusability is an after thought.\n",
       "\n",
       "`pidgy` documents are meant to be treated as test objects. In fact, the `pidgy` test suite\n",
       "executed by `pytest` through [Github Actions][actions] uses `pidgy` notebooks (ie. documents with the `\".md\" or \".md.ipynb\"` extension). `pidgy` supplies its own `pytest` extensions, and it uses [`nbval`][nbval] and the `pytest`\"--doctest-modules\"`flag. With these conditions we discover pytest conventions, unitests, doctests, and options cell input output validated. Ultimately,`pidgy` documents may represent units of literate that double as formal test objects.\n",
       "\n",
       "The document accessed by the `\"pytest11\"` console_script and includes the extension with a pytest runner.\n",
       "\n",
       "    class pidgyModule(importnb.utils.pytest_importnb.NotebookModule):\n",
       "\n",
       "The `pidgyModule` derives from an existing `pytest` extension that extracts formal tests from `notebook`s\n",
       "as if they were regular python files. We'll use the `pidgy.pidgyLoader` to load Markdown-forward documents\n",
       "as python objects.\n",
       "\n",
       "        loader = pidgy.pidgyLoader\n",
       "\n",
       "    class pidgyTests(importnb.utils.pytest_importnb.NotebookTests):\n",
       "\n",
       "`pidgyTests` makes sure to include the alternative source formats to tangle to python executions.\n",
       "\n",
       "        modules = pidgyModule,\n",
       "\n",
       "[nbval]: https://github.com/computationalmodelling/nbval/ \"The pidgy kernel works directly with `nbval`.\"\n",
       "[actions]: https://github.com/deathbeds/pidgy/runs/478462971\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "{{load(pidgy.pytest_config.readme, 2)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### [<code>[source]</code>](pidgy/readme.md)`\"readme.md\"` is a good name for a file.\n",
       "\n",
       "> [**Eat Me, Drink Me, Read Me.**][readme history]\n",
       "\n",
       "In `pidgy`, the `\"readme.md\"` is treated as the description and implementation\n",
       "of the `__main__` program. The code below outlines the `pidgy` command line\n",
       "application to reuse literate `pidgy` documents in `markdown` and `notebook`\n",
       "files. It outlines how static `pidgy` documents may be reused outside of the\n",
       "interactive context.\n",
       "\n",
       "<!--excerpt-->\n",
       "\n",
       "    ...\n",
       "\n",
       "<!--\n",
       "\n",
       "    import click, IPython, pidgy, nbconvert, pathlib, re\n",
       "\n",
       "-->\n",
       "\n",
       "    @click.group()\n",
       "    def application()->None:\n",
       "\n",
       "The `pidgy` `application` will group together a few commands that can view,\n",
       "execute, and test pidgy documents.\n",
       "\n",
       "<!---->\n",
       "\n",
       "#### `\"pidgy run\"` literature as code\n",
       "\n",
       "    @application.command(context_settings=dict(allow_extra_args=True))\n",
       "    @click.option('--verbose/--quiet', default=True)\n",
       "    @click.argument('ref', type=click.STRING)\n",
       "    @click.pass_context\n",
       "    def run(ctx, ref, verbose):\n",
       "\n",
       "`pidgy` `run` makes it possible to execute `pidgy` documents as programs, and\n",
       "view their pubished results.\n",
       "\n",
       "        import pidgy, importnb, runpy, sys, importlib, jinja2\n",
       "        comment = re.compile(r'(?s:<!--.*?-->)')\n",
       "        absolute = str(pathlib.Path().absolute())\n",
       "        sys.path = ['.'] + sys.path\n",
       "        with pidgy.pidgyLoader(main=True), importnb.Notebook(main=True):\n",
       "            click.echo(F\"Running {ref}.\")\n",
       "            sys.argv, argv = [ref] + ctx.args, sys.argv\n",
       "            try:\n",
       "                if pathlib.Path(ref).exists():\n",
       "                    for ext in \".py .ipynb .md\".split(): ref = ref[:-len(ext)] if ref[-len(ext):] == ext else ref\n",
       "                if ref in sys.modules:\n",
       "                    with pidgy.pidgyLoader(): # cant reload main\n",
       "                        object = importlib.reload(importlib.import_module(ref))\n",
       "                else: object = importlib.import_module(ref)\n",
       "                if verbose:\n",
       "                    md = (nbconvert.get_exporter('markdown')(\n",
       "                        exclude_output=object.__file__.endswith('.md.ipynb')).from_filename(object.__file__)[0]\n",
       "                            if object.__file__.endswith('.ipynb')\n",
       "                            else pathlib.Path(object.__file__).read_text())\n",
       "                    md = re.sub(comment, '', md)\n",
       "                    click.echo(\n",
       "                        jinja2.Template(md).render(vars(object)))\n",
       "            finally: sys.argv = argv\n",
       "\n",
       "<!---->\n",
       "\n",
       "#### Test `pidgy` documents in pytest.\n",
       "\n",
       "    @application.command(context_settings=dict(allow_extra_args=True))\n",
       "    @click.argument('files', nargs=-1, type=click.STRING)\n",
       "    @click.pass_context\n",
       "    def test(ctx, files):\n",
       "\n",
       "Formally test markdown documents, notebooks, and python files.\n",
       "\n",
       "         import pytest\n",
       "         pytest.main(ctx.args+['--doctest-modules', '--disable-pytest-warnings']+list(files))\n",
       "\n",
       "<!---->\n",
       "\n",
       "#### Install `pidgy` as a known kernel.\n",
       "\n",
       "    @application.group()\n",
       "    def kernel():\n",
       "\n",
       "`pidgy` is mainly designed to improve the interactive experience of creating\n",
       "literature in computational notebooks.\n",
       "\n",
       "<!---->\n",
       "\n",
       "    @kernel.command()\n",
       "    def install(user=False, replace=None, prefix=None):\n",
       "\n",
       "`install` the pidgy kernel.\n",
       "\n",
       "        manager = __import__('jupyter_client').kernelspec.KernelSpecManager()\n",
       "        path = str((pathlib.Path(__file__).parent / 'kernelspec').absolute())\n",
       "        try:\n",
       "            dest = manager.install_kernel_spec(path, 'pidgy')\n",
       "        except:\n",
       "            click.echo(F\"System install was unsuccessful. Attempting to install the pidgy kernel to the user.\")\n",
       "            dest = manager.install_kernel_spec(path, 'pidgy', True)\n",
       "        click.echo(F\"The pidgy kernel was install in {dest}\")\n",
       "\n",
       "<!--\n",
       "\n",
       "    @kernel.command()\n",
       "    def uninstall(user=True, replace=None, prefix=None):\n",
       "\n",
       "`uninstall` the kernel.\n",
       "\n",
       "        import jupyter_client\n",
       "        jupyter_client.kernelspec.KernelSpecManager().remove_kernel_spec('pidgy')\n",
       "        click.echo(F\"The pidgy kernel was removed.\")\n",
       "\n",
       "\n",
       "    @kernel.command()\n",
       "    @click.option('-f')\n",
       "    def start(user=True, replace=None, prefix=None, f=None):\n",
       "\n",
       "Launch a `pidgy` kernel applications.\n",
       "\n",
       "        import ipykernel.kernelapp\n",
       "        with pidgy.pidgyLoader():\n",
       "            from . import kernel\n",
       "        ipykernel.kernelapp.IPKernelApp.launch_instance(\n",
       "            kernel_class=kernel.pidgyKernel)\n",
       "    ...\n",
       "\n",
       "-->\n",
       "\n",
       "[art of the readme]: https://github.com/noffle/art-of-readme\n",
       "[readme history]: https://medium.com/@NSomar/readme-md-history-and-components-a365aff07f10\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "{{load(pidgy.readme, 2)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "toc-hr-collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### [<code>[source]</code>](pidgy/kernel.md)Configuring the `pidgy` shell and kernel architecture.\n",
       "\n",
       "![](https://jupyter.readthedocs.io/en/latest/_images/other_kernels.png)\n",
       "\n",
       "Interactive programming in `pidgy` documents is accessed using the polyglot\n",
       "[Jupyter] kernel architecture. In fact, the provenance the [Jupyter]\n",
       "name is a combination the native kernel architectures for\n",
       "[ju~~lia~~][julia], [pyt~~hon~~][python], and [r]. [Jupyter]'s\n",
       "generalization of the kernel/shell interface allows\n",
       "over 100 languages to be used in `notebook and jupyterlab`.\n",
       "It is possible to define prescribe wrapper kernels around existing\n",
       "methods; this is the appraoach that `pidgy` takes\n",
       "\n",
       "> A kernel provides programming language support in Jupyter. IPython is the default kernel. Additional kernels include R, Julia, and many more.\n",
       ">\n",
       "> > - [`jupyter` kernel definition](https://jupyter.readthedocs.io/en/latest/glossary.html#term-kernel)\n",
       "\n",
       "`pidgy` is not not a native kernel. It is a wrapper kernel around the\n",
       "existing `ipykernel and IPython.InteractiveShell` configurables.\n",
       "`IPython` adds extra syntax to python that simulate literate programming\n",
       "macros.\n",
       "\n",
       "<!--\n",
       "\n",
       "    import jupyter_client, IPython, ipykernel.ipkernel, ipykernel.kernelapp, pidgy, traitlets, pidgy, traitlets, ipykernel.kernelspec, ipykernel.zmqshell, pathlib, traitlets\n",
       "\n",
       "-->\n",
       "\n",
       "The shell is the application either jupyterlab or jupyter notebook, the kernel\n",
       "determines the programming language. Below we design a just jupyter kernel that\n",
       "can be installed using\n",
       "\n",
       "- What is the advantage of installing the kernel and how to do it.\n",
       "\n",
       "```bash\n",
       "pidgy kernel install\n",
       "```\n",
       "\n",
       "#### Configure the `pidgy` shell.\n",
       "\n",
       "    class pidgyInteractiveShell(ipykernel.zmqshell.ZMQInteractiveShell):\n",
       "\n",
       "Configure a native `pidgy` `IPython.InteractiveShell`\n",
       "\n",
       "        loaders = traitlets.Dict(allow_none=True)\n",
       "        weave = traitlets.Any(allow_none=True)\n",
       "        tangle = ipykernel.zmqshell.ZMQInteractiveShell.input_transformer_manager\n",
       "        extras = traitlets.Any(allow_none=True)\n",
       "        testing = traitlets.Any(allow_none=True)\n",
       "        enable_html_pager = traitlets.Bool(True)\n",
       "\n",
       "`pidgyInteractiveShell.enable_html_pager` is necessary to see rich displays in\n",
       "the inspector.\n",
       "\n",
       "        def __init__(self,*args, **kwargs):\n",
       "            super().__init__(*args, **kwargs)\n",
       "            with pidgy.pidgyLoader():\n",
       "                from .extension import load_ipython_extension\n",
       "            load_ipython_extension(self)\n",
       "\n",
       "#### Configure the `pidgy` kernel.\n",
       "\n",
       "    class pidgyKernel(ipykernel.ipkernel.IPythonKernel):\n",
       "        shell_class = traitlets.Type(pidgyInteractiveShell)\n",
       "        _last_parent = traitlets.Dict()\n",
       "\n",
       "        def init_metadata(self, parent):\n",
       "            self._last_parent = parent\n",
       "            return super().init_metadata(parent)\n",
       "\n",
       "\n",
       "        def do_inspect(self, code, cursor_pos, detail_level=0):\n",
       "\n",
       "<details><summary>Customizing the Jupyter inspector behavior for literate computing</summary><p>\n",
       "When we have access to the kernel class it is possible to customize\n",
       "a number of interactive shell features.   The do inspect function\n",
       "adds some features to `jupyter`'s  inspection behavior when working in \n",
       "`pidgy`.\n",
       "</p><pre></code>\n",
       "\n",
       "            object = {'found': False}\n",
       "            if code[:cursor_pos][-3:] == '!!!':\n",
       "                object = {'found': True, 'data': {'text/markdown': self.shell.weave.format_markdown(code[:cursor_pos-3]+code[cursor_pos:])}}\n",
       "            else:\n",
       "                try:\n",
       "                    object = super().do_inspect(code, cursor_pos, detail_level=0)\n",
       "                except: ...\n",
       "\n",
       "            if not object['found']:\n",
       "\n",
       "Simulate finding an object and return a preview of the markdown.\n",
       "\n",
       "                object['found'] = True\n",
       "                line, offset = IPython.utils.tokenutil.line_at_cursor(code, cursor_pos)\n",
       "                lead = code[:cursor_pos]\n",
       "                col = cursor_pos - offset\n",
       "\n",
       "\n",
       "                code = F\"\"\"<code>·L{\n",
       "                    len(lead.splitlines()) + int(not(col))\n",
       "                },C{col + 1}</code><br/>\\n\\n\"\"\" + code[:cursor_pos]+'·'+('' if col else '<br/>\\n')+code[cursor_pos:]\n",
       "\n",
       "                object['data'] = {'text/markdown': code}\n",
       "\n",
       "We include the line number and cursor position to enrich the connection between\n",
       "the inspector and the source code displayed on another part of the screen.\n",
       "\n",
       "            return object\n",
       "        ...\n",
       "\n",
       "</details>\n",
       "\n",
       "#### `pidgy`-like interfaces in other languages.\n",
       "\n",
       "[julia]: #\n",
       "[r]: #\n",
       "[python]: #\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "{{load(pidgy.kernel, 2)}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "toc-hr-collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### [<code>[source]</code>](pidgy/tangle.ipynb)Tangling [Markdown] to [Python]\n",
       "\n",
       "The `tangle` step is the keystone of `pidgy` by defining the\n",
       "heuristics that translate [Markdown] to [Python] execute\n",
       "blocks of narrative as interactive code, and entire programs.\n",
       "A key constraint in the translation is a line-for-line mapping\n",
       "between representations, with this we'll benefit from reusable \n",
       "tracebacks for [Markdown] source.\n",
       "\n",
       "There are many ways to translate [Markdown] to other formats specifically with tools\n",
       "like `\"pandoc\"`.  The formats are document formatting language, and not programs.\n",
       "The [Markdown] to [Python] translation adds a computable dimension to the document.\n",
       "`pidgy` is one implementation and it should be possible to apply to different heuristics to other\n",
       "programming languages.\n",
       "\n",
       "\n",
       "<!--\n",
       "    \n",
       "    import IPython, typing as τ, mistune as markdown, IPython, importnb as _import_, textwrap, ast, doctest, typing, re, dataclasses\n",
       "    if __name__ == '__main__':\n",
       "        import pidgy\n",
       "        shell = IPython.get_ipython()\n",
       "\n",
       "-->\n",
       "\n",
       "\n",
       "The `pidgyTransformer` manages the high level API the `IPython.InteractiveShell` interacts with for `pidgy`.\n",
       "The `IPython.core.inputtransformer2.TransformerManager` is a configurable class for modifying\n",
       "input source to before it passes to the compiler.  It is the object that introduces `IPython`s line\n",
       "and cell magics.\n",
       "\n",
       "    >>> assert isinstance(shell.input_transformer_manager, IPython.core.inputtransformer2.TransformerManager)\n",
       "    \n",
       "This configurable class has three different flavors of transformations.\n",
       "\n",
       "* `shell.input_transformer_manager.cleanup_transforms`\n",
       "* `shell.input_transformer_manager.line_transforms`\n",
       "* `shell.input_transformer_manager.token_transformers`\n",
       "\n",
       "\n",
       "    class pidgyTransformer(IPython.core.inputtransformer2.TransformerManager):\n",
       "        def pidgy_transform(self, cell: str) -> str: \n",
       "            return self.tokenizer.untokenize(self.tokenizer.parse(''.join(cell)))\n",
       "        \n",
       "        def transform_cell(self, cell):\n",
       "            return super().transform_cell(self.pidgy_transform(cell))\n",
       "        \n",
       "        def __init__(self, *args, **kwargs):\n",
       "            super().__init__(*args, **kwargs)\n",
       "            self.tokenizer = Tokenizer()\n",
       "\n",
       "        def pidgy_magic(self, *text): \n",
       "            return IPython.display.Code(self.pidgy_transform(''.join(text)), language='python')\n",
       "\n",
       "\n",
       "#### Block level lexical analysis.\n",
       "\n",
       "Translating [Markdown] to [Python] rely only on block level objects in the [Markdown]\n",
       "grammar.  The `BlockLexer` is a modified analyzer that adds logic to include `doctest` \n",
       "blocks in the grammar.\n",
       "\n",
       "\n",
       "    class BlockLexer(markdown.BlockLexer):\n",
       "        class grammar_class(markdown.BlockGrammar):\n",
       "            doctest = doctest.DocTestParser._EXAMPLE_RE\n",
       "            block_code = re.compile(r'^((?!\\s+>>>\\s) {4}[^\\n]+\\n*)+')\n",
       "            default_rules = \"newline hrule block_code fences heading nptable lheading block_quote list_block def_links def_footnotes table paragraph text\".split()\n",
       "\n",
       "        def parse_doctest(self, m): self.tokens.append({'type': 'paragraph', 'text': m.group(0)})\n",
       "\n",
       "        def parse_fences(self, m):\n",
       "            if m.group(2): self.tokens.append({'type': 'paragraph', 'text': m.group(0)})\n",
       "            else: super().parse_fences(m)\n",
       "\n",
       "        def parse_hrule(self, m): self.tokens.append(dict(type='hrule', text=m.group(0)))\n",
       "            \n",
       "        def parse_def_links(self, m):\n",
       "            super().parse_def_links(m)\n",
       "            self.tokens.append(dict(type='def_link', text=m.group(0)))\n",
       "            \n",
       "        def parse(self, text: str, default_rules=None, normalize=True) -> typing.List[dict]:\n",
       "            if not self.depth: self.tokens = []\n",
       "            with self: tokens = super().parse(whiten(text), default_rules)\n",
       "            if normalize and not self.depth: tokens = self.normalize(text, tokens)\n",
       "            return tokens\n",
       "        \n",
       "        depth = 0\n",
       "        def __enter__(self): self.depth += 1\n",
       "        def __exit__(self, *e): self.depth -= 1\n",
       "\n",
       "\n",
       "The `doctest` token is identified before the block code.\n",
       "\n",
       "\n",
       "<!--\n",
       "    \n",
       "    for x in \"default_rules footnote_rules list_rules\".split():\n",
       "        setattr(BlockLexer, x, list(getattr(BlockLexer, x)))\n",
       "        getattr(BlockLexer, x).insert(getattr(BlockLexer, x).index('block_code'), 'doctest')\n",
       "        if 'block_html' in getattr(BlockLexer, x):\n",
       "            getattr(BlockLexer, x).pop(getattr(BlockLexer, x).index('block_html'))\n",
       "\n",
       "\n",
       "-->\n",
       "\n",
       "\n",
       "Our translation creates tokens specific to each [Markdown] rule, \n",
       "for code it is only necessary to identify code and paragraph tokens.\n",
       "The normalizer compacts tokens into the necessary tokens.\n",
       "\n",
       "\n",
       "    class Normalizer(BlockLexer):\n",
       "        def normalize(self, text, tokens):\n",
       "            \"\"\"Combine non-code tokens into contiguous blocks.\"\"\"\n",
       "            compacted = []\n",
       "            while tokens:\n",
       "                token = tokens.pop(0)\n",
       "                if 'text' not in token: continue\n",
       "                else: \n",
       "                    if not token['text'].strip(): continue\n",
       "                    block, body = token['text'].splitlines(), \"\"\n",
       "                while block:\n",
       "                    line = block.pop(0)\n",
       "                    if line:\n",
       "                        before, line, text = text.partition(line)\n",
       "                        body += before + line\n",
       "                if token['type']=='code':\n",
       "                    compacted.append({'type': 'code', 'lang': None, 'text': body})\n",
       "                else:\n",
       "                    if compacted and compacted[-1]['type'] == 'paragraph':\n",
       "                        compacted[-1]['text'] += body\n",
       "                    else: compacted.append({'type': 'paragraph', 'text': body})\n",
       "            if compacted and compacted[-1]['type'] == 'paragraph':\n",
       "                compacted[-1]['text'] += text\n",
       "            elif text.strip():\n",
       "                compacted.append({'type': 'paragraph', 'text': text})\n",
       "            # Deal with front matter\n",
       "            if compacted[0]['text'].startswith('---\\n') and '\\n---' in compacted[0]['text'][4:]:\n",
       "                token = compacted.pop(0)\n",
       "                front_matter, sep, paragraph = token['text'][4:].partition('---')\n",
       "                compacted = [{'type': 'front_matter', 'text': F\"\\n{front_matter}\"},\n",
       "                            {'type': 'paragraph', 'text': paragraph}] + compacted\n",
       "            return compacted\n",
       "\n",
       "\n",
       "#### Tokenizer logic\n",
       "\n",
       "The tokenizer controls the translation of markdown strings to python strings.  Our major constraint is that the Markdown input should retain line numbers.\n",
       "\n",
       "\n",
       "    class Tokenizer(Normalizer):\n",
       "        def untokenize(self, tokens: τ.List[dict], source: str = \"\"\"\"\"\", last: int =0) -> str:\n",
       "            INDENT = indent = base_indent(tokens) or 4\n",
       "            for i, token in enumerate(tokens):\n",
       "                object = token['text']\n",
       "                if token and token['type'] == 'code':\n",
       "                    if object.lstrip().startswith(FENCE):\n",
       "\n",
       "                        object = ''.join(''.join(object.partition(FENCE)[::2]).rpartition(FENCE)[::2])\n",
       "                        indent = INDENT + num_first_indent(object)\n",
       "                        object = textwrap.indent(object, INDENT*SPACE)\n",
       "\n",
       "                    if object.lstrip().startswith(MAGIC):  ...\n",
       "                    else: indent = num_last_indent(object)\n",
       "                elif token and token['type'] == 'front_matter': \n",
       "                    object = textwrap.indent(\n",
       "                        F\"locals().update(__import__('yaml').safe_load({quote(object)}))\\n\", indent*SPACE)\n",
       "\n",
       "                elif not object: ...\n",
       "                else:\n",
       "                    object = textwrap.indent(object, SPACE*max(indent-num_first_indent(object), 0))\n",
       "                    for next in tokens[i+1:]:\n",
       "                        if next['type'] == 'code':\n",
       "                            next = num_first_indent(next['text'])\n",
       "                            break\n",
       "                    else: next = indent       \n",
       "                    Δ = max(next-indent, 0)\n",
       "\n",
       "                    if not Δ and source.rstrip().rstrip(CONTINUATION).endswith(COLON): \n",
       "                        Δ += 4\n",
       "\n",
       "                    spaces = num_whitespace(object)\n",
       "                    \"what if the spaces are ling enough\"\n",
       "                    object = object[:spaces] + Δ*SPACE+ object[spaces:]\n",
       "                    if not source.rstrip().rstrip(CONTINUATION).endswith(QUOTES): \n",
       "                        object = quote(object)\n",
       "                source += object\n",
       "\n",
       "            # add a semicolon to the source if the last block is code.\n",
       "            for token in reversed(tokens):\n",
       "                if token['text'].strip():\n",
       "                    if token['type'] != 'code': \n",
       "                        source = source.rstrip() + SEMI\n",
       "                    break\n",
       "\n",
       "            return source\n",
       "            \n",
       "    pidgy = pidgyTransformer()\n",
       "\n",
       "\n",
       "<details><summary>Utility functions for the tangle module</summary>\n",
       "\n",
       "\n",
       "    def load_ipython_extension(shell):\n",
       "        shell.input_transformer_manager = shell.tangle = pidgyTransformer()        \n",
       "    \n",
       "    def unload_ipython_extension(shell):\n",
       "        shell.input_transformer_manager = __import__('IPython').core.inputtransformer2.TransformerManager()\n",
       "    \n",
       "    (FENCE, CONTINUATION, SEMI, COLON, MAGIC, DOCTEST), QUOTES, SPACE ='``` \\\\ ; : %% >>>'.split(), ('\"\"\"', \"'''\"), ' '\n",
       "    WHITESPACE = re.compile('^\\s*', re.MULTILINE)\n",
       "\n",
       "    def num_first_indent(text):\n",
       "        for str in text.splitlines():\n",
       "            if str.strip(): return len(str) - len(str.lstrip())\n",
       "        return 0\n",
       "    \n",
       "    def num_last_indent(text):\n",
       "        for str in reversed(text.splitlines()):\n",
       "            if str.strip(): return len(str) - len(str.lstrip())\n",
       "        return 0\n",
       "\n",
       "    def base_indent(tokens):\n",
       "        \"Look ahead for the base indent.\"\n",
       "        for i, token in enumerate(tokens):\n",
       "            if token['type'] == 'code':\n",
       "                code = token['text']\n",
       "                if code.lstrip().startswith(FENCE): continue\n",
       "                indent = num_first_indent(code)\n",
       "                break\n",
       "        else: indent = 4\n",
       "        return indent\n",
       "\n",
       "    def quote(text):\n",
       "        \"\"\"wrap text in `QUOTES`\"\"\"\n",
       "        if text.strip():\n",
       "            left, right = len(text)-len(text.lstrip()), len(text.rstrip())\n",
       "            quote = QUOTES[(text[right-1] in QUOTES[0]) or (QUOTES[0] in text)]\n",
       "            return text[:left] + quote + text[left:right] + quote + text[right:]\n",
       "        return text    \n",
       "\n",
       "    def num_whitespace(text): return len(text) - len(text.lstrip())\n",
       "    \n",
       "    def whiten(text: str) -> str:\n",
       "        \"\"\"`whiten` strips empty lines because the `markdown.BlockLexer` doesn't like that.\"\"\"\n",
       "        return '\\n'.join(x.rstrip() for x in text.splitlines())\n",
       "\n",
       "\n",
       "</summary></details>\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "{{load(pidgy.tangle, 2)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### [<code>[source]</code>](pidgy/extras.ipynb)Extra langauge features of `pidgy`\n",
       "\n",
       "`pidgy` experiments extra language features for python, using the same system\n",
       "that IPython uses to add features like line and cell magics.\n",
       "\n",
       "<!--\n",
       "\n",
       "\n",
       "    import IPython, typing as τ, mistune as markdown, IPython, importnb as _import_, textwrap, ast, doctest, typing, re\n",
       "    import dataclasses, ast, pidgy\n",
       "    with pidgy.pidgyLoader(lazy=True):\n",
       "        try: from . import events\n",
       "        except: import events\n",
       "\n",
       "\n",
       "-->\n",
       "\n",
       "##### naming variables with gestures.\n",
       "\n",
       "We know naming is hard, there is no point focusing on it. `pidgy` allows authors\n",
       "to use emojis as variables in python. They add extra color and expression to the narrative.\n",
       "\n",
       "\n",
       "    def demojize(lines, delimiters=('_', '_')):\n",
       "        str = ''.join(lines)\n",
       "        import tokenize, emoji, stringcase; tokens = []\n",
       "        try:\n",
       "            for token in list(tokenize.tokenize(\n",
       "                __import__('io').BytesIO(str.encode()).readline)):\n",
       "                if token.type == tokenize.ERRORTOKEN:\n",
       "                    string = emoji.demojize(token.string, delimiters=delimiters\n",
       "                                           ).replace('-', '_').replace(\"’\", \"_\")\n",
       "                    if tokens and tokens[-1].type == tokenize.NAME: tokens[-1] = tokenize.TokenInfo(tokens[-1].type, tokens[-1].string + string, tokens[-1].start, tokens[-1].end, tokens[-1].line)\n",
       "                    else: tokens.append(\n",
       "                        tokenize.TokenInfo(\n",
       "                            tokenize.NAME, string, token.start, token.end, token.line))\n",
       "                else: tokens.append(token)\n",
       "            return tokenize.untokenize(tokens).decode().splitlines(True)\n",
       "        except BaseException: raise SyntaxError(str)\n",
       "\n",
       "\n",
       "##### Top level return and yield statements.\n",
       "\n",
       "<!--\n",
       "\n",
       "\n",
       "    def unload_ipython_extension(shell):\n",
       "        shell.extras.unregister()\n",
       "\n",
       "\n",
       "-->\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "{{load(pidgy.extras, 3)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "toc-hr-collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### [<code>[source]</code>](pidgy/weave.md)Weaving cells in pidgin programs\n",
       "\n",
       "<!--\n",
       "\n",
       "    import datetime, dataclasses, sys, IPython as python, IPython, nbconvert as export, collections, IPython as python, mistune as markdown, hashlib, functools, hashlib, jinja2.meta, pidgy\n",
       "    exporter, shell = export.exporters.TemplateExporter(), python.get_ipython()\n",
       "    modules = lambda:[x for x in sys.modules if '.' not in x and not str.startswith(x,'_')]\n",
       "    with pidgy.pidgyLoader(lazy=True):\n",
       "        try:\n",
       "            from . import events\n",
       "        except:\n",
       "            import events\n",
       "\n",
       "\n",
       "-->\n",
       "\n",
       "pidgin programming is an incremental approach to documents.\n",
       "\n",
       "    def load_ipython_extension(shell):\n",
       "        shell.display_formatter.formatters['text/markdown'].for_type(str, lambda x: x)\n",
       "        shell.weave = Weave(shell=shell)\n",
       "        shell.weave.register()\n",
       "\n",
       "    @dataclasses.dataclass\n",
       "    class Weave(events.Events):\n",
       "        shell: IPython.InteractiveShell = dataclasses.field(default_factory=IPython.get_ipython)\n",
       "        environment: jinja2.Environment = dataclasses.field(default=exporter.environment)\n",
       "        _null_environment = jinja2.Environment()\n",
       "\n",
       "        def format_markdown(self, text):\n",
       "            try:\n",
       "                template = exporter.environment.from_string(text, globals=getattr(self.shell, 'user_ns', {}))\n",
       "                text = template.render()\n",
       "            except BaseException as Exception:\n",
       "                self.shell.showtraceback((type(Exception), Exception, Exception.__traceback__))\n",
       "            return text\n",
       "\n",
       "        def format_metadata(self):\n",
       "            parent = getattr(self.shell.kernel, '_last_parent', {})\n",
       "            return {}\n",
       "\n",
       "        def _update_filters(self):\n",
       "            self.environment.filters.update({\n",
       "                k: v for k, v in getattr(self.shell, 'user_ns', {}).items() if callable(v) and k not in self.environment.filters})\n",
       "\n",
       "\n",
       "        def post_run_cell(self, result):\n",
       "            text = strip_front_matter(result.info.raw_cell)\n",
       "            lines = text.splitlines() or ['']\n",
       "            IPython.display.display(IPython.display.Markdown(\n",
       "                self.format_markdown(text) if lines[0].strip() else F\"\"\"<!--\\n{text}\\n\\n-->\"\"\", metadata=self.format_metadata())\n",
       "            )\n",
       "            return result\n",
       "\n",
       "    def unload_ipython_extension(shell):\n",
       "        try:\n",
       "            shell.weave.unregister()\n",
       "        except:...\n",
       "\n",
       "    def strip_front_matter(text):\n",
       "        if text.startswith('---\\n'):\n",
       "            front_matter, sep, rest = text[4:].partition(\"\\n---\")\n",
       "            if sep: return ''.join(rest.splitlines(True)[1:])\n",
       "        return text\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "{{load(pidgy.weave, 2)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "toc-hr-collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### [<code>[source]</code>](pidgy/testing.md)Interactive testing of literate programs\n",
       "\n",
       "Testing is something we added because of the application of notebooks as test units.\n",
       "\n",
       "A primary use case of notebooks is to test ideas. Typically this in informally using\n",
       "manual validation to qualify the efficacy of narrative and code. To ensure testable literate documents\n",
       "we formally test code incrementally during interactive computing.\n",
       "\n",
       "<!--\n",
       "\n",
       "    import unittest, doctest, textwrap, dataclasses, IPython, re, pidgy, sys, typing, types, contextlib, ast, inspect\n",
       "    with pidgy.pidgyLoader(lazy=True):\n",
       "        try: from . import events\n",
       "        except: import events\n",
       "\n",
       "-->\n",
       "\n",
       "    def make_test_suite(*objects: typing.Union[\n",
       "        unittest.TestCase, types.FunctionType, str\n",
       "    ], vars, name) -> unittest.TestSuite:\n",
       "\n",
       "The interactive testing suite execute `doctest and unittest` conventions\n",
       "for a flexible interface to verifying the computational qualities of literate programs.\n",
       "\n",
       "        suite, doctest_suite = unittest.TestSuite(), doctest.DocTestSuite()\n",
       "        suite.addTest(doctest_suite)\n",
       "        for object in objects:\n",
       "            if isinstance(object, type) and issubclass(object, unittest.TestCase):\n",
       "                suite.addTest(unittest.defaultTestLoader.loadTestsFromTestCase(object))\n",
       "            elif isinstance(object, str):\n",
       "                doctest_suite.addTest(doctest.DocTestCase(\n",
       "                doctest.DocTestParser().get_doctest(object, vars, name, name, 1), doctest.ELLIPSIS))\n",
       "                doctest_suite.addTest(doctest.DocTestCase(\n",
       "                InlineDoctestParser().get_doctest(object, vars, name, name, 1), checker=NullOutputCheck))\n",
       "            elif inspect.isfunction(object):\n",
       "                suite.addTest(unittest.FunctionTestCase(object))\n",
       "        return suite\n",
       "\n",
       "    @dataclasses.dataclass\n",
       "    class Testing(events.Events):\n",
       "\n",
       "The `Testing` class executes the test suite each time a cell is executed.\n",
       "\n",
       "        function_pattern: str = 'test_'\n",
       "        def post_run_cell(self, result):\n",
       "            globs, filename = self.shell.user_ns, F\"In[{self.shell.last_execution_result.execution_count}]\"\n",
       "\n",
       "            if not (result.error_before_exec or result.error_in_exec):\n",
       "                with ipython_compiler(self.shell):\n",
       "                    definitions = [self.shell.user_ns[x] for x in getattr(self.shell.metadata, 'definitions', [])\n",
       "                        if x.startswith(self.function_pattern) or\n",
       "                        (isinstance(self.shell.user_ns[x], type)\n",
       "                         and issubclass(self.shell.user_ns[x], unittest.TestCase))\n",
       "                    ]\n",
       "                    result = self.run(make_test_suite(result.info.raw_cell, *definitions, vars=self.shell.user_ns, name=filename), result)\n",
       "\n",
       "\n",
       "        def run(self, suite: unittest.TestCase, cell) -> unittest.TestResult:\n",
       "            result = unittest.TestResult(); suite.run(result)\n",
       "            if result.failures:\n",
       "                msg = '\\n'.join(msg for text, msg in result.failures)\n",
       "                msg = re.sub(re.compile(\"<ipython-input-[0-9]+-\\S+>\"), F'In[{cell.execution_count}]', clean_doctest_traceback(msg))\n",
       "                sys.stderr.writelines((str(result) + '\\n' + msg).splitlines(True))\n",
       "                return result\n",
       "\n",
       "    @contextlib.contextmanager\n",
       "    def ipython_compiler(shell):\n",
       "\n",
       "We'll have to replace how `doctest` compiles code with the `IPython` machinery.\n",
       "\n",
       "        def compiler(input, filename, symbol, *args, **kwargs):\n",
       "            nonlocal shell\n",
       "            return shell.compile(\n",
       "                ast.Interactive(\n",
       "                    body=shell.transform_ast(\n",
       "                    shell.compile.ast_parse(shell.transform_cell(textwrap.indent(input, ' '*4)))\n",
       "                ).body),\n",
       "                F\"In[{shell.last_execution_result.execution_count}]\",\n",
       "                \"single\",\n",
       "            )\n",
       "\n",
       "        yield setattr(doctest, \"compile\", compiler)\n",
       "        doctest.compile = compile\n",
       "\n",
       "    def clean_doctest_traceback(str, *lines):\n",
       "        str = re.sub(re.compile(\"\"\"\\n\\s+File [\\s\\S]+, line [0-9]+, in runTest\\s+raise[\\s\\S]+\\([\\s\\S]+\\)\\n?\"\"\"), '\\n', str)\n",
       "        return re.sub(re.compile(\"Traceback \\(most recent call last\\):\\n\"), '', str)\n",
       "\n",
       "<details><summary>Utilities for the testing module.</summary>\n",
       "    \n",
       "    class NullOutputCheck(doctest.OutputChecker):\n",
       "        def check_output(self, *e): return True\n",
       "\n",
       "    class InlineDoctestParser(doctest.DocTestParser):\n",
       "        _EXAMPLE_RE = re.compile(r'`(?P<indent>\\s{0})'\n",
       "    r'(?P<source>[^`].*?)'\n",
       "    r'`')\n",
       "        def _parse_example(self, m, name, lineno): return m.group('source'), None, \"...\", None\n",
       "\n",
       "\n",
       "    def load_ipython_extension(shell):\n",
       "        shell.testing = Testing(shell=shell).register()\n",
       "\n",
       "    def unload_ipython_extension(shell):\n",
       "        shell.testing.unregister()\n",
       "\n",
       "</details>\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "{{load(pidgy.testing, 2)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### [<code>[source]</code>](pidgy/metadata.md)Capturing metadata during the interactive compute process\n",
       "\n",
       "To an organization, human compute time bears an important cost\n",
       "and programming represents a small part of that cycle.\n",
       "\n",
       "    def load_ipython_extension(shell):\n",
       "\n",
       "The `metadata` module assists in collecting metadata about the interactive compute process.\n",
       "It appends the metadata atrribute to the shell.\n",
       "\n",
       "        shell.metadata = Metadata(shell=shell).register()\n",
       "\n",
       "<!--\n",
       "\n",
       "    import dataclasses, ast, pidgy\n",
       "    with pidgy.pidgyLoader(lazy=True):\n",
       "        try: from . import events\n",
       "        except: import events\n",
       "\n",
       "-->\n",
       "\n",
       "    @dataclasses.dataclass\n",
       "    class Metadata(events.Events, ast.NodeTransformer):\n",
       "        definitions: list = dataclasses.field(default_factory=list)\n",
       "        def pre_execute(self):\n",
       "            self.definitions = []\n",
       "\n",
       "        def visit_FunctionDef(self, node):\n",
       "            self.definitions.append(node.name)\n",
       "            return node\n",
       "\n",
       "        visit_ClassDef = visit_FunctionDef\n",
       "\n",
       "<!--\n",
       "\n",
       "    def unload_ipython_extension(shell):\n",
       "        shell.metadata.unregister()\n",
       "\n",
       "-->\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "{{load(pidgy.metadata, 2)}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{load('readme.md')}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook index.md.ipynb to markdown\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "    # NBVAL_SKIP\n",
       "\n",
       "    \n",
       "    if __name__ == '__main__' and not '__file__' in globals():\n",
       "        !jupyter nbconvert --to markdown --stdout --TemplateExporter.exclude_input=True index.md.ipynb > docs/index.md"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    # NBVAL_SKIP\n",
    "\n",
    "    \n",
    "    if __name__ == '__main__' and not '__file__' in globals():\n",
    "        !jupyter nbconvert --to markdown --stdout --TemplateExporter.exclude_input=True index.md.ipynb > docs/index.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pidgy 3",
   "language": "python",
   "name": "pidgy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
