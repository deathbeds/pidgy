{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    if __name__ == '__main__':\n",
    "        %reload_ext __init__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    import __main__, IPython, typing as τ, mistune as markdown, IPython as python, importnb as _import_, textwrap, ipykernel.kernelapp, ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    if __name__ == '__main__':\n",
    "        shell = get_ipython()\n",
    "        import notebook, jupyter\n",
    "        pidgin, Tokenize = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "    def load_ipython_extension(shell):\n",
       "        \"\"\"\n",
       "The `pidgin` `load_ipython_extension`'s primary function transforms the `jupyter`\n",
       "`notebook`s into a literate computing interfaces.\n",
       "`markdown` becomes the primary plain-text format for submitting code,\n",
       "and the `markdown` is translated to `python` source code\n",
       "before compilation.\n",
       "The implementation configures the appropriate\n",
       "features of the `IPython.InteractiveShell` to accomodate\n",
       "the interactive literate programming experience.\n",
       "\n",
       "In this section, we'll implement a `shell.input_transformer_manager`\n",
       "that handles the logical translation of `markdown` to `python`.\n",
       "The translation maintains the source line numbers and \n",
       "normalizes the narrative relative to the source code.  Consequently,\n",
       "introduces new syntaxes at the interfaces between `markdown and python`.\n",
       "\n",
       "        \"\"\"\n",
       "        pidgin_transformer = PidginTransformer()        \n",
       "        shell.input_transformer_manager = pidgin_transformer\n",
       "        \n",
       "        \"\"\"\n",
       "`IPython` provides configurable interactive `shell` properties.  Some of the configurable properties\n",
       "control how `input` code is translated into valid source code. \n",
       "The `pidgin` translation is managed by a custom `IPython.core.inputtransformer2.TransformerManager`.\n",
       "        \n",
       "        \"\"\"\"\"\"\n",
       "        >>> shell.input_transformer_manager\n",
       "        <...PidginTransformer object...>\n",
       "        \n",
       "        \"\"\"\"\"\"\n",
       "\n",
       "The `shell.input_transformer_manager` applies string transformations to clean up the `input`\n",
       "to be valid `python`.  There are three stages of line of transforms.\n",
       "\n",
       "1. Cleanup transforms that operate on the entire cell `input`.\n",
       "\n",
       "        \"\"\"\"\"\"\n",
       "        >>> shell.input_transformers_cleanup\n",
       "        [<...leading_empty_lines...>, <...leading_indent...>, <...PromptStripper...>, ...]\n",
       "        \n",
       "        \"\"\"\"\"\"\n",
       "        \n",
       "2. Line transforms that are applied the cell `input` with split lines. \n",
       "This is where `IPython` introduces their bespoke cell magic syntaxes.\n",
       "        \n",
       "        \"\"\"\"\"\"\n",
       "        >>> shell.input_transformer_manager.line_transforms\n",
       "        [...<...cell_magic...>...]\n",
       "        \n",
       "        \"\"\"\"\"\"\n",
       "        \n",
       "3. Token transformers that look for specific tokens at the like level.  `IPython`'s default\n",
       "behavior introduces new symbols into the programming language.\n",
       "\n",
       "        \"\"\"\"\"\"\n",
       "        >>> shell.input_transformer_manager.token_transformers\n",
       "        [<...MagicAssign...SystemAssign...EscapedCommand...HelpEnd...>]\n",
       "        \n",
       "        \"\"\"\"\"\"\n",
       "\n",
       "After all of the `input` transformations are complete, the `input` should be valid source that `ast.parse, compile or shell.compile` \n",
       "may accept.\n",
       "\n",
       "        \"\"\"\"\"\"\n",
       "        >>> shell.ast_transformers\n",
       "        [...]\n",
       "        \n",
       "        \"\"\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "text/markdown": {
       "end_time": "2020-02-01T16:07:16.218361",
       "modules": [],
       "names": [],
       "start_time": "2020-02-01T16:07:16.190090"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "    def load_ipython_extension(shell):\n",
    "        \"\"\"\n",
    "The `pidgin` `load_ipython_extension`'s primary function transforms the `jupyter`\n",
    "`notebook`s into a literate computing interfaces.\n",
    "`markdown` becomes the primary plain-text format for submitting code,\n",
    "and the `markdown` is translated to `python` source code\n",
    "before compilation.\n",
    "The implementation configures the appropriate\n",
    "features of the `IPython.InteractiveShell` to accomodate\n",
    "the interactive literate programming experience.\n",
    "\n",
    "In this section, we'll implement a `shell.input_transformer_manager`\n",
    "that handles the logical translation of `markdown` to `python`.\n",
    "The translation maintains the source line numbers and \n",
    "normalizes the narrative relative to the source code.  Consequently,\n",
    "introduces new syntaxes at the interfaces between `markdown and python`.\n",
    "\n",
    "        \"\"\"\n",
    "        pidgin_transformer = PidginTransformer()        \n",
    "        shell.input_transformer_manager = pidgin_transformer\n",
    "        \n",
    "        \"\"\"\n",
    "`IPython` provides configurable interactive `shell` properties.  Some of the configurable properties\n",
    "control how `input` code is translated into valid source code. \n",
    "The `pidgin` translation is managed by a custom `IPython.core.inputtransformer2.TransformerManager`.\n",
    "        \n",
    "        \"\"\"\"\"\"\n",
    "        >>> shell.input_transformer_manager\n",
    "        <...PidginTransformer object...>\n",
    "        \n",
    "        \"\"\"\"\"\"\n",
    "\n",
    "The `shell.input_transformer_manager` applies string transformations to clean up the `input`\n",
    "to be valid `python`.  There are three stages of line of transforms.\n",
    "\n",
    "1. Cleanup transforms that operate on the entire cell `input`.\n",
    "\n",
    "        \"\"\"\"\"\"\n",
    "        >>> shell.input_transformers_cleanup\n",
    "        [<...leading_empty_lines...>, <...leading_indent...>, <...PromptStripper...>, ...]\n",
    "        \n",
    "        \"\"\"\"\"\"\n",
    "        \n",
    "2. Line transforms that are applied the cell `input` with split lines. \n",
    "This is where `IPython` introduces their bespoke cell magic syntaxes.\n",
    "        \n",
    "        \"\"\"\"\"\"\n",
    "        >>> shell.input_transformer_manager.line_transforms\n",
    "        [...<...cell_magic...>...]\n",
    "        \n",
    "        \"\"\"\"\"\"\n",
    "        \n",
    "3. Token transformers that look for specific tokens at the like level.  `IPython`'s default\n",
    "behavior introduces new symbols into the programming language.\n",
    "\n",
    "        \"\"\"\"\"\"\n",
    "        >>> shell.input_transformer_manager.token_transformers\n",
    "        [<...MagicAssign...SystemAssign...EscapedCommand...HelpEnd...>]\n",
    "        \n",
    "        \"\"\"\"\"\"\n",
    "\n",
    "After all of the `input` transformations are complete, the `input` should be valid source that `ast.parse, compile or shell.compile` \n",
    "may accept.\n",
    "\n",
    "        \"\"\"\"\"\"\n",
    "        >>> shell.ast_transformers\n",
    "        [...]\n",
    "        \n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "    class PidginTransformer(IPython.core.inputtransformer2.TransformerManager):\n",
       "        def pidgin_transform(self, cell: str) -> str: \n",
       "            tokens = self.tokenizer.parse(''.join(cell))\n",
       "            return self.tokenizer.untokenize(tokens)\n",
       "        \n",
       "        def pidgin_cleanup(self, cell: str) -> list: \n",
       "            return self.pidgin_transform(cell).splitlines(True)\n",
       "        \n",
       "        def __init__(self, *args, **kwargs):\n",
       "            super().__init__(*args, **kwargs)\n",
       "            self.tokenizer = Tokenizer()\n",
       "            self.cleanup_transforms.insert(0, self.pidgin_cleanup)\n",
       "            self.line_transforms.append(demojize)\n",
       "\n",
       "        def pidgin_magic(self, *text): \n",
       "            \"\"\"Expand the text to tokens to tokens and \n",
       "            compact as a formatted `\"python\"` code.\"\"\"\n",
       "            return IPython.display.Code(self.pidgin_transform(''.join(text)), language='python')"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "text/markdown": {
       "end_time": "2020-02-01T16:07:32.577174",
       "modules": [],
       "names": [],
       "start_time": "2020-02-01T16:07:32.571403"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "    class PidginTransformer(IPython.core.inputtransformer2.TransformerManager):\n",
    "        def pidgin_transform(self, cell: str) -> str: \n",
    "            tokens = self.tokenizer.parse(''.join(cell))\n",
    "            return self.tokenizer.untokenize(tokens)\n",
    "        \n",
    "        def pidgin_cleanup(self, cell: str) -> list: \n",
    "            return self.pidgin_transform(cell).splitlines(True)\n",
    "        \n",
    "        def __init__(self, *args, **kwargs):\n",
    "            super().__init__(*args, **kwargs)\n",
    "            self.tokenizer = Tokenizer()\n",
    "            self.cleanup_transforms.insert(0, self.pidgin_cleanup)\n",
    "            self.line_transforms.append(demojize)\n",
    "\n",
    "        def pidgin_magic(self, *text): \n",
    "            \"\"\"Expand the text to tokens to tokens and \n",
    "            compact as a formatted `\"python\"` code.\"\"\"\n",
    "            return IPython.display.Code(self.pidgin_transform(''.join(text)), language='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "    import mistune as markdown, textwrap, __main__, IPython, typing, re, IPython, nbconvert, ipykernel, doctest, ast\n",
       "    __all__ = 'pidgin',"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "text/markdown": {
       "end_time": "2020-02-01T16:07:33.371879",
       "modules": [],
       "names": [
        "typing",
        "re",
        "nbconvert",
        "doctest"
       ],
       "start_time": "2020-02-01T16:07:33.369878"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "    import mistune as markdown, textwrap, __main__, IPython, typing, re, IPython, nbconvert, ipykernel, doctest, ast\n",
    "    __all__ = 'pidgin',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "    class Tokenizer(markdown.BlockLexer):\n",
       "        \"\"\"Tokenize `input` text into `\"code\" and not \"code\"` tokens that will be translated into valid `python` source.\n",
       "        \n",
       "        \"\"\"\n",
       "        class grammar_class(markdown.BlockGrammar):\n",
       "            doctest = doctest.DocTestParser._EXAMPLE_RE\n",
       "\n",
       "        def parse(self, text: str, default_rules=None) -> typing.List[dict]:\n",
       "            if not self.depth: self.tokens = []\n",
       "            with self: tokens = super().parse(whiten(text), default_rules)\n",
       "            if not self.depth: tokens = self.compact(text, tokens)\n",
       "            return tokens\n",
       "\n",
       "        def parse_doctest(self, m): self.tokens.append({'type': 'paragraph', 'text': m.group(0)})\n",
       "            \n",
       "        def parse_fences(self, m):\n",
       "            if m.group(2): self.tokens.append({'type': 'paragraph', 'text': m.group(0)})\n",
       "            else: super().parse_fences(m)\n",
       "            \n",
       "        def parse_hrule(self, m):\n",
       "            self.tokens.append({'type': 'hrule', 'text': m.group(0)})\n",
       "            \n",
       "        def compact(self, text, tokens):\n",
       "            \"\"\"Combine non-code tokens into contiguous blocks.\"\"\"\n",
       "            compacted = []\n",
       "            while tokens:\n",
       "                token = tokens.pop(0)\n",
       "                if 'text' not in token: continue\n",
       "                else: \n",
       "                    if not token['text'].strip(): continue\n",
       "                    block, body = token['text'].splitlines(), \"\"\n",
       "                while block:\n",
       "                    line = block.pop(0)\n",
       "                    if line:\n",
       "                        before, line, text = text.partition(line)\n",
       "                        body += before + line\n",
       "                if token['type']=='code':\n",
       "                    compacted.append({'type': 'code', 'lang': None, 'text': body})\n",
       "                else:\n",
       "                    if compacted and compacted[-1]['type'] == 'paragraph':\n",
       "                        compacted[-1]['text'] += body\n",
       "                    else: compacted.append({'type': 'paragraph', 'text': body})\n",
       "            if compacted and compacted[-1]['type'] == 'paragraph':\n",
       "                compacted[-1]['text'] += text\n",
       "            elif text.strip():\n",
       "                compacted.append({'type': 'paragraph', 'text': text})\n",
       "            return compacted\n",
       "        \n",
       "        depth = 0\n",
       "        def __enter__(self): self.depth += 1\n",
       "        def __exit__(self, *e): self.depth -= 1\n",
       "        \n",
       "        def untokenize(self, tokens: τ.List[dict], source: str = \"\"\"\"\"\", last: int =0) -> str:\n",
       "            INDENT = indent = base_indent(tokens) or 4\n",
       "            for i, token in enumerate(tokens):\n",
       "                object = token['text']\n",
       "                if token and token['type'] == 'code':\n",
       "                    object = '\\n'.join(map(str.rstrip, object.splitlines()))\n",
       "                    if object.lstrip().startswith(FENCE):\n",
       "\n",
       "                        object = ''.join(''.join(object.partition(FENCE)[::2]).rpartition(FENCE)[::2])\n",
       "                        indent = INDENT + num_first_indent(object)\n",
       "                        object = textwrap.indent(object, INDENT*SPACE)\n",
       "\n",
       "                    if object.lstrip().startswith(MAGIC):  ...\n",
       "                    else: indent = num_last_indent(object)\n",
       "                elif not object: ...\n",
       "                else:\n",
       "                    object = textwrap.indent(object, indent*SPACE)\n",
       "                    for next in tokens[i+1:]:\n",
       "                        if next['type'] == 'code':\n",
       "                            next = num_first_indent(next['text'])\n",
       "                            break\n",
       "                    else: next = indent       \n",
       "                    Δ = max(next-indent, 0)\n",
       "\n",
       "                    if not Δ and source.rstrip().rstrip(CONTINUATION).endswith(COLON): \n",
       "                        Δ += 4\n",
       "\n",
       "                    spaces = num_whitespace(object)\n",
       "                    \"what if the spaces are ling enough\"\n",
       "                    object = object[:spaces] + Δ*SPACE+ object[spaces:]\n",
       "                    if not source.rstrip().rstrip(CONTINUATION).endswith(QUOTES): \n",
       "                        object = quote(object)\n",
       "                source += object\n",
       "\n",
       "            for token in reversed(tokens):\n",
       "                if token['text'].strip():\n",
       "                    if token['type'] != 'code': \n",
       "                        source = source.rstrip() + SEMI\n",
       "                    break\n",
       "\n",
       "            return source \n",
       "            \n",
       "    for x in \"default_rules footnote_rules list_rules\".split():\n",
       "        setattr(Tokenizer, x, list(getattr(Tokenizer, x)))\n",
       "        getattr(Tokenizer, x).insert(getattr(Tokenizer, x).index('block_code'), 'doctest')\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "text/markdown": {
       "end_time": "2020-02-01T16:07:34.171539",
       "modules": [],
       "names": [
        "x"
       ],
       "start_time": "2020-02-01T16:07:34.151797"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "    class Tokenizer(markdown.BlockLexer):\n",
    "        \"\"\"Tokenize `input` text into `\"code\" and not \"code\"` tokens that will be translated into valid `python` source.\n",
    "        \n",
    "        \"\"\"\n",
    "        class grammar_class(markdown.BlockGrammar):\n",
    "            doctest = doctest.DocTestParser._EXAMPLE_RE\n",
    "\n",
    "        def parse(self, text: str, default_rules=None) -> typing.List[dict]:\n",
    "            if not self.depth: self.tokens = []\n",
    "            with self: tokens = super().parse(whiten(text), default_rules)\n",
    "            if not self.depth: tokens = self.compact(text, tokens)\n",
    "            return tokens\n",
    "\n",
    "        def parse_doctest(self, m): self.tokens.append({'type': 'paragraph', 'text': m.group(0)})\n",
    "            \n",
    "        def parse_fences(self, m):\n",
    "            if m.group(2): self.tokens.append({'type': 'paragraph', 'text': m.group(0)})\n",
    "            else: super().parse_fences(m)\n",
    "            \n",
    "        def parse_hrule(self, m):\n",
    "            self.tokens.append({'type': 'hrule', 'text': m.group(0)})\n",
    "            \n",
    "        def compact(self, text, tokens):\n",
    "            \"\"\"Combine non-code tokens into contiguous blocks.\"\"\"\n",
    "            compacted = []\n",
    "            while tokens:\n",
    "                token = tokens.pop(0)\n",
    "                if 'text' not in token: continue\n",
    "                else: \n",
    "                    if not token['text'].strip(): continue\n",
    "                    block, body = token['text'].splitlines(), \"\"\n",
    "                while block:\n",
    "                    line = block.pop(0)\n",
    "                    if line:\n",
    "                        before, line, text = text.partition(line)\n",
    "                        body += before + line\n",
    "                if token['type']=='code':\n",
    "                    compacted.append({'type': 'code', 'lang': None, 'text': body})\n",
    "                else:\n",
    "                    if compacted and compacted[-1]['type'] == 'paragraph':\n",
    "                        compacted[-1]['text'] += body\n",
    "                    else: compacted.append({'type': 'paragraph', 'text': body})\n",
    "            if compacted and compacted[-1]['type'] == 'paragraph':\n",
    "                compacted[-1]['text'] += text\n",
    "            elif text.strip():\n",
    "                compacted.append({'type': 'paragraph', 'text': text})\n",
    "            return compacted\n",
    "        \n",
    "        depth = 0\n",
    "        def __enter__(self): self.depth += 1\n",
    "        def __exit__(self, *e): self.depth -= 1\n",
    "        \n",
    "        def untokenize(self, tokens: τ.List[dict], source: str = \"\"\"\"\"\", last: int =0) -> str:\n",
    "            INDENT = indent = base_indent(tokens) or 4\n",
    "            for i, token in enumerate(tokens):\n",
    "                object = token['text']\n",
    "                if token and token['type'] == 'code':\n",
    "                    object = '\\n'.join(map(str.rstrip, object.splitlines()))\n",
    "                    if object.lstrip().startswith(FENCE):\n",
    "\n",
    "                        object = ''.join(''.join(object.partition(FENCE)[::2]).rpartition(FENCE)[::2])\n",
    "                        indent = INDENT + num_first_indent(object)\n",
    "                        object = textwrap.indent(object, INDENT*SPACE)\n",
    "\n",
    "                    if object.lstrip().startswith(MAGIC):  ...\n",
    "                    else: indent = num_last_indent(object)\n",
    "                elif not object: ...\n",
    "                else:\n",
    "                    object = textwrap.indent(object, indent*SPACE)\n",
    "                    for next in tokens[i+1:]:\n",
    "                        if next['type'] == 'code':\n",
    "                            next = num_first_indent(next['text'])\n",
    "                            break\n",
    "                    else: next = indent       \n",
    "                    Δ = max(next-indent, 0)\n",
    "\n",
    "                    if not Δ and source.rstrip().rstrip(CONTINUATION).endswith(COLON): \n",
    "                        Δ += 4\n",
    "\n",
    "                    spaces = num_whitespace(object)\n",
    "                    \"what if the spaces are ling enough\"\n",
    "                    object = object[:spaces] + Δ*SPACE+ object[spaces:]\n",
    "                    if not source.rstrip().rstrip(CONTINUATION).endswith(QUOTES): \n",
    "                        object = quote(object)\n",
    "                source += object\n",
    "\n",
    "            for token in reversed(tokens):\n",
    "                if token['text'].strip():\n",
    "                    if token['type'] != 'code': \n",
    "                        source = source.rstrip() + SEMI\n",
    "                    break\n",
    "\n",
    "            return source \n",
    "            \n",
    "    for x in \"default_rules footnote_rules list_rules\".split():\n",
    "        setattr(Tokenizer, x, list(getattr(Tokenizer, x)))\n",
    "        getattr(Tokenizer, x).insert(getattr(Tokenizer, x).index('block_code'), 'doctest')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "    class PidginPreprocessor(nbconvert.preprocessors.Preprocessor):\n",
       "        def preprocess_cell(self, cell, resources, index, ):\n",
       "            if cell['cell_type'] == 'code':\n",
       "                cell['source'] = idgin_transformer.transform_cell(''.join(cell['source']))\n",
       "            return cell, resources"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "text/markdown": {
       "end_time": "2020-02-01T16:07:34.811505",
       "modules": [],
       "names": [],
       "start_time": "2020-02-01T16:07:34.809172"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "    class PidginPreprocessor(nbconvert.preprocessors.Preprocessor):\n",
    "        def preprocess_cell(self, cell, resources, index, ):\n",
    "            if cell['cell_type'] == 'code':\n",
    "                cell['source'] = idgin_transformer.transform_cell(''.join(cell['source']))\n",
    "            return cell, resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    (FENCE, CONTINUATION, SEMI, COLON, MAGIC, DOCTEST), QUOTES, SPACE ='``` \\\\ ; : %% >>>'.split(), ('\"\"\"', \"'''\"), ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    WHITESPACE = re.compile('^\\s*', re.MULTILINE)\n",
    "\n",
    "    def num_first_indent(text):\n",
    "        for str in text.splitlines():\n",
    "            if str.strip(): return len(str) - len(str.lstrip())\n",
    "        return 0\n",
    "    \n",
    "    def num_last_indent(text):\n",
    "        for str in reversed(text.splitlines()):\n",
    "            if str.strip(): return len(str) - len(str.lstrip())\n",
    "        return 0\n",
    "\n",
    "    def base_indent(tokens):\n",
    "        \"Look ahead for the base indent.\"\n",
    "        for i, token in enumerate(tokens):\n",
    "            if token['type'] == 'code':\n",
    "                code = token['text']\n",
    "                if code.lstrip().startswith(FENCE): continue\n",
    "                indent = num_first_indent(code)\n",
    "                break\n",
    "        else: indent = 4\n",
    "        return indent\n",
    "\n",
    "    def quote(text):\n",
    "        \"\"\"wrap text in `QUOTES`\"\"\"\n",
    "        if text.strip():\n",
    "            left, right = len(text)-len(text.lstrip()), len(text.rstrip())\n",
    "            quote = QUOTES[(text[right-1] in QUOTES[0]) or (QUOTES[0] in text)]\n",
    "            return text[:left] + quote + text[left:right] + quote + text[right:]\n",
    "        return text    \n",
    "\n",
    "    def num_whitespace(text): return len(text) - len(text.lstrip())\n",
    "    \n",
    "    def whiten(text: str) -> str:\n",
    "        \"\"\"`whiten` strips empty lines because the `markdown.BlockLexer` doesn't like that.\"\"\"\n",
    "        return '\\n'.join(x.rstrip() for x in text.splitlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert pidgin to valid python files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import pidgin notebooks as modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "    class PidginLoader(__import__('importnb').Notebook): \n",
       "        extensions = \".ipynb .md.ipynb\".split()\n",
       "        def code(self, str): return ''.join(pidgin.transform_cell(str))"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "text/markdown": {
       "end_time": "2020-02-01T16:07:36.780465",
       "modules": [],
       "names": [],
       "start_time": "2020-02-01T16:07:36.777992"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "    class PidginLoader(__import__('importnb').Notebook): \n",
    "        extensions = \".ipynb .md.ipynb\".split()\n",
    "        def code(self, str): return ''.join(pidgin.transform_cell(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A custom shell and kernel for `pidgin`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "    def demojize(lines, delimiters=('_', '_')):\n",
       "        str = ''.join(lines)\n",
       "        import tokenize, emoji, stringcase; tokens = []\n",
       "        try:\n",
       "            for token in list(tokenize.tokenize(\n",
       "                __import__('io').BytesIO(str.encode()).readline)):\n",
       "                if token.type == tokenize.ERRORTOKEN:\n",
       "                    string = emoji.demojize(token.string, delimiters=delimiters\n",
       "                                           ).replace('-', '_').replace(\"’\", \"_\")\n",
       "                    if tokens and tokens[-1].type == tokenize.NAME: tokens[-1] = tokenize.TokenInfo(tokens[-1].type, tokens[-1].string + string, tokens[-1].start, tokens[-1].end, tokens[-1].line)\n",
       "                    else: tokens.append(\n",
       "                        tokenize.TokenInfo(\n",
       "                            tokenize.NAME, string, token.start, token.end, token.line))\n",
       "                else: tokens.append(token)\n",
       "            return tokenize.untokenize(tokens).decode().splitlines(True)\n",
       "        except BaseException: raise SyntaxError(str)\n",
       "\n",
       "    pidgin = PidginTransformer()"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "text/markdown": {
       "end_time": "2020-02-01T16:07:37.532281",
       "modules": [],
       "names": [
        "demojize"
       ],
       "start_time": "2020-02-01T16:07:37.527058"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "    def demojize(lines, delimiters=('_', '_')):\n",
    "        str = ''.join(lines)\n",
    "        import tokenize, emoji, stringcase; tokens = []\n",
    "        try:\n",
    "            for token in list(tokenize.tokenize(\n",
    "                __import__('io').BytesIO(str.encode()).readline)):\n",
    "                if token.type == tokenize.ERRORTOKEN:\n",
    "                    string = emoji.demojize(token.string, delimiters=delimiters\n",
    "                                           ).replace('-', '_').replace(\"’\", \"_\")\n",
    "                    if tokens and tokens[-1].type == tokenize.NAME: tokens[-1] = tokenize.TokenInfo(tokens[-1].type, tokens[-1].string + string, tokens[-1].start, tokens[-1].end, tokens[-1].line)\n",
    "                    else: tokens.append(\n",
    "                        tokenize.TokenInfo(\n",
    "                            tokenize.NAME, string, token.start, token.end, token.line))\n",
    "                else: tokens.append(token)\n",
    "            return tokenize.untokenize(tokens).decode().splitlines(True)\n",
    "        except BaseException: raise SyntaxError(str)\n",
    "\n",
    "    pidgin = PidginTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "    import ast\n",
       "    class ReturnYield(ast.NodeTransformer):\n",
       "        def visit_FunctionDef(self, node): return node\n",
       "        visit_AsyncFunctionDef = visit_FunctionDef\n",
       "        def visit_Return(self, node):\n",
       "            replace = ast.parse('''__import__('IPython').display.display()''').body[0]\n",
       "            replace.value.args = node.value.elts if isinstance(node.value, ast.Tuple) else [node.value]\n",
       "            return ast.copy_location(replace, node)\n",
       "\n",
       "        def visit_Expr(self, node):\n",
       "            if isinstance(node.value, (ast.Yield, ast.YieldFrom)):  return ast.copy_location(self.visit_Return(node.value), node)\n",
       "            return node\n",
       "        \n",
       "        visit_Expression = visit_Expr"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "text/markdown": {
       "end_time": "2020-02-01T16:07:38.416816",
       "modules": [],
       "names": [],
       "start_time": "2020-02-01T16:07:38.412992"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "    import ast\n",
    "    class ReturnYield(ast.NodeTransformer):\n",
    "        def visit_FunctionDef(self, node): return node\n",
    "        visit_AsyncFunctionDef = visit_FunctionDef\n",
    "        def visit_Return(self, node):\n",
    "            replace = ast.parse('''__import__('IPython').display.display()''').body[0]\n",
    "            replace.value.args = node.value.elts if isinstance(node.value, ast.Tuple) else [node.value]\n",
    "            return ast.copy_location(replace, node)\n",
    "\n",
    "        def visit_Expr(self, node):\n",
    "            if isinstance(node.value, (ast.Yield, ast.YieldFrom)):  return ast.copy_location(self.visit_Return(node.value), node)\n",
    "            return node\n",
    "        \n",
    "        visit_Expression = visit_Expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "    class PidginInteractiveShell(IPython.InteractiveShell):\n",
       "        \"\"\"Configure a native `pidgin` `IPython.InteractiveShell`\"\"\"\n",
       "    PidginInteractiveShell.input_transformer_manager.default_value = PidginTransformer\n",
       "    PidginInteractiveShell.enable_html_pager.default_value = True\n",
       "\n",
       "    class PidginKernelApp(ipykernel.kernelapp.IPKernelApp): \n",
       "        \"\"\"Configure a native `pidgin` `__import__('ipykernel').kernelapp.IPKernelApp\"\"\"\n",
       "    PidginKernelApp.shell.default_value = PidginInteractiveShell"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "text/markdown": {
       "end_time": "2020-02-01T16:07:39.022388",
       "modules": [],
       "names": [],
       "start_time": "2020-02-01T16:07:39.016613"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "    class PidginInteractiveShell(IPython.InteractiveShell):\n",
    "        \"\"\"Configure a native `pidgin` `IPython.InteractiveShell`\"\"\"\n",
    "    PidginInteractiveShell.input_transformer_manager.default_value = PidginTransformer\n",
    "    PidginInteractiveShell.enable_html_pager.default_value = True\n",
    "\n",
    "    class PidginKernelApp(ipykernel.kernelapp.IPKernelApp): \n",
    "        \"\"\"Configure a native `pidgin` `__import__('ipykernel').kernelapp.IPKernelApp\"\"\"\n",
    "    PidginKernelApp.shell.default_value = PidginInteractiveShell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    def unload_ipython_extension(shell):\n",
    "        \"\"\"\n",
    "Unload the IPython extension.\n",
    "\n",
    "        \"\"\"\n",
    "        shell.input_transformer_manager = __import__('IPython').core.inputtransformer2.TransformerManager()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
