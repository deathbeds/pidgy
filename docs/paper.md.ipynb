{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    if __name__ == '__main__':\n",
    "        %reload_ext pidgin\n",
    "\n",
    "    import jupyter, notebook, IPython, pidgin, mistune as markdown, IPython as python, ast, jinja2 as template, importnb as _import_, doctest, pathlib, graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    from pidgin import *\n",
    "    shell = get_ipython()\n",
    "    import pandas as ðŸ¼\n",
    "    Ã˜ = __name__ == '__main__'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    if Ã˜:\n",
    "        input_formats = !pandoc --list-input-formats\n",
    "        input_formats = {x.split('_')[0] for x in input_formats}\n",
    "        kernels = ðŸ¼.read_html(appendix.get('https://github.com/jupyter/jupyter/wiki/Jupyter-kernels'))[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## The `pidgin` implementation\n",
       "### An `IPython` extension.\n",
       "\n",
       "    def load_ipython_extension(shell):\n",
       "        \"\"\"\n",
       "The `pidgin` `load_ipython_extension`'s primary function transforms the `jupyter`\n",
       "`notebook`s into a literate computing interfaces.\n",
       "`markdown` becomes the primary plain-text format for submitting code,\n",
       "and the `markdown` is translated to `python` source code\n",
       "before compilation.\n",
       "The implementation configures the appropriate\n",
       "features of the `IPython.InteractiveShell` to accomodate\n",
       "the interactive literate programming experience.\n",
       "\n",
       "In this section, we'll implement a `shell.input_transformer_manager`\n",
       "that handles the logical translation of `markdown` to `python`.\n",
       "The translation maintains the source line numbers and \n",
       "normalizes the narrative relative to the source code.  Consequently,\n",
       "introduces new syntaxes at the interfaces between `markdown and python`.\n",
       "\n",
       "        \"\"\"\n",
       "        pidgin_transformer = PidginTransformer()        \n",
       "        shell.input_transformer_manager = pidgin_transformer\n",
       "        \n",
       "        \"\"\"\n",
       "`IPython` provides configurable interactive `shell` properties.  Some of the configurable properties\n",
       "control how `input` code is translated into valid source code. \n",
       "The `pidgin` translation is managed by a custom `IPython.core.inputtransformer2.TransformerManager`.\n",
       "        \n",
       "        \"\"\"\"\"\"\n",
       "        >>> shell.input_transformer_manager\n",
       "        <...PidginTransformer object...>\n",
       "        \n",
       "        \"\"\"\"\"\"\n",
       "\n",
       "The `shell.input_transformer_manager` applies string transformations to clean up the `input`\n",
       "to be valid `python`.  There are three stages of line of transforms.\n",
       "\n",
       "1. Cleanup transforms that operate on the entire cell `input`.\n",
       "\n",
       "        \"\"\"\"\"\"\n",
       "        >>> shell.input_transformers_cleanup\n",
       "        [<...leading_empty_lines...>, <...leading_indent...>, <...PromptStripper...>, ...]\n",
       "        \n",
       "        \"\"\"\"\"\"\n",
       "        \n",
       "2. Line transforms that are applied the cell `input` with split lines. \n",
       "This is where `IPython` introduces their bespoke cell magic syntaxes.\n",
       "        \n",
       "        \"\"\"\"\"\"\n",
       "        >>> shell.input_transformer_manager.line_transforms\n",
       "        [...<...cell_magic...>...]\n",
       "        \n",
       "        \"\"\"\"\"\"\n",
       "        \n",
       "3. Token transformers that look for specific tokens at the like level.  `IPython`'s default\n",
       "behavior introduces new symbols into the programming language.\n",
       "\n",
       "        \"\"\"\"\"\"\n",
       "        >>> shell.input_transformer_manager.token_transformers\n",
       "        [<...MagicAssign...SystemAssign...EscapedCommand...HelpEnd...>]\n",
       "        \n",
       "        \"\"\"\"\"\"\n",
       "\n",
       "After all of the `input` transformations are complete, the `input` should be valid source that `ast.parse, compile or shell.compile` \n",
       "may accept.\n",
       "\n",
       "        \"\"\"\"\"\"\n",
       "        >>> shell.ast_transformers\n",
       "        [...]\n",
       "        \n",
       "        \"\"\"\n",
       "\n",
       "        if not any(x for x in shell.ast_transformers if isinstance(x, ReturnYield)):\n",
       "            shell.ast_transformers.append(ReturnYield())\n",
       "\n",
       "\n",
       "\n",
       "    class PidginTransformer(IPython.core.inputtransformer2.TransformerManager):\n",
       "        def pidgin_transform(self, cell: str) -> str: \n",
       "            tokens = self.tokenizer.parse(''.join(cell))\n",
       "            return self.tokenizer.untokenize(tokens)\n",
       "        \n",
       "        def pidgin_cleanup(self, cell: str) -> list: \n",
       "            return self.pidgin_transform(cell).splitlines(True)\n",
       "        \n",
       "        def __init__(self, *args, **kwargs):\n",
       "            super().__init__(*args, **kwargs)\n",
       "            self.tokenizer = Tokenizer()\n",
       "            self.cleanup_transforms.insert(0, self.pidgin_cleanup)\n",
       "            self.line_transforms.append(demojize)\n",
       "\n",
       "        def pidgin_magic(self, *text): \n",
       "            \"\"\"Expand the text to tokens to tokens and \n",
       "            compact as a formatted `\"python\"` code.\"\"\"\n",
       "            return IPython.display.Code(self.pidgin_transform(''.join(text)), language='python')\n",
       "        \n",
       "\n",
       "\n",
       "\n",
       "    import ast\n",
       "    class ReturnYield(ast.NodeTransformer):\n",
       "        def visit_FunctionDef(self, node): return node\n",
       "        visit_AsyncFunctionDef = visit_FunctionDef\n",
       "        def visit_Return(self, node):\n",
       "            replace = ast.parse('''__import__('IPython').display.display()''').body[0]\n",
       "            replace.value.args = node.value.elts if isinstance(node.value, ast.Tuple) else [node.value]\n",
       "            return ast.copy_location(replace, node)\n",
       "\n",
       "        def visit_Expr(self, node):\n",
       "            if isinstance(node.value, (ast.Yield, ast.YieldFrom)):  return ast.copy_location(self.visit_Return(node.value), node)\n",
       "            return node\n",
       "        \n",
       "        visit_Expression = visit_Expr\n",
       "\n",
       "\n",
       "\n",
       "    import mistune as markdown, textwrap, __main__, IPython, typing, re, IPython, nbconvert, ipykernel, doctest, ast\n",
       "    __all__ = 'pidgin',\n",
       "\n",
       "\n",
       "\n",
       "    class Tokenizer(markdown.BlockLexer):\n",
       "            \"\"\"\n",
       "### Tokenizer\n",
       "\n",
       "<details>\n",
       "<summary>Tokenize `input` text into `\"code\" and not \"code\"` tokens that will be translated into valid `python` source.</summary>\n",
       "        \n",
       "            \"\"\"\n",
       "            class grammar_class(markdown.BlockGrammar):\n",
       "                doctest = doctest.DocTestParser._EXAMPLE_RE\n",
       "\n",
       "            def parse(self, text: str, default_rules=None) -> typing.List[dict]:\n",
       "                if not self.depth: self.tokens = []\n",
       "                with self: tokens = super().parse(whiten(text), default_rules)\n",
       "                if not self.depth: tokens = self.compact(text, tokens)\n",
       "                return tokens\n",
       "\n",
       "            def parse_doctest(self, m): self.tokens.append({'type': 'paragraph', 'text': m.group(0)})\n",
       "\n",
       "            def parse_fences(self, m):\n",
       "                if m.group(2): self.tokens.append({'type': 'paragraph', 'text': m.group(0)})\n",
       "                else: super().parse_fences(m)\n",
       "\n",
       "            def parse_hrule(self, m):\n",
       "                self.tokens.append({'type': 'hrule', 'text': m.group(0)})\n",
       "\n",
       "            def compact(self, text, tokens):\n",
       "                \"\"\"Combine non-code tokens into contiguous blocks.\"\"\"\n",
       "                compacted = []\n",
       "                while tokens:\n",
       "                    token = tokens.pop(0)\n",
       "                    if 'text' not in token: continue\n",
       "                    else: \n",
       "                        if not token['text'].strip(): continue\n",
       "                        block, body = token['text'].splitlines(), \"\"\n",
       "                    while block:\n",
       "                        line = block.pop(0)\n",
       "                        if line:\n",
       "                            before, line, text = text.partition(line)\n",
       "                            body += before + line\n",
       "                    if token['type']=='code':\n",
       "                        compacted.append({'type': 'code', 'lang': None, 'text': body})\n",
       "                    else:\n",
       "                        if compacted and compacted[-1]['type'] == 'paragraph':\n",
       "                            compacted[-1]['text'] += body\n",
       "                        else: compacted.append({'type': 'paragraph', 'text': body})\n",
       "                if compacted and compacted[-1]['type'] == 'paragraph':\n",
       "                    compacted[-1]['text'] += text\n",
       "                elif text.strip():\n",
       "                    compacted.append({'type': 'paragraph', 'text': text})\n",
       "                return compacted\n",
       "\n",
       "            depth = 0\n",
       "            def __enter__(self): self.depth += 1\n",
       "            def __exit__(self, *e): self.depth -= 1\n",
       "\n",
       "            def untokenize(self, tokens: Ï„.List[dict], source: str = \"\"\"\"\"\", last: int =0) -> str:\n",
       "                INDENT = indent = base_indent(tokens) or 4\n",
       "                for i, token in enumerate(tokens):\n",
       "                    object = token['text']\n",
       "                    if token and token['type'] == 'code':\n",
       "                        if object.lstrip().startswith(FENCE):\n",
       "\n",
       "                            object = ''.join(''.join(object.partition(FENCE)[::2]).rpartition(FENCE)[::2])\n",
       "                            indent = INDENT + num_first_indent(object)\n",
       "                            object = textwrap.indent(object, INDENT*SPACE)\n",
       "\n",
       "                        if object.lstrip().startswith(MAGIC):  ...\n",
       "                        else: indent = num_last_indent(object)\n",
       "                    elif not object: ...\n",
       "                    else:\n",
       "                        object = textwrap.indent(object, indent*SPACE)\n",
       "                        for next in tokens[i+1:]:\n",
       "                            if next['type'] == 'code':\n",
       "                                next = num_first_indent(next['text'])\n",
       "                                break\n",
       "                        else: next = indent       \n",
       "                        Î” = max(next-indent, 0)\n",
       "\n",
       "                        if not Î” and source.rstrip().rstrip(CONTINUATION).endswith(COLON): \n",
       "                            Î” += 4\n",
       "\n",
       "                        spaces = num_whitespace(object)\n",
       "                        \"what if the spaces are ling enough\"\n",
       "                        object = object[:spaces] + Î”*SPACE+ object[spaces:]\n",
       "                        if not source.rstrip().rstrip(CONTINUATION).endswith(QUOTES): \n",
       "                            object = quote(object)\n",
       "                    source += object\n",
       "\n",
       "                for token in reversed(tokens):\n",
       "                    if token['text'].strip():\n",
       "                        if token['type'] != 'code': \n",
       "                            source = source.rstrip() + SEMI\n",
       "                        break\n",
       "\n",
       "                return source \n",
       "            \n",
       "    for x in \"default_rules footnote_rules list_rules\".split():\n",
       "        setattr(Tokenizer, x, list(getattr(Tokenizer, x)))\n",
       "        getattr(Tokenizer, x).insert(getattr(Tokenizer, x).index('block_code'), 'doctest')\n",
       "        \n",
       "    ...\n",
       "    \"\"\"\n",
       "</details>&nbsp;\n",
       "\n",
       "    \"\"\"\n",
       "    pidgin = PidginTransformer()\n",
       "\n",
       "\n",
       "A potential outcome of a `pidgin` program is reusable code. \n",
       "\n",
       "Import pidgin notebooks as modules.\n",
       "\n",
       "\n",
       "    class PidginLoader(__import__('importnb').Notebook): \n",
       "        extensions = \".ipynb .md.ipynb\".split()\n",
       "        def code(self, str): return ''.join(pidgin.transform_cell(str))\n",
       "\n",
       "\n",
       "\n",
       "    class PidginPreprocessor(nbconvert.preprocessors.Preprocessor):\n",
       "        def preprocess_cell(self, cell, resources, index, ):\n",
       "            if cell['cell_type'] == 'code':\n",
       "                cell['source'] = pidgin_transformer.transform_cell(''.join(cell['source']))\n",
       "            return cell, resources\n",
       "\n",
       "\n",
       "The shell is the application either jupyterlab or jupyter notebook, the kernel determines the programming language.  Below we design a just jupyter kernel that can be installed using \n",
       "\n",
       "    !pidgin kernel install\n",
       "\n",
       "\n",
       "    class PidginInteractiveShell(IPython.InteractiveShell):\n",
       "        \"\"\"Configure a native `pidgin` `IPython.InteractiveShell`\"\"\"\n",
       "    PidginInteractiveShell.input_transformer_manager.default_value = PidginTransformer\n",
       "    PidginInteractiveShell.enable_html_pager.default_value = True\n",
       "\n",
       "    class PidginKernelApp(ipykernel.kernelapp.IPKernelApp): \n",
       "        \"\"\"Configure a native `pidgin` `__import__('ipykernel').kernelapp.IPKernelApp\"\"\"\n",
       "    PidginKernelApp.shell.default_value = PidginInteractiveShell\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "text/markdown": {
       "end_time": "2020-02-01T16:59:21.011800",
       "modules": [],
       "names": [],
       "start_time": "2020-02-01T16:59:20.838599"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## The `pidgin` implementation\n",
    "### An `IPython` extension.\n",
    "\n",
    "{{appendix.exports(imports)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"461pt\" height=\"170pt\"\n",
       " viewBox=\"0.00 0.00 461.00 170.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 166)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-166 457,-166 457,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster_pidgin</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"8,-8 8,-154 262,-154 262,-8 8,-8\"/>\n",
       "<text text-anchor=\"middle\" x=\"135\" y=\"-138.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">new school</text>\n",
       "</g>\n",
       "<g id=\"clust3\" class=\"cluster\">\n",
       "<title>cluster_web</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"270,-8 270,-154 445,-154 445,-8 270,-8\"/>\n",
       "<text text-anchor=\"middle\" x=\"357.5\" y=\"-138.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">old school</text>\n",
       "</g>\n",
       "<!-- PIDGIN -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>PIDGIN</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"126\" cy=\"-106\" rx=\"40.149\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"126\" y=\"-101.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">PIDGIN</text>\n",
       "</g>\n",
       "<!-- PYTHON -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>PYTHON</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"62\" cy=\"-34\" rx=\"45.9459\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"62\" y=\"-29.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">PYTHON</text>\n",
       "</g>\n",
       "<!-- PIDGIN&#45;&gt;PYTHON -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>PIDGIN&#45;&gt;PYTHON</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M110.8329,-88.937C102.8995,-80.0119 93.0363,-68.9159 84.2554,-59.0373\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"86.7624,-56.5895 77.5028,-51.4407 81.5306,-61.2401 86.7624,-56.5895\"/>\n",
       "</g>\n",
       "<!-- MARKDOWN -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>MARKDOWN</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"190\" cy=\"-34\" rx=\"64.296\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"190\" y=\"-29.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">MARKDOWN</text>\n",
       "</g>\n",
       "<!-- PIDGIN&#45;&gt;MARKDOWN -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>PIDGIN&#45;&gt;MARKDOWN</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M141.1671,-88.937C149.0422,-80.0775 158.8187,-69.0789 167.5507,-59.2554\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"170.2457,-61.4918 174.2734,-51.6924 165.0138,-56.8413 170.2457,-61.4918\"/>\n",
       "</g>\n",
       "<!-- WEB -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>WEB</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"364\" cy=\"-106\" rx=\"29.5104\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"364\" y=\"-101.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">WEB</text>\n",
       "</g>\n",
       "<!-- PASCAL -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>PASCAL</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"321\" cy=\"-34\" rx=\"43.4111\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"321\" y=\"-29.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">PASCAL</text>\n",
       "</g>\n",
       "<!-- WEB&#45;&gt;PASCAL -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>WEB&#45;&gt;PASCAL</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M353.8096,-88.937C348.7281,-80.4284 342.4682,-69.9468 336.7833,-60.4279\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"339.6987,-58.4833 331.5663,-51.6924 333.6889,-62.0725 339.6987,-58.4833\"/>\n",
       "</g>\n",
       "<!-- TEX -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>TEX</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"410\" cy=\"-34\" rx=\"27.0958\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"410\" y=\"-29.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">TEX</text>\n",
       "</g>\n",
       "<!-- WEB&#45;&gt;TEX -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>WEB&#45;&gt;TEX</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M374.9014,-88.937C380.5155,-80.1496 387.4741,-69.2579 393.7112,-59.4956\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"396.7424,-61.2519 399.1769,-50.9405 390.8435,-57.4831 396.7424,-61.2519\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x1143bb588>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/markdown": [
       "## programming in `markdown and python` \n",
       "[ðŸ““](imports.ipynb)\n",
       "\n",
       "\n",
       "    def load_ipython_extension(shell):\n",
       "        \"\"\"\n",
       "The `pidgin` `load_ipython_extension`'s primary function transforms the `jupyter`\n",
       "`notebook`s into a literate computing interfaces.\n",
       "`markdown` becomes the primary plain-text format for submitting code,\n",
       "and the `markdown` is translated to `python` source code\n",
       "before compilation.\n",
       "The implementation configures the appropriate\n",
       "features of the `IPython.InteractiveShell` to accomodate\n",
       "the interactive literate programming experience.\n",
       "\n",
       "In this section, we'll implement a `shell.input_transformer_manager`\n",
       "that handles the logical translation of `markdown` to `python`.\n",
       "The translation maintains the source line numbers and \n",
       "normalizes the narrative relative to the source code.  Consequently,\n",
       "introduces new syntaxes at the interfaces between `markdown and python`.\n",
       "\n",
       "        \"\"\"\n",
       "        pidgin_transformer = PidginTransformer()        \n",
       "        shell.input_transformer_manager = pidgin_transformer\n",
       "        \n",
       "        \"\"\"\n",
       "`IPython` provides configurable interactive `shell` properties.  Some of the configurable properties\n",
       "control how `input` code is translated into valid source code. \n",
       "The `pidgin` translation is managed by a custom `IPython.core.inputtransformer2.TransformerManager`.\n",
       "        \n",
       "        \"\"\"\"\"\"\n",
       "        >>> shell.input_transformer_manager\n",
       "        <...PidginTransformer object...>\n",
       "        \n",
       "        \"\"\"\"\"\"\n",
       "\n",
       "The `shell.input_transformer_manager` applies string transformations to clean up the `input`\n",
       "to be valid `python`.  There are three stages of line of transforms.\n",
       "\n",
       "1. Cleanup transforms that operate on the entire cell `input`.\n",
       "\n",
       "        \"\"\"\"\"\"\n",
       "        >>> shell.input_transformers_cleanup\n",
       "        [<...leading_empty_lines...>, <...leading_indent...>, <...PromptStripper...>, ...]\n",
       "        \n",
       "        \"\"\"\"\"\"\n",
       "        \n",
       "2. Line transforms that are applied the cell `input` with split lines. \n",
       "This is where `IPython` introduces their bespoke cell magic syntaxes.\n",
       "        \n",
       "        \"\"\"\"\"\"\n",
       "        >>> shell.input_transformer_manager.line_transforms\n",
       "        [...<...cell_magic...>...]\n",
       "        \n",
       "        \"\"\"\"\"\"\n",
       "        \n",
       "3. Token transformers that look for specific tokens at the like level.  `IPython`'s default\n",
       "behavior introduces new symbols into the programming language.\n",
       "\n",
       "        \"\"\"\"\"\"\n",
       "        >>> shell.input_transformer_manager.token_transformers\n",
       "        [<...MagicAssign...SystemAssign...EscapedCommand...HelpEnd...>]\n",
       "        \n",
       "        \"\"\"\"\"\"\n",
       "\n",
       "After all of the `input` transformations are complete, the `input` should be valid source that `ast.parse, compile or shell.compile` \n",
       "may accept.\n",
       "\n",
       "        \"\"\"\"\"\"\n",
       "        >>> shell.ast_transformers\n",
       "        [...]\n",
       "        \n",
       "        \"\"\"\n",
       "\n",
       "        if not any(x for x in shell.ast_transformers if isinstance(x, ReturnYield)):\n",
       "            shell.ast_transformers.append(ReturnYield())\n",
       "\n",
       "\n",
       "\n",
       "    class PidginTransformer(IPython.core.inputtransformer2.TransformerManager):\n",
       "        def pidgin_transform(self, cell: str) -> str: \n",
       "            tokens = self.tokenizer.parse(''.join(cell))\n",
       "            return self.tokenizer.untokenize(tokens)\n",
       "        \n",
       "        def pidgin_cleanup(self, cell: str) -> list: \n",
       "            return self.pidgin_transform(cell).splitlines(True)\n",
       "        \n",
       "        def __init__(self, *args, **kwargs):\n",
       "            super().__init__(*args, **kwargs)\n",
       "            self.tokenizer = Tokenizer()\n",
       "            self.cleanup_transforms.insert(0, self.pidgin_cleanup)\n",
       "            self.line_transforms.append(demojize)\n",
       "\n",
       "        def pidgin_magic(self, *text): \n",
       "            \"\"\"Expand the text to tokens to tokens and \n",
       "            compact as a formatted `\"python\"` code.\"\"\"\n",
       "            return IPython.display.Code(self.pidgin_transform(''.join(text)), language='python')\n",
       "        \n",
       "\n",
       "\n",
       "\n",
       "    import ast\n",
       "    class ReturnYield(ast.NodeTransformer):\n",
       "        def visit_FunctionDef(self, node): return node\n",
       "        visit_AsyncFunctionDef = visit_FunctionDef\n",
       "        def visit_Return(self, node):\n",
       "            replace = ast.parse('''__import__('IPython').display.display()''').body[0]\n",
       "            replace.value.args = node.value.elts if isinstance(node.value, ast.Tuple) else [node.value]\n",
       "            return ast.copy_location(replace, node)\n",
       "\n",
       "        def visit_Expr(self, node):\n",
       "            if isinstance(node.value, (ast.Yield, ast.YieldFrom)):  return ast.copy_location(self.visit_Return(node.value), node)\n",
       "            return node\n",
       "        \n",
       "        visit_Expression = visit_Expr\n",
       "\n",
       "\n",
       "\n",
       "    import mistune as markdown, textwrap, __main__, IPython, typing, re, IPython, nbconvert, ipykernel, doctest, ast\n",
       "    __all__ = 'pidgin',\n",
       "\n",
       "\n",
       "\n",
       "    class Tokenizer(markdown.BlockLexer):\n",
       "            \"\"\"\n",
       "### Tokenizer\n",
       "\n",
       "<details>\n",
       "<summary>Tokenize `input` text into `\"code\" and not \"code\"` tokens that will be translated into valid `python` source.</summary>\n",
       "        \n",
       "            \"\"\"\n",
       "            class grammar_class(markdown.BlockGrammar):\n",
       "                doctest = doctest.DocTestParser._EXAMPLE_RE\n",
       "\n",
       "            def parse(self, text: str, default_rules=None) -> typing.List[dict]:\n",
       "                if not self.depth: self.tokens = []\n",
       "                with self: tokens = super().parse(whiten(text), default_rules)\n",
       "                if not self.depth: tokens = self.compact(text, tokens)\n",
       "                return tokens\n",
       "\n",
       "            def parse_doctest(self, m): self.tokens.append({'type': 'paragraph', 'text': m.group(0)})\n",
       "\n",
       "            def parse_fences(self, m):\n",
       "                if m.group(2): self.tokens.append({'type': 'paragraph', 'text': m.group(0)})\n",
       "                else: super().parse_fences(m)\n",
       "\n",
       "            def parse_hrule(self, m):\n",
       "                self.tokens.append({'type': 'hrule', 'text': m.group(0)})\n",
       "\n",
       "            def compact(self, text, tokens):\n",
       "                \"\"\"Combine non-code tokens into contiguous blocks.\"\"\"\n",
       "                compacted = []\n",
       "                while tokens:\n",
       "                    token = tokens.pop(0)\n",
       "                    if 'text' not in token: continue\n",
       "                    else: \n",
       "                        if not token['text'].strip(): continue\n",
       "                        block, body = token['text'].splitlines(), \"\"\n",
       "                    while block:\n",
       "                        line = block.pop(0)\n",
       "                        if line:\n",
       "                            before, line, text = text.partition(line)\n",
       "                            body += before + line\n",
       "                    if token['type']=='code':\n",
       "                        compacted.append({'type': 'code', 'lang': None, 'text': body})\n",
       "                    else:\n",
       "                        if compacted and compacted[-1]['type'] == 'paragraph':\n",
       "                            compacted[-1]['text'] += body\n",
       "                        else: compacted.append({'type': 'paragraph', 'text': body})\n",
       "                if compacted and compacted[-1]['type'] == 'paragraph':\n",
       "                    compacted[-1]['text'] += text\n",
       "                elif text.strip():\n",
       "                    compacted.append({'type': 'paragraph', 'text': text})\n",
       "                return compacted\n",
       "\n",
       "            depth = 0\n",
       "            def __enter__(self): self.depth += 1\n",
       "            def __exit__(self, *e): self.depth -= 1\n",
       "\n",
       "            def untokenize(self, tokens: Ï„.List[dict], source: str = \"\"\"\"\"\", last: int =0) -> str:\n",
       "                INDENT = indent = base_indent(tokens) or 4\n",
       "                for i, token in enumerate(tokens):\n",
       "                    object = token['text']\n",
       "                    if token and token['type'] == 'code':\n",
       "                        if object.lstrip().startswith(FENCE):\n",
       "\n",
       "                            object = ''.join(''.join(object.partition(FENCE)[::2]).rpartition(FENCE)[::2])\n",
       "                            indent = INDENT + num_first_indent(object)\n",
       "                            object = textwrap.indent(object, INDENT*SPACE)\n",
       "\n",
       "                        if object.lstrip().startswith(MAGIC):  ...\n",
       "                        else: indent = num_last_indent(object)\n",
       "                    elif not object: ...\n",
       "                    else:\n",
       "                        object = textwrap.indent(object, indent*SPACE)\n",
       "                        for next in tokens[i+1:]:\n",
       "                            if next['type'] == 'code':\n",
       "                                next = num_first_indent(next['text'])\n",
       "                                break\n",
       "                        else: next = indent       \n",
       "                        Î” = max(next-indent, 0)\n",
       "\n",
       "                        if not Î” and source.rstrip().rstrip(CONTINUATION).endswith(COLON): \n",
       "                            Î” += 4\n",
       "\n",
       "                        spaces = num_whitespace(object)\n",
       "                        \"what if the spaces are ling enough\"\n",
       "                        object = object[:spaces] + Î”*SPACE+ object[spaces:]\n",
       "                        if not source.rstrip().rstrip(CONTINUATION).endswith(QUOTES): \n",
       "                            object = quote(object)\n",
       "                    source += object\n",
       "\n",
       "                for token in reversed(tokens):\n",
       "                    if token['text'].strip():\n",
       "                        if token['type'] != 'code': \n",
       "                            source = source.rstrip() + SEMI\n",
       "                        break\n",
       "\n",
       "                return source \n",
       "            \n",
       "    for x in \"default_rules footnote_rules list_rules\".split():\n",
       "        setattr(Tokenizer, x, list(getattr(Tokenizer, x)))\n",
       "        getattr(Tokenizer, x).insert(getattr(Tokenizer, x).index('block_code'), 'doctest')\n",
       "        \n",
       "    ...\n",
       "    \"\"\"\n",
       "</details>&nbsp;\n",
       "\n",
       "    \"\"\"\n",
       "    pidgin = PidginTransformer()\n",
       "\n",
       "\n",
       "A potential outcome of a `pidgin` program is reusable code. \n",
       "\n",
       "Import pidgin notebooks as modules.\n",
       "\n",
       "\n",
       "    class PidginLoader(__import__('importnb').Notebook): \n",
       "        extensions = \".ipynb .md.ipynb\".split()\n",
       "        def code(self, str): return ''.join(pidgin.transform_cell(str))\n",
       "\n",
       "\n",
       "\n",
       "    class PidginPreprocessor(nbconvert.preprocessors.Preprocessor):\n",
       "        def preprocess_cell(self, cell, resources, index, ):\n",
       "            if cell['cell_type'] == 'code':\n",
       "                cell['source'] = pidgin_transformer.transform_cell(''.join(cell['source']))\n",
       "            return cell, resources\n",
       "\n",
       "\n",
       "The shell is the application either jupyterlab or jupyter notebook, the kernel determines the programming language.  Below we design a just jupyter kernel that can be installed using \n",
       "\n",
       "    !pidgin kernel install\n",
       "\n",
       "\n",
       "    class PidginInteractiveShell(IPython.InteractiveShell):\n",
       "        \"\"\"Configure a native `pidgin` `IPython.InteractiveShell`\"\"\"\n",
       "    PidginInteractiveShell.input_transformer_manager.default_value = PidginTransformer\n",
       "    PidginInteractiveShell.enable_html_pager.default_value = True\n",
       "\n",
       "    class PidginKernelApp(ipykernel.kernelapp.IPKernelApp): \n",
       "        \"\"\"Configure a native `pidgin` `__import__('ipykernel').kernelapp.IPKernelApp\"\"\"\n",
       "    PidginKernelApp.shell.default_value = PidginInteractiveShell\n",
       "\n",
       "\n",
       "\n",
       "    graphviz.Source(\n",
       "digraph{rankdir=UD \n",
       "subgraph cluster_pidgin {label=\"new school\" PIDGIN->{PYTHON MARKDOWN}}\n",
       "subgraph cluster_web {label=\"old school\" WEB->{PASCAL TEX} }}\n",
       "    \n",
       "    )"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "text/markdown": {
       "end_time": "2020-02-01T16:59:21.690958",
       "modules": [],
       "names": [],
       "start_time": "2020-02-01T16:59:21.523840"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## programming in `markdown and python` \n",
    "[ðŸ““]({{pathlib.Path(imports.__file__).name}})\n",
    "\n",
    "\n",
    "{{appendix.exports(imports)}}\n",
    "\n",
    "    graphviz.Source(\n",
    "digraph{rankdir=UD \n",
    "subgraph cluster_pidgin {label=\"new school\" PIDGIN->{PYTHON MARKDOWN}}\n",
    "subgraph cluster_web {label=\"old school\" WEB->{PASCAL TEX} }}\n",
    "    \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## testing `\"code\"` in the `markdown` narrative.\n",
       "[ðŸ“”](testing.md.ipynb)\n",
       "\n",
       "In literate programs, `\"code\"` is deeply entangled with the narrative.\n",
       "`\"code\"` object can signify meaning and can be validated through testing.\n",
       "`python` introduced the `doctest` literate programming convention that indicates some text in a narrative can be tested.\n",
       "`pidgin` extends the `doctest` opinion to the inline markdown code.\n",
       "Each time a `pidgin` cell is executed, the `doctest`s and inline code are executed ensuring that\n",
       "any code in a `pidgin` program is valid.\n",
       "\n",
       "\n",
       "\n",
       "    def post_run_cell(result):\n",
       "        result.runner = test_markdown_string(result.info.raw_cell, IPython.get_ipython(), False, doctest.ELLIPSIS)\n",
       "\n",
       "    def load_ipython_extension(shell): \n",
       "        unload_ipython_extension(shell)\n",
       "        shell.events.register('post_run_cell', post_run_cell)\n",
       "\n",
       "\n",
       "\n",
       "    import doctest, contextlib, mistune as markdown, re, ast, __main__, IPython, operator\n",
       "    shell = IPython.get_ipython()\n",
       "\n",
       "\n",
       "`test_markdown_string` extends the standard python `doctest` tools \n",
       "to inline code objects written in markdown.  \n",
       "This approach compliments are markdown forward programming language to test\n",
       "intertextual references between code and narrative.\n",
       "\n",
       "\n",
       "    INLINE = re.compile(\n",
       "        markdown.InlineGrammar.code\n",
       "        .pattern[1:]\n",
       "        .replace('[\\s\\S]*', '?P<source>[\\s\\S]+')\n",
       "        .replace('+)\\s*', '{1,2})(?P<indent>\\s{0})'), \n",
       "    )\n",
       "\n",
       "\n",
       "    (TICK,), SPACE = '`'.split(), ' '\n",
       "\n",
       "\n",
       "\n",
       "    def test_markdown_string(str, shell=shell, verbose=False, compileflags=None):\n",
       "        globs, filename = shell.user_ns, F\"In[{shell.last_execution_result.execution_count}]\"\n",
       "        runner = doctest.DocTestRunner(verbose=verbose, optionflags=compileflags)  \n",
       "        parsers = DocTestParser(runner), InlineDoctestParser(runner)\n",
       "        parsers = {\n",
       "            parser: doctest.DocTestFinder(verbose, parser).find(str, filename) for parser in parsers\n",
       "        }\n",
       "        examples = sum([test.examples for x in parsers.values() for test in x], [])\n",
       "        examples.sort(key=operator.attrgetter('lineno'))\n",
       "        with ipython_compiler(shell):\n",
       "            for example in examples:\n",
       "                for parser, value in parsers.items():\n",
       "                    for value in value:\n",
       "                        if example in value.examples:\n",
       "                            with parser:\n",
       "                                runner.run(doctest.DocTest(\n",
       "                                    [example], globs, value.name, filename, example.lineno, value.docstring\n",
       "                                ), compileflags=compileflags, clear_globs=False)\n",
       "        shell.log.info(F\"In[{shell.last_execution_result.execution_count}]: {runner.summarize()}\")\n",
       "        return runner\n",
       "\n",
       "\n",
       "\n",
       "    @contextlib.contextmanager\n",
       "    def ipython_compiler(shell):\n",
       "        def compiler(input, filename, symbol, *args, **kwargs):\n",
       "            nonlocal shell\n",
       "            return shell.compile(\n",
       "                ast.Interactive(\n",
       "                    body=shell.transform_ast(\n",
       "                        shell.compile.ast_parse(input)\n",
       "                    ).body\n",
       "                ),\n",
       "                F\"In[{shell.last_execution_result.execution_count}]\",\n",
       "                \"single\",\n",
       "            )\n",
       "\n",
       "        yield setattr(doctest, \"compile\", compiler)\n",
       "        try:\n",
       "            doctest.compile = compile\n",
       "        except:\n",
       "            ...\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "text/markdown": {
       "end_time": "2020-02-01T16:59:21.847571",
       "modules": [],
       "names": [],
       "start_time": "2020-02-01T16:59:21.733721"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## testing `\"code\"` in the `markdown` narrative.\n",
    "[ðŸ“”]({{pathlib.Path(testing.__file__).name}})\n",
    "\n",
    "{{appendix.exports(testing)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Weaving the `markdown` to a rich display.\n",
       "[ðŸ“—](exports.md.ipynb)\n",
       "\n",
       "This is the weaving step.\n",
       "\n",
       "\n",
       "\n",
       "    import datetime, dataclasses, sys, IPython as python, IPython, nbconvert as export, collections, IPython as python, mistune as markdown, hashlib\n",
       "    exporter, shell = export.exporters.TemplateExporter(), python.get_ipython()\n",
       "    modules = lambda:[x for x in sys.modules if '.' not in x and not str.startswith(x,'_')]\n",
       "\n",
       "\n",
       "\n",
       "Markdown input can fail to render when jinja2 is used in correctly.  Markdown is never wrong, but sometimes jinja is.\n",
       "\n",
       "\n",
       "\n",
       "    ---------------------------------------------------------------------------\n",
       "\n",
       "    NameError                                 Traceback (most recent call last)\n",
       "\n",
       "    <ipython-input-1-25dc2f3cd961> in <module>\n",
       "    ----> 1 @dataclasses.dataclass\n",
       "          2 class Metadata:\n",
       "          3     def pre_run_cell(self, info):\n",
       "          4         self.modules = modules()\n",
       "          5         self.start = datetime.datetime.utcnow().isoformat()\n",
       "\n",
       "\n",
       "    NameError: name 'dataclasses' is not defined\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "text/markdown": {
       "end_time": "2020-02-01T16:59:22.291758",
       "modules": [],
       "names": [],
       "start_time": "2020-02-01T16:59:22.172732"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Weaving the `markdown` to a rich display.\n",
    "[ðŸ“—]({{pathlib.Path(exports.__file__).name}})\n",
    "\n",
    "{{appendix.exports(exports)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "UndefinedError",
     "evalue": "'syntax' is undefined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUndefinedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/jinja2/asyncsupport.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_async\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moriginal_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mloop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_event_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/jinja2/environment.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1006\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m             \u001b[0mexc_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/jinja2/environment.py\u001b[0m in \u001b[0;36mhandle_exception\u001b[0;34m(self, exc_info, rendered, source_hint)\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandard_exc_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mjoin_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/jinja2/_compat.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<template>\u001b[0m in \u001b[0;36mtop-level template code\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/jinja2/environment.py\u001b[0m in \u001b[0;36mgetattr\u001b[0;34m(self, obj, attribute)\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattribute\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mundefined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattribute\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUndefinedError\u001b[0m: 'syntax' is undefined"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## `pidgin` metasyntax at language interfaces.\n",
       "[ðŸ“—]({{pathlib.Path(syntax.__file__).name}})\n",
       "\n",
       "The combinations of document, programming, and templating languages\n",
       "provides unique syntaxes as the interfaces.\n",
       "\n",
       "{{appendix.exports(syntax)}}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "text/markdown": {
       "end_time": "2020-02-01T16:59:22.557939",
       "modules": [],
       "names": [],
       "start_time": "2020-02-01T16:59:22.422981"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## `pidgin` metasyntax at language interfaces.\n",
    "[ðŸ“—]({{pathlib.Path(syntax.__file__).name}})\n",
    "\n",
    "The combinations of document, programming, and templating languages\n",
    "provides unique syntaxes as the interfaces.\n",
    "\n",
    "{{appendix.exports(syntax)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Reusing `pidgin` documents.\n",
       "\n",
       "Notebooks gain value when they be reusable at rest.\n",
       "\n",
       "We'll make a cli application that deploys `pidgin` as a web, cli, converter.\n",
       "\n",
       "\n",
       "    @click.group()\n",
       "    def pidgin(): \n",
       "The `pidgin` command line application operates on passive notebooks\n",
       "documents.\n",
       "\n",
       "\n",
       "\n",
       "    @pidgin.command()\n",
       "    def serve(modules):\n",
       "Serve notebook modules from fastapi creating an openapi schema for each \n",
       "literate document.\n",
       "\n",
       "\n",
       "\n",
       "    @pidgin.command()\n",
       "    def run(modules, parallel=True):\n",
       "Run a collection of notebook modules.\n",
       "\n",
       "\n",
       "\n",
       "    @pidgin.command()\n",
       "    def convert(modules):\n",
       "Convert notebook written in pidgin to difference formats.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "text/markdown": {
       "end_time": "2020-02-01T16:59:28.799276",
       "modules": [],
       "names": [],
       "start_time": "2020-02-01T16:59:28.662079"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Reusing `pidgin` documents.\n",
    "\n",
    "Notebooks gain value when they be reusable at rest.\n",
    "\n",
    "We'll make a cli application that deploys `pidgin` as a web, cli, converter.\n",
    "\n",
    "\n",
    "{{appendix.exports(cli)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    def unload_ipython_extension(shell):\n",
    "        for x in (exports, testing, imports): x.unload_ipython_extension(shell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook paper.md.ipynb to markdown\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "    if __name__ == '__main__':\n",
    "        !jupyter nbconvert --to markdown --TemplateExporter.exclude_input=True --stdout paper.md.ipynb > readme.md\n",
    "    ...;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/usr/local/share/jupyter', '/usr/share/jupyter']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/markdown": [
       "    jupyter_client.kernelspec.SYSTEM_JUPYTER_PATH"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "text/markdown": {
       "end_time": "2020-02-01T17:12:35.812809",
       "modules": [],
       "names": [],
       "start_time": "2020-02-01T17:12:35.809285"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "    jupyter_client.kernelspec.SYSTEM_JUPYTER_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "    import jupyter_client.kernelspec"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "text/markdown": {
       "end_time": "2020-02-01T17:12:18.814961",
       "modules": [],
       "names": [],
       "start_time": "2020-02-01T17:12:18.813270"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "    import jupyter_client.kernelspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "    import jupyter"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "text/markdown": {
       "end_time": "2020-02-01T17:10:07.530807",
       "modules": [],
       "names": [],
       "start_time": "2020-02-01T17:10:07.528889"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "    import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
